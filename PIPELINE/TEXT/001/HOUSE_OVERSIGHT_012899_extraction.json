{
  "file_name": "HOUSE_OVERSIGHT_012899.txt",
  "file_path": "TEXT\\001\\HOUSE_OVERSIGHT_012899.txt",
  "content": {
    "full_text": "﻿Ben Goertzel with Cassio Pennachin & Nil Geisweiller &\nthe OpenCog Team\nEngineering General Intelligence, Part 1:\nA Path to Advanced AGI via Embodied Learning and\nCognitive Synergy\nSeptember 19, 2013\n\nThis book is dedicated by Ben Goertzel to his beloved,\ndeparted grandfather, Leo Zwell – an amazingly\nwarm-hearted, giving human being who was also a deep\nthinker and excellent scientist, who got Ben started on the\npath of science. As a careful experimentalist, Leo would\nhave been properly skeptical of the big hypotheses made\nhere – but he would have been eager to see them put to the\ntest!\n\nPreface\nThis is a large, two-part book with an even larger goal: To outline a practical approach to\nengineering software systems with general intelligence at the human level and ultimately beyond.\nMachines with flexible problem-solving ability, open-ended learning capability, creativity and\neventually, their own kind of genius.\nPart 1, this volume, reviews various critical conceptual issues related to the nature of intelligence\nand mind. It then sketches the broad outlines of a novel, integrative architecture for\nArtificial General Intelligence (AGI) called CogPrime ... and describes an approach for giving a\nyoung AGI system (CogPrime or otherwise) appropriate experience, so that it can develop its\nown smarts, creativity and wisdom through its own experience. Along the way a formal theory\nof general intelligence is sketched, and a broad roadmap leading from here to human-level artificial\nintelligence. Hints are also given regarding how to eventually, potentially create machines\nadvancing beyond human level – including some frankly futuristic speculations about strongly\nself-modifying AGI architectures with flexibility far exceeding that of the human brain.\nPart 2 then digs far deeper into the details of CogPrime’s multiple structures, processes and\nfunctions, culminating in a general argument as to why we believe CogPrime will be able to\nachieve general intelligence at the level of the smartest humans (and potentially greater), and\na detailed discussion of how a CogPrime-powered virtual agent or robot would handle some\nsimple practical tasks such as social play with blocks in a preschool context. It first describes\nthe CogPrime software architecture and knowledge representation in detail; then reviews the\ncognitive cycle via which CogPrime perceives and acts in the world and reflects on itself; and\nnext turns to various forms of learning: procedural, declarative (e.g. inference), simulative and\nintegrative. Methods of enabling natural language functionality in CogPrime are then discussed;\nand then the volume concludes with a chapter summarizing the argument that CogPrime can\nlead to human-level (and eventually perhaps greater) AGI, and a chapter giving a thought\nexperiment describing the internal dynamics via which a completed CogPrime system might\nsolve the problem of obeying the request “Build me something with blocks that I haven’t seen\nbefore.”\nThe chapters here are written to be read in linear order – and if consumed thus, they tell\na coherent story about how to get from here to advanced AGI. However, the impatient reader\nmay be forgiven for proceeding a bit nonlinearly. An alternate reading path for the impatient\nreader would be to start with the first few chapters of Part 1, then skim the final two chapters of\nPart 2, and then return to reading in linear order. The final two chapters of Part 2 give a broad\noverview of why we think the CogPrime design will work, in a way that depends on the technical\nvii\nviii\ndetails of the previous chapters, but (we believe) not so sensitively as to be incomprehensible\nwithout them.\nThis is admittedly an unusual sort of book, mixing demonstrated conclusions with unproved\nconjectures in a complex way, all oriented toward an extraordinarily ambitious goal. Further,\nthe chapters are somewhat variant in their levels of detail – some very nitty-gritty, some more\nhigh level, with much of the variation due to how much concrete work has been done on the\ntopic of the chapter at time of writing. However, it is important to understand that the ideas\npresented here are not mere armchair speculation – they are currently being used as the basis\nfor an open-source software project called OpenCog, which is being worked on by software\ndevelopers around the world. Right now OpenCog embodies only a percentage of the overall\nCogPrime design as described here. But if OpenCog continues to attract sufficient funding\nor volunteer interest, then the ideas presented in these volumes will be validated or refuted\nvia practice. (As a related note: here and there in this book, we will refer to the \"current\"\nCogPrime implementation (in the OpenCog framework); in all cases this refers to OpenCog as\nof late 2013.)\nTo state one believes one knows a workable path to creating a human-level (and potentially\ngreater) general intelligence is to make a dramatic statement, given the conventional way of\nthinking about the topic in the contemporary scientific community. However, we feel that once\na little more time has passed, the topic will lose its drama (if not its interest and importance),\nand it will be widely accepted that there are many ways to create intelligent machines – some\nsimpler and some more complicated; some more brain-like or human-like and some less so; some\nmore efficient and some more wasteful of resources; etc. We have little doubt that, from the\nperspective of AGI science 50 or 100 years hence (and probably even 10-20 years hence), the\nspecific designs presented here will seem awkward, messy, inefficient and circuitous in various\nrespects. But that is how science and engineering progress. Given the current state of knowledge\nand understanding, having any concrete, comprehensive design and plan for creating AGI is\na significant step forward; and it is in this spirit that we present here our thinking about the\nCogPrime architecture and the nature of general intelligence.\nIn the words of Sir Edmund Hillary, the first to scale Everest: “Nothing Venture, Nothing\nWin.”\nPrehistory of the Book\nThe writing of this book began in earnest in 2001, at which point it was informally referred to\nas “The Novamente Book.” The original “Novamente Book” manuscript ultimately got too big\nfor its own britches, and subdivided into a number of different works – The Hidden Pattern\n[Goe06a], a philosophy of mind book published in 2006; Probabilistic Logic Networks [GIGH08],\na more technical work published in 2008; Real World Reasoning [GGC + 11], a sequel to Probabilistic\nLogic Networks published in 2011; and the two parts of this book.\nThe ideas described in this book have been the collaborative creation of multiple overlapping\ncommunities of people over a long period of time. The vast bulk of the writing here was done by\nBen Goertzel; but Cassio Pennachin and Nil Geisweiller made sufficient writing, thinking and\nediting contributions over the years to more than merit their inclusion of co-authors. Further,\nmany of the chapters here have co-authors beyond the three main co-authors of the book; and\nthe set of chapter co-authors does not exhaust the set of significant contributors to the ideas\npresented.\nThe core concepts of the CogPrime design and the underlying theory were conceived by Ben\nGoertzel in the period 1995-1996 when he was a Research Fellow at the University of Western\nAustralia; but those early ideas have been elaborated and improved by many more people than\ncan be listed here (as well as by Ben’s ongoing thinking and research). The collaborative design\nprocess ultimately resulting in CogPrime started in 1997 when Intelligenesis Corp. was formed\n– the Webmind AI Engine created in Intelligenesis’s research group during 1997-2001 was the\npredecessor to the Novamente Cognition Engine created at Novamente LLC during 2001-2008,\nwhich was the predecessor to CogPrime.\nix\nAcknowledgements\nFor sake of simplicity, this acknowledgements section is presented from the perspective of the\nprimary author, Ben Goertzel. Ben will thus begin by expressing his thanks to his primary\nco-authors, Cassio Pennachin (collaborator since 1998) and Nil Geisweiller (collaborator since\n2005). Without outstandingly insightful, deep-thinking colleagues like you, the ideas presented\nhere – let alone the book itself– would not have developed nearly as effectively as what has\nhappened. Similar thanks also go to the other OpenCog collaborators who have co-authored\nvarious chapters of the book.\nBeyond the co-authors, huge gratitude must also be extended to everyone who has been\ninvolved with the OpenCog project, and/or was involved in Novamente LLC and Webmind Inc.\nbefore that. We are grateful to all of you for your collaboration and intellectual companionship!\nBuilding a thinking machine is a huge project, too big for any one human; it will take a team\nand I’m happy to be part of a great one. It is through the genius of human collectives, going\nbeyond any individual human mind, that genius machines are going to be created.\nA tiny, incomplete sample from the long list of those others deserving thanks is:\n• Ken Silverman and Gwendalin Qi Aranya (formerly Gwen Goertzel), both of whom listened\nto me talk at inordinate length about many of the ideas presented here a long, long time\nbefore anyone else was interested in listening. Ken and I schemed some AGI designs at\nSimon’s Rock College in 1983, years before we worked together on the Webmind AI Engine.\n• Allan Combs, who got me thinking about consciousness in various different ways, at a very\nearly point in my career. I’m very pleased to still count Allan as a friend and sometime\ncollaborator! Fred Abraham as well, for introducing me to the intersection of chaos theory\nand cognition, with a wonderful flair. George Christos, a deep AI/math/physics thinker from\nPerth, for re-awakening my interest in attractor neural nets and their cognitive implications,\nin the mid-1990s.\n• All of the 130 staff of Webmind Inc. during 1998-2001 while that remarkable, ambitious,\npeculiar AGI-oriented firm existed. Special shout-outs to the \"Voice of Reason\" Pei Wang\nand the \"Siberian Madmind\" Anton Kolonin, Mike Ross, Cate Hartley, Karin Verspoor and\nthe tragically prematurely deceased Jeff Pressing (compared to whom we are all mental\nmidgets), who all made serious conceptual contributions to my thinking about AGI. Lisa\nPazer and Andy Siciliano who made Webmind happen on the business side. And of course\nCassio Pennachin, a co-author of this book; and Ken Silverman, who co-architected the\nwhole Webmind system and vision with me from the start.\nx\n• The Webmind Diehards, who helped begin the Novamente project that succeeded Webmind\nbeginning in 2001: Cassio Pennachin, Stephan Vladimir Bugaj, Takuo Henmi, Matthew\nIkle’, Thiago Maia, Andre Senna, Guilherme Lamacie and Saulo Pinto\n• Those who helped get the Novamente project off the ground and keep it progressing over the\nyears, including some of the Webmind Diehards and also Moshe Looks, Bruce Klein, Izabela\nLyon Freire, Chris Poulin, Murilo Queiroz, Predrag Janicic, David Hart, Ari Heljakka, Hugo\nPinto, Deborah Duong, Paul Prueitt, Glenn Tarbox, Nil Geisweiller and Cassio Pennachin\n(the co-authors of this book), Sibley Verbeck, Jeff Reed, Pejman Makhfi, Welter Silva,\nLukasz Kaiser and more\n• All those who have helped with the OpenCog system, including Linas Vepstas, Joel Pitt,\nJared Wigmore / Jade O’Neill, Zhenhua Cai, Deheng Huang, Shujing Ke, Lake Watkins,\nAlex van der Peet, Samir Araujo, Fabricio Silva, Yang Ye, Shuo Chen, Michel Drenthe, Ted\nSanders, Gustavo Gama and of course Nil and Cassio again. Tyler Emerson and Eliezer\nYudkowsky, for choosing to have the Singularity Institute for AI (now MIRI) provide seed\nfunding for OpenCog.\n• The numerous members of the AGI community who have tossed around AGI ideas with me\nsince the first AGI conference in 2006, including but definitely not limited to: Stan Franklin,\nJuergen Schmidhuber, Marcus Hutter, Kai-Uwe Kuehnberger, Stephen Reed, Blerim Enruli,\nKristinn Thorisson, Joscha Bach, Abram Demski, Itamar Arel, Mark Waser, Randal Koene,\nPaul Rosenbloom, Zhongzhi Shi, Steve Omohundro, Bill Hibbard, Eray Ozkural, Brandon\nRohrer, Ben Johnston, John Laird, Shane Legg, Selmer Bringsjord, Anders Sandberg, Alexei\nSamsonovich, Wlodek Duch, and more\n• The inimitable \"Artilect Warrior\" Hugo de Garis, who (when he was working at Xiamen\nUniversity) got me started working on AGI in the Orient (and introduced me to my wife\nRuiting in the process). And Changle Zhou, who brought Hugo to Xiamen and generously\nshared his brilliant research students with Hugo and me. And Min Jiang, collaborator of\nHugo and Changle, a deep AGI thinker who is helping with OpenCog theory and practice\nat time of writing.\n• Gino Yu, who got me started working on AGI here in Hong Kong, where I am living at time\nof writing. As of 2013 the bulk of OpenCog work is occurring in Hong Kong via a research\ngrant that Gino and I obtained together\n• Dan Stoicescu, whose funding helped Novamente through some tough times.\n• Jeffrey Epstein, whose visionary funding of my AGI research has helped me through a\nnumber of tight spots over the years. At time of writing, Jeffrey is helping support the\nOpenCog Hong Kong project.\n• Zeger Karssen, founder of Atlantis Press, who conceived the Thinking Machines book series\nin which this book appears, and who has been a strong supporter of the AGI conference\nseries from the beginning\n• My wonderful wife Ruiting Lian, a source of fantastic amounts of positive energy for me\nsince we became involved four years ago. Ruiting has listened to me discuss the ideas\ncontained here time and time again, often with judicious and insightful feedback (as she\nis an excellent AI researcher in her own right); and has been wonderfully tolerant of me\ndiverting numerous evenings and weekends to getting this book finished (as well as to other\nAGI-related pursuits). And my parents Ted and Carol and kids Zar, Zeb and Zade, who\nhave also indulged me in discussions on many of the themes discussed here on countless\noccasions! And my dear, departed grandfather Leo Zwell, for getting me started in science.\n• Crunchkin and Pumpkin, for regularly getting me away from the desk to stroll around the\nvillage where we live; many of my best ideas about AGI and other topics have emerged\nwhile walking with my furry four-legged family members\nxi\nSeptember 2013\nBen Goertzel\n\nContents\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1 AI Returns to Its Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 AGI versus Narrow AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.3 CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.4 The Secret Sauce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.5 Extraordinary Proof? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.6 Potential Approaches to AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.6.1 Build AGI from Narrow AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.6.2 Enhancing Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.6.3 Emulating the Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.6.4 Evolve an AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.6.5 Derive an AGI design mathematically . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.6.6 Use heuristic computer science methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.6.7 Integrative Cognitive Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.6.8 Can Digital Computers Really Be Intelligent? . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.7 Five Key Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.7.1 Memory and Cognition in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.8 Virtually and Robotically Embodied AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.9 Language Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.10 AGI Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.11 Structure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n1.12 Key Claims of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nSection I Artificial and Natural General Intelligence\n2 What Is Human-Like General Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.1.1 What Is General Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.1.2 What Is Human-like General Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.2 Commonly Recognized Aspects of Human-like Intelligence . . . . . . . . . . . . . . . . . . . 20\n2.3 Further Characterizations of Humanlike Intelligence . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.3.1 Competencies Characterizing Human-like Intelligence . . . . . . . . . . . . . . . . . 24\n2.3.2 Gardner’s Theory of Multiple Intelligences . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nxiii\nxiv\nContents\n2.3.3 Newell’s Criteria for a Human Cognitive Architecture . . . . . . . . . . . . . . . . . 26\n2.3.4 intelligence and Creativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.4 Preschool as a View into Human-like General Intelligence . . . . . . . . . . . . . . . . . . . . 27\n2.4.1 Design for an AGI Preschool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.5 Integrative and Synergetic Approaches to Artificial General Intelligence . . . . . . . 29\n2.5.1 Achieving Humanlike Intelligence via Cognitive Synergy . . . . . . . . . . . . . . . 30\n3 A Patternist Philosophy of Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2 Some Patternist Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.3 Cognitive Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis . . . . . . . . 42\n3.4.1 Component-Systems and Self-Generating Systems . . . . . . . . . . . . . . . . . . . . 42\n3.4.2 Analysis and Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.4.3 The Dynamic of Iterative Analysis and Synthesis . . . . . . . . . . . . . . . . . . . . 46\n3.4.4 Self and Focused Attention as Approximate Attractors of the Dynamic\nof Iterated Forward-Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.5 Perspectives on Machine Consciousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.6 Postscript: Formalizing Pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4 Brief Survey of Cognitive Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.2 Symbolic Cognitive Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.1 SOAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.2.2 ACT-R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.2.3 Cyc and Texai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.2.4 NARS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.2.5 GLAIR and SNePS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3 Emergentist Cognitive Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.3.1 DeSTIN: A Deep Reinforcement Learning Approach to AGI . . . . . . . . . . . 66\n4.3.2 Developmental Robotics Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n4.4 Hybrid Cognitive Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.4.1 Neural versus Symbolic; Global versus Local . . . . . . . . . . . . . . . . . . . . . . . . . 75\n4.5 Globalist versus Localist Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n4.5.1 CLARION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n4.5.2 The Society of Mind and the Emotion Machine . . . . . . . . . . . . . . . . . . . . . . 80\n4.5.3 DUAL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4.5.4 4D/RCS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n4.5.5 PolyScheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.5.6 Joshua Blue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n4.5.7 LIDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n4.5.8 The Global Workspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n4.5.9 The LIDA Cognitive Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n4.5.10 Psi and MicroPsi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n4.5.11 The Emergence of Emotion in the Psi Model . . . . . . . . . . . . . . . . . . . . . . . . . 91\n4.5.12 Knowledge Representation, Action Selection and Planning in Psi . . . . . . . 93\nContents\nxv\n4.5.13 Psi versus CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n5 A Generic Architecture of Human-Like Cognition . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.2 Key Ingredients of the Integrative Human-Like Cognitive Architecture Diagram 96\n5.3 An Architecture Diagram for Human-Like General Intelligence . . . . . . . . . . . . . . . 97\n5.4 Interpretation and Application of the Integrative Diagram . . . . . . . . . . . . . . . . . . . 104\n6 A Brief Overview of CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.2 High-Level Architecture of CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n6.3 Current and Prior Applications of OpenCog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n6.3.1 Transitioning from Virtual Agents to a Physical Robot . . . . . . . . . . . . . . . . 110\n6.4 Memory Types and Associated Cognitive Processes in CogPrime . . . . . . . . . . . . . 110\n6.4.1 Cognitive Synergy in PLN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.5 Goal-Oriented Dynamics in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n6.6 Analysis and Synthesis Processes in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nSection II Toward a General Theory of General Intelligence\n7 A Formal Model of Intelligent Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.2 A Simple Formal Agents Model (SRAM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7.2.1 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7.2.2 Memory Stores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n7.2.3 The Cognitive Schematic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.3 Toward a Formal Characterization of Real-World General Intelligence . . . . . . . . . 135\n7.3.1 Biased Universal Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n7.3.2 Connecting Legg and Hutter’s Model of Intelligent Agents to the Real\nWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n7.3.3 Pragmatic General Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.4 Incorporating Computational Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.3.5 Assessing the Intelligence of Real-World Agents . . . . . . . . . . . . . . . . . . . . . . 139\n7.4 Intellectual Breadth: Quantifying the Generality of an Agent’s Intelligence . . . . . 141\n7.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n8 Cognitive Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n8.1 Cognitive Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n8.2 Cognitive Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n8.3 Cognitive Synergy in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n8.3.1 Cognitive Processes in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n8.4 Some Critical Synergies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n8.5 The Cognitive Schematic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n8.6 Cognitive Synergy for Procedural and Declarative Learning . . . . . . . . . . . . . . . . . . 153\n8.6.1 Cognitive Synergy in MOSES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n8.6.2 Cognitive Synergy in PLN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n8.7 Is Cognitive Synergy Tricky? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\nxvi\nContents\n8.7.1 The Puzzle: Why Is It So Hard to Measure Partial Progress Toward\nHuman-Level AGI? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.7.2 A Possible Answer: Cognitive Synergy is Tricky! . . . . . . . . . . . . . . . . . . . . . . 158\n8.7.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n9 General Intelligence in the Everyday Human World . . . . . . . . . . . . . . . . . . . . . . 161\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n9.2 Some Broad Properties of the Everyday World That Help Structure Intelligence 162\n9.3 Embodied Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n9.3.1 Generalizing the Embodied Communication Prior . . . . . . . . . . . . . . . . . . . . 166\n9.4 Naive Physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n9.4.1 Objects, Natural Units and Natural Kinds . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n9.4.2 Events, Processes and Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n9.4.3 Stuffs, States of Matter, Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n9.4.4 Surfaces, Limits, Boundaries, Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n9.4.5 What Kind of Physics Is Needed to Foster Human-like Intelligence? . . . . . 169\n9.5 Folk Psychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\n9.5.1 Motivation, Requiredness, Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n9.6 Body and Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n9.6.1 The Human Sensorium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n9.6.2 The Human Body’s Multiple Intelligences . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n9.7 The Extended Mind and Body . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n9.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n10 A Mind-World Correspondence Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n10.2 What Might a General Theory of General Intelligence Look Like? . . . . . . . . . . . . 178\n10.3 Steps Toward A (Formal) General Theory of General Intelligence . . . . . . . . . . . . . 179\n10.4 The Mind-World Correspondence Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n10.5 How Might the Mind-World Correspondence Principle Be Useful? . . . . . . . . . . . . 181\n10.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\nSection III Cognitive and Ethical Development\n11 Stages of Cognitive Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n11.2 Piagetan Stages in the Context of a General Systems Theory of Development . . 188\n11.3 Piaget’s Theory of Cognitive Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n11.3.1 Perry’s Stages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n11.3.2 Keeping Continuity in Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n11.4 Piaget’s Stages in the Context of Uncertain Inference . . . . . . . . . . . . . . . . . . . . . . . 193\n11.4.1 The Infantile Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\n11.4.2 The Concrete Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\n11.4.3 The Formal Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n11.4.4 The Reflexive Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\nContents\nxvii\n12 The Engineering and Development of Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n12.2 Review of Current Thinking on the Risks of AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n12.3 The Value of an Explicit Goal System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n12.4 Ethical Synergy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n12.4.1 Stages of Development of Declarative Ethics . . . . . . . . . . . . . . . . . . . . . . . . . 211\n12.4.2 Stages of Development of Empathic Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n12.4.3 An Integrative Approach to Ethical Development . . . . . . . . . . . . . . . . . . . . . 215\n12.4.4 Integrative Ethics and Integrative AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a\nMultifactorial Ethical Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n12.5.1 The Golden Rule and the Stages of Ethical Development . . . . . . . . . . . . . 222\n12.5.2 The Need for Context-Sensitivity and Adaptiveness in Deploying\nEthical Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n12.6 The Ethical Treatment of AGIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n12.6.1 Possible Consequences of Depriving AGIs of Freedom . . . . . . . . . . . . . . . . . 228\n12.6.2 AGI Ethics as Boundaries Between Humans and AGIs Become Blurred . 229\n12.7 Possible Benefits of Closely Linking AGIs to the Global Brain . . . . . . . . . . . . . . . . 230\n12.7.1 The Importance of Fostering Deep, Consensus-Building Interactions\nBetween People with Divergent Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n12.8 Possible Benefits of Creating Societies of AGIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n12.9 AGI Ethics As Related to Various Future Scenarios . . . . . . . . . . . . . . . . . . . . . . . . 234\n12.9.1 Capped Intelligence Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234\n12.9.2 Superintelligent AI: Soft-Takeoff Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\n12.9.3 Superintelligent AI: Hard-Takeoff Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . 235\n12.9.4 Global Brain Mindplex Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n12.10Conclusion: Eight Ways to Bias AGI Toward Friendliness . . . . . . . . . . . . . . . . . . . . 239\n12.10.1Encourage Measured Co-Advancement of AGI Software and AGI Ethics\nTheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n12.10.2Develop Advanced AGI Sooner Not Later . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nSection IV Networks for Explicit and Implicit Knowledge Representation\n13 Local, Global and Glocal Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . 245\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n13.2 Localized Knowledge Representation using Weighted, Labeled Hypergraphs . . . . 246\n13.2.1 Weighted, Labeled Hypergraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n13.3 Atoms: Their Types and Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n13.3.1 Some Basic Atom Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n13.3.2 Variable Atoms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n13.3.3 Logical Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251\n13.3.4 Temporal Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n13.3.5 Associative Links . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253\n13.3.6 Procedure Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n13.3.7 Links for Special External Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n13.3.8 Truth Values and Attention Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255\n13.4 Knowledge Representation via Attractor Neural Networks . . . . . . . . . . . . . . . . . . . 256\nxviii\nContents\n13.4.1 The Hopfield neural net model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n13.4.2 Knowledge Representation via Cell Assemblies . . . . . . . . . . . . . . . . . . . . . . 257\n13.5 Neural Foundations of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n13.5.1 Hebbian Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n13.5.2 Virtual Synapses and Hebbian Learning Between Assemblies . . . . . . . . . . 258\n13.5.3 Neural Darwinism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n13.6 Glocal Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n13.6.1 A Semi-Formal Model of Glocal Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\n13.6.2 Glocal Memory in the Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n13.6.3 Glocal Hopfield Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n13.6.4 Neural-Symbolic Glocality in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n14 Representing Implicit Knowledge via Hypergraphs . . . . . . . . . . . . . . . . . . . . . . . 271\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n14.2 Key Vertex and Edge Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n14.3 Derived Hypergraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n14.3.1 SMEPH Vertices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n14.3.2 SMEPH Edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n14.4 Implications of Patternist Philosophy for Derived Hypergraphs of Intelligent\nSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\n14.4.1 SMEPH Principles in CogPrime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276\n15 Emergent Networks of Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n15.2 Small World Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n15.3 Dual Network Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n15.3.1 Hierarchical Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n15.3.2 Associative, Heterarchical Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n15.3.3 Dual Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\nSection V A Path to Human-Level AGI\n16 AGI Preschool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n16.1.1 Contrast to Standard AI Evaluation Methodologies . . . . . . . . . . . . . . . . . . . 290\n16.2 Elements of Preschool Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291\n16.3 Elements of Preschool Curriculum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292\n16.3.1 Preschool in the Light of Intelligence Theory . . . . . . . . . . . . . . . . . . . . . . . . 293\n16.4 Task-Based Assessment in AGI Preschool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295\n16.5 Beyond Preschool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n16.6 Issues with Virtual Preschool Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n16.6.1 Integrating Virtual Worlds with Robot Simulators . . . . . . . . . . . . . . . . . . . . 301\n16.6.2 BlocksNBeads World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n17 A Preschool-Based Roadmap to Advanced AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307\n17.2 Measuring Incremental Progress Toward Human-Level AGI . . . . . . . . . . . . . . . . . . 308\n17.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\nContents\nxix\n18 Advanced Self-Modification: A Possible Path to Superhuman AGI . . . . . . . . 317\n18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n18.2 Cognitive Schema Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318\n18.3 Self-Modification via Supercompilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n18.3.1 Three Aspects of Supercompilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n18.3.2 Supercompilation for Goal-Directed Program Modification . . . . . . . . . . . . . 322\n18.4 Self-Modification via Theorem-Proving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\nA Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nA.1 List of Specialized Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325\nA.2 Glossary of Specialized Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n\nChapter 1\nIntroduction\n1.1 AI Returns to Its Roots\nOur goal in this book is straightforward, albeit ambitious: to present a conceptual and technical\ndesign for a thinking machine, a software program capable of the same qualitative sort of general\nintelligence as human beings. It’s not certain exactly how far the design outlined here will be\nable to take us, but it seems plausible that once fully implemented, tuned and tested, it will be\nable to achieve general intelligence at the human level and in some respects beyond.\nOur ultimate aim is Artificial General Intelligence construed in the broadest sense, including\nartificial creativity and artificial genius. We feel it is important to emphasize the extremely\nbroad potential of Artificial General Intelligence systems. The human brain is not built to be\nmodified, except via the slow process of evolution. Engineered AGI systems, built according to\ndesigns like the one outlined here, will be much more susceptible to rapid improvement from\ntheir initial state. It seems reasonable to us to expect that, relatively shortly after achieving the\nfirst roughly human-level AGI system, AGI systems with various sorts of beyond-human-level\ncapabilities will be achieved.\nThough these long-term goals are core to our motivations, we will spend much of our time here\nexplaining how we think we can make AGI systems do relatively simple things, like the things\nhuman children do in preschool. The penultimate chapter of (Part 2 of) the book describes a\nthought-experiment involving a robot playing with blocks, responding to the request \"Build me\nsomething I haven’t seen before.\" We believe that preschool creativity contains the seeds of,\nand the core structures and dynamics underlying, adult human level genius ... and new, as yet\nunforeseen forms of artificial innovation.\nMuch of the book focuses on a specific AGI architecture, which we call CogPrime, and which\nis currently in the midst of implementation using the OpenCog software framework. CogPrime\nis large and complex and embodies a host of specific decisions regarding the various aspects of\nintelligence. We don’t view CogPrime as the unique path to advanced AGI, nor as the ultimate\nend-all of AGI research. We feel confident there are multiple possible paths to advanced AGI,\nand that in following any of these paths, multiple theoretical and practical lessons will be\nlearned, leading to modifications of the ideas possessed while along the early stages of the path.\nBut our goal here is to articulate one path that we believe makes sense to follow, one overall\ndesign that we believe can work.\n1\n2 1 Introduction\n1.2 AGI versus Narrow AI\nAn outsider to the AI field might think this sort of book commonplace in the research literature,\nbut insiders know that’s far from the truth. The field of Artificial Intelligence (AI) was founded\nin the mid 1950s with the aim of constructing “thinking machines” - that is, computer systems\nwith human-like general intelligence, including humanoid robots that not only look but act\nand think with intelligence equal to and ultimately greater than human beings. But in the\nintervening years, the field has drifted far from its ambitious roots, and this book represents\npart of a movement aimed at restoring the initial goals of the AI field, but in a manner powered\nby new tools and new ideas far beyond those available half a century ago.\nAfter the first generation of AI researchers found the task of creating human-level AGI very\ndifficult given the technology of their time, the AI field shifted focus toward what Ray Kurzweil\nhas called \"narrow AI\" – the understanding of particular specialized aspects of intelligence; and\nthe creation of AI systems displaying intelligence regarding specific tasks in relatively narrow\ndomains. In recent years, however, the situation has been changing. More and more researchers\nhave recognized the necessity – and feasibility – of returning to the original goals of the field.\nIn the decades since the 1950s, cognitive science and neuroscience have taught us a lot about\nwhat a cognitive architecture needs to look like to support roughly human-like general intelligence.\nComputer hardware has advanced to the point where we can build distributed systems\ncontaining large amounts of RAM and large numbers of processors, carrying out complex tasks\nin real time. The AI field has spawned a host of ingenious algorithms and data structures, which\nhave been successfully deployed for a huge variety of purposes.\nDue to all this progress, increasingly, there has been a call for a transition from the current\nfocus on highly specialized “narrow AI” problem solving systems, back to confronting the more\ndifficult issues of “human level intelligence” and more broadly “artificial general intelligence\n(AGI).” Recent years have seen a growing number of special sessions, workshops and conferences\ndevoted specifically to AGI, including the annual BICA (Biologically Inspired Cognitive\nArchitectures) AAAI Symposium, and the international AGI conference series (one in 2006,\nand annual since 2008). And, even more exciting, as reviewed in Chapter 4, there are a number\nof contemporary projects focused directly and explicitly on AGI (sometimes under the name\n\"AGI\", sometimes using related terms such as \"Human Level Intelligence\").\nIn spite of all this progress, however, we feel that no one has yet clearly articulated a detailed,\nsystematic design for an AGI, with potential to yield general intelligence at the human level\nand ultimately beyond. In this spirit, our main goal in this lengthy two-part book is to outline\na novel design for a thinking machine – an AGI design which we believe has the capability to\nproduce software systems with intelligence at the human adult level and ultimately beyond.\nMany of the technical details of this design have been previously presented online in a wikibook\n[Goe10b]; and the basic ideas of the design have been presented briefly in a series of conference\npapers [GPSL03, GPPG06, Goe09c]. But the overall design has not been presented in a coherent\nand systematic way before this book. In order to frame this design properly, we also present\na considerable number of broader theoretical and conceptual ideas here, some more and some\nless technical in nature.\n1.4 The Secret Sauce 3\n1.3 CogPrime\nThe AGI design presented here has not previously been granted a name independently of its\nparticular software implementations, but for the purposes of this book it needs one, so we’ve\nchristened it CogPrime . This fits with the name “OpenCogPrime” that has already been\nused to describe the software implementation of CogPrime within the open-source OpenCog\nAGI software framework. The OpenCogPrime software, right now, implements only a small\nfraction of the CogPrime design as described here. However, OpenCog was designed specifically\nto enable efficient, scalable implementation of the full CogPrime design (as well as to serve as a\nmore general framework for AGI R&D); and work currently proceeds in this direction, though\nthere is a lot of work still to be done and many challenges remain. 1\nThe CogPrime design is more comprehensive and thorough than anything that has been\npresented in the literature previously, including the work of others reviewed in Chapter 4. It\ncovers all the key aspects of human intelligence, and explains how they interoperate and how\nthey can be implemented in digital computer software. Part 1 of this work outlines CogPrime at\na high level, and makes a number of more general points about artificial general intelligence and\nthe path thereto; then Part 2 digs deeply into the technical particulars of CogPrime. Even Part\n2, however, doesn’t explain all the details of CogPrime that have been worked out so far, and\nit definitely doesn’t explain all the implementation details that have gone into designing and\nbuilding OpenCogPrime. Creating a thinking machine is a large task, and even the intermediate\nlevel of detail takes up a lot of pages.\n1.4 The Secret Sauce\nThere is no consensus on why all the related technological and scientific progress mentioned\nabove has not yet yielded AI software systems with human-like general intelligence (or even\ngreater levels of brilliance!). However, we hypothesize that the core reason boils down to the\nfollowing three points:\n• Intelligence depends on the emergence of certain high-level structures and dynamics across\na system’s whole knowledge base;\n• We have not discovered any one algorithm or approach capable of yielding the emergence\nof these structures;\n• Achieving the emergence of these structures within a system formed by integrating a number\nof different AI algorithms and structures requires careful attention to the manner in which\n1 This brings up a terminological note: At several places in this Volume and the next we will refer to the current\nCogPrime or OpenCog implementation; in all cases this refers to OpenCog as of late 2013. We realize the risk\nof mentioning the state of our software system at time of writing: for future readers this may give the wrong\nimpression, because if our project goes well, more and more of CogPrime will get implemented and tested as\ntime goes on (e.g. within the OpenCog framework, under active development at time of writing). However, not\nmentioning the current implementation at all seems an even worse course to us, since we feel readers will be\ninterested to know which of our ideas – at time of writing – have been honed via practice and which have not.\nOnline resources such as http://opencog.org may be consulted by readers curious about the current state\nof the main OpenCog implementation; though in future forks of the code may be created, or other systems may\nbe built using some or all of the ideas in this book, etc.\n4 1 Introduction\nthese algorithms and structures are integrated; and so far the integration has not been done\nin the correct way.\nThe human brain appears to be an integration of an assemblage of diverse structures and\ndynamics, built using common components and arranged according to a sensible cognitive architecture.\nHowever, its algorithms and structures have been honed by evolution to work closely\ntogether – they are very tightly inter-adapted, in the same way that the different organs of\nthe body are adapted to work together. Due to their close interoperation they give rise to the\noverall systemic behaviors that characterize human-like general intelligence. We believe that\nthe main missing ingredient in AI so far is cognitive synergy: the fitting-together of different\nintelligent components into an appropriate cognitive architecture, in such a way that the\ncomponents richly and dynamically support and assist each other, interrelating very closely in\na similar manner to the components of the brain or body and thus giving rise to appropriate\nemergent structures and dynamics. This leads us to one of the central hypotheses underlying\nthe CogPrime approach to AGI: that the cognitive synergy ensuing from integrating\nmultiple symbolic and subsymbolic learning and memory components in an appropriate\ncognitive architecture and environment, can yield robust intelligence at the\nhuman level and ultimately beyond.\nThe reason this sort of intimate integration has not yet been explored much is that it’s difficult\non multiple levels, requiring the design of an architecture and its component algorithms with\na view toward the structures and dynamics that will arise in the system once it is coupled\nwith an appropriate environment. Typically, the AI algorithms and structures corresponding\nto different cognitive functions have been developed based on divergent theoretical principles,\nby disparate communities of researchers, and have been tuned for effective performance on\ndifferent tasks in different environments. Making such diverse components work together in a\ntruly synergetic and cooperative way is a tall order, yet we believe that this – rather than some\nparticular algorithm, structure or architectural principle – is the “secret sauce” needed to create\nhuman-level AGI based on technologies available today.\n1.5 Extraordinary Proof?\nThere is a saying that “extraordinary claims require extraordinary proof” and by that standard,\nif one believes that having a design for an advanced AGI is an extraordinary claim, this\nbook must be rated a failure. We don’t offer extraordinary proof that CogPrime, once fully\nimplemented and educated, will be capable of human-level general intelligence and more.\nIt would be nice if we could offer mathematical proof that CogPrime has the potential we\nthink it does, but at the current time mathematics is simply not up to the job. We’ll pursue\nthis direction briefly in Chapter 7 and other chapters, where we’ll clarify exactly what kind\nof mathematical claim “CogPrime has the potential for human-level intelligence” turns out to\nbe. Once this has been clarified, it will be clear that current mathematical knowledge does not\nyet let us evaluate, or even fully formalize, this kind of claim. Perhaps one day rigorous and\ndetailed analyses of practical AGI designs will be feasible – and we look forward to that day –\nbut it’s not here yet.\nAlso, it would of course be profoundly exciting if we could offer dramatic practical demonstrations\nof CogPrime’s capabilities. We do have a partial software implementation, in the\nOpenCogPrime system, but currently the things OpenCogPrime does are too simple to really\n1.5 Extraordinary Proof? 5\nserve as proofs of CogPrime’s power for advanced AGI. We have used some CogPrime ideas in\nthe OpenCog framework to do things like natural language understanding and data mining, and\nto control virtual dogs in online virtual worlds; and this has been very useful work in multiple\nsenses. It has taught us more about the CogPrime design; it has produced some useful software\nsystems; and it constitutes fractional work building toward a full OpenCog based implementation\nof CogPrime. However, to date, the things OpenCogPrime has done are all things that\ncould have been done in different ways without the CogPrime architecture (though perhaps not\nas elegantly nor with as much room for interesting expansion).\nThe bottom line is that building an AGI is a big job. Software companies like Microsoft spend\ndozens to hundreds of man-years building software products like word processors and operating\nsystems, so it should be no surprise that creating a digital intelligence is also a relatively largescale\nsoftware engineering project. As time advances and software tools improve, the number of\nman-hours required to develop advanced AGI gradually decreases – but right now, as we write\nthese words, it’s still a rather big job. In the OpenCogPrime project we are making a serious\nattempt to create a CogPrime based AGI using an open-source development methodology,\nwith the open-source Linux operating system as one of our inspirations. But the open-source\nmethodology doesn’t work magic either, and it remains a large project, currently at an early\nstage. I emphasize this point so that readers lacking software engineering expertise don’t take\nthe currently fairly limited capabilities of OpenCogPrime as somehow a damning indictment of\nthe potential of the CogPrime design. The design is one thing, the implementation another –\nand the OpenCogPrime implementation currently encompasses perhaps one third to one half\nof the key ideas in this book.\nSo we don’t have extraordinary proof to offer. What we aim to offer instead are clearlyconstructed\nconceptual and technical arguments as to why we think the CogPrime design has\ndramatic AGI potential.\nIt is also possible to push back a bit on the common intuition that having a design for humanlevel\nAGI is such an “extraordinary claim.” It may be extraordinary relative to contemporary\nscience and culture, but we have a strong feeling that the AGI problem is not difficult in the\nsame ways that most people (including most AI researchers) think it is. We suspect that in\nhindsight, after human-level AGI has been achieved, people will look back in shock that it took\nhumanity so long to come up with a workable AGI design. As you’ll understand once you’ve\nfinished Part 1 of the book, we don’t think general intelligence is nearly as “extraordinary”\nand mysterious as it’s commonly made out to be. Yes, building a thinking machine is hard –\nbut humanity has done a lot of other hard things before. It may seem difficult to believe that\nhuman-level general intelligence could be achieved by something as simple as a collection of\nalgorithms linked together in an appropriate way and used to control an agent. But we suggest\nthat, once the first powerful AGI systems are produced, it will become apparent that engineering\nhuman-level minds is not so profoundly different from engineering other complex systems.\nAll in all, we’ll consider the book successful if a significant percentage of open-minded,\nappropriately-educated readers come away from it scratching their chins and pondering: “Hmm.\nYou know, that just might work.” and a small percentage come away thinking \"Now that’s an\ninitiative I’d really like to help with!\".\n6 1 Introduction\n1.6 Potential Approaches to AGI\nIn principle, there is a large number of approaches one might take to building an AGI, starting\nfrom the knowledge, software and machinery now available. This is not the place to review\nthem in detail, but a brief list seems apropos, including commentary on why these are not the\napproaches we have chosen for our own research. Our intent here is not to insult or dismiss\nthese other potential approaches, but merely to indicate why, as researchers with limited time\nand resources, we have made a different choice regarding where to focus our own energies.\n1.6.1 Build AGI from Narrow AI\nMost of the AI programs around today are “narrow AI” programs – they carry out one particular\nkind of task intelligently. One could try to make an advanced AGI by combining a bunch of\nenhanced narrow AI programs inside some kind of overall framework.\nHowever, we’re rather skeptical of this approach because none of these narrow AI programs\nhave the ability to generalize across domains – and we don’t see how combining them or extending\nthem is going to cause this to magically emerge.\n1.6.2 Enhancing Chatbots\nOne could seek to make an advanced AGI by taking a chatbot, and trying to improve its code\nto make it actually understand what it’s talking about. We have some direct experience with\nthis route, as in 2010 our AI consulting firm was contracted to improve Ray Kurzweil’s online\nchatbot \"Ramona\". Our new Ramona understands a lot more than the previous Ramona version\nor a typical chatbot, due to using Wikipedia and other online resources, but still it’s far from\nan AGI.\nA more ambitious attempt in this direction was Jason Hutchens’ a-i.com project, which\nsought to create a human child level AGI via development and teaching of a statistical learning\nbased chatbot (rather than the typical rule-based kind). The difficulty with this approach,\nhowever, is that the architecture of a chatbot is fundamentally different from the architecture\nof a generally intelligent mind. Much of what’s important about the human mind is not directly\nobservable in conversations, so if you start from conversation and try to work toward an AGI\narchitecture from there, you’re likely to miss many critical aspects.\n1.6.3 Emulating the Brain\nOne can approach AGI by trying to figure out how the brain works, using brain imaging and\nother tools from neuroscience, and then emulating the brain in hardware or software.\nOne rather substantial problem with this approach is that we don’t really understand how\nthe brain works yet, because our software for measuring the brain is still relatively crude. There\nis no brain scanning method that combines high spatial and temporal accuracy, and none is\n1.6 Potential Approaches to AGI 7\nlikely to come about for a decade or two. So to do brain-emulation AGI seriously, one needs to\nwait a while until brain scanning technology improves.\nCurrent AI methods like neural nets that are loosely based on the brain, are really not brainlike\nenough to make a serious claim at emulating the brain’s approach to general intelligence.\nWe don’t yet have any real understanding of how the brain represents abstract knowledge, for\nexample, or how it does reasoning (though the authors, like many others, have made some\nspeculations in this regard [GMIH08]).\nAnother problem with this approach is that once you’re done, what you get is something\nwith a very humanlike mind, and we already have enough of those! However, this is perhaps\nnot such a serious objection, because a digital-computer-based version of a human mind could\nbe studied much more thoroughly than a biology-based human mind. We could observe its\ndynamics in real-time in perfect precision, and could then learn things that would allow us to\nbuild other sorts of digital minds.\n1.6.4 Evolve an AGI\nAnother approach is to try to run an evolutionary process inside the computer, and wait for\nadvanced AGI to evolve.\nOne problem with this is that we don’t know how evolution works all that well. There’s a\nfield of artificial life, but so far its results have been fairly disappointing. It’s not yet clear how\nmuch one can vary on the chemical structures that underly real biology, and still get powerful\nevolution like we see in real biology. If we need good artificial chemistry to get good artificial\nbiology, then do we need good artificial physics to get good artificial chemistry?\nAnother problem with this approach, of course, is that it might take a really long time.\nEvolution took billions of years on Earth, using a massive amount of computational power. To\nmake the evolutionary approach to AGI effective, one would need some radical innovations to\nthe evolutionary process (such as, perhaps, using probabilistic methods like BOA [Pel05] or\nMOSES [Loo06] in place of traditional evolution).\n1.6.5 Derive an AGI design mathematically\nOne can try to use the mathematical theory of intelligence to figure out how to make advanced\nAGI.\nThis interests us greatly, but there’s a huge gap between the rigorous math of intelligence\nas it exists today and anything of practical value. As we’ll discuss in Chapter 7, most of the\nrigorous math of intelligence right now is about how to make AI on computers with dramatically\nunrealistic amounts of memory or processing power. When one tries to create a theoretical\nunderstanding of real-world general intelligence, one arrives at quite different sorts of considerations,\nas we will roughly outline in Chapter 10. Ideally we would like to be able to study the\nCogPrime design using a rigorous mathematical theory of real-world general intelligence, but at\nthe moment that’s not realistic. The best we can do is to conceptually analyze CogPrime and\nits various components in terms of relevant mathematical and theoretical ideas; and perform\nanalysis of CogPrime’s individual structures and components at varying levels of rigor.\n8 1 Introduction\n1.6.6 Use heuristic computer science methods\nThe computer science field contains a number of abstract formalisms, algorithms and structures\nthat have relevance beyond specific narrow AI applications, yet aren’t necessarily understood\nas thoroughly as would be required to integrate them into the rigorous mathematical theory of\nintelligence. Based on these formalisms, algorithms and structures, a number of \"single formalism/algorithm\nfocused\" AGI approaches have been outlined, some of which will be reviewed in\nChapter 4. For example Pei Wang’s NARS (”Non-Axiomatic Reasoning System”) approach is\nbased on a specific logic which he argues to be the \"logic of general intelligence\" – so, while his\nsystem contains many other aspects than this logic, he considers this logic to be the crux of the\nsystem and the source of its potential power as an AGI system.\nThe basic intuition on the part of these \"single formalism/algorithm focused\" researchers\nseems to be that there is one key formalism or algorithm underlying intelligence, and if you\nachieve this key aspect in your AGI program, you’re going to get something that fundamentally\nthinks like a person, even if it has some differences due to its different implementation and\nembodiment. On the other hand, it’s also possible that this idea is philosophically incorrect:\nthat there is no one key formalism, algorithm, structure or idea underlying general intelligence.\nThe CogPrime approach is based on the intuition that to achieve human-level, roughly humanlike\ngeneral intelligence based on feasible computational resources, one needs an appropriate\nheterogeneous combination of algorithms and structures, each coping with different types of\nknowledge and different aspects of the problem of achieving goals in complex environments.\n1.6.7 Integrative Cognitive Architecture\nFinally, to create advanced AGI one can try to build some sort of integrative cognitive architecture:\na software system with multiple components that each carry out some cognitive function,\nand that connect together in a specific way to try to yield overall intelligence.\nCognitive science gives us some guidance about the overall architecture, and computer science\nand neuroscience give us a lot of ideas about what to put in the different components. But still\nthis approach is very complex and there is a lot of need for creative invention.\nThis is the approach we consider most “serious” at present (at least until neuroscience advances\nfurther). And, as will be discussed in depth in these pages, this is the approach we’ve\nchosen: CogPrime is an integrative AGI architecture.\n1.6.8 Can Digital Computers Really Be Intelligent?\nAll the AGI approaches we’ve just mentioned assume that it’s possible to make AGI on digital\ncomputers. While we suspect this is correct, we must note that it isn’t proven.\nIt might be that – as Penrose [Pen96], Hameroff [Ham87] and others have argued – we need\nquantum computers or quantum gravity computers to make AGI. However, there is no evidence\nof this at this stage. Of course the brain like all matter is described by quantum mechanics,\nbut this doesn’t imply that the brain is a “macroscopic quantum system” in a strong sense\n(like, say, a Bose-Einstein condensate). And even if the brain does use quantum phenomena in\n1.7 Five Key Words 9\na dramatic way to carry out some of its cognitive processes (a hypothesis for which there is no\ncurrent evidence), this doesn’t imply that these quantum phenomena are necessary in order to\ncarry out the given cognitive processes. For example there is evidence that birds use quantum\nnonlocal phenomena to carry out navigation based on the Earth’s magnetic fields [GRM + 11];\nyet scientists have built instruments that carry out the same functions without using any special\nquantum effects. The importance of quantum phenomena in biology (except via their obvious\nrole in giving rise to biological phenomena describable via classical physics) remains a subject\nof debate [AGBD + 08].\nQuantum “magic” aside, it is also conceivable that building AGI is fundamentally impossible\nfor some other reason we don’t understand. Without getting religious about it, it is rationally\nquite possible that some aspects of the universe are beyond the scope of scientific methods.\nScience is fundamentally about recognizing patterns in finite sets of bits (e.g. finite sets of\nfinite-precision observations), whereas mathematics recognizes many sets much larger than this.\nSelmer Bringsjord [BZ03], and other advocates of “hypercomputing” approaches to intelligence,\nargue that the human mind depends on massively large infinite sets and therefore can never be\nsimulated on digital computers nor understood via finite sets of finite-precision measurements\nsuch as science deals with.\nBut again, while this sort of possibility is interesting to speculate about, there’s no real reason\nto believe it at this time. Brain science and AI are both very young sciences and the “working\nhypothesis” that digital computers can manifest advanced AGI has hardly been explored at\nall yet, relative to what will be possible in the next decades as computers get more and more\npowerful and our understanding of neuroscience and cognitive science gets more and more\ncomplete. The CogPrime AGI design presented here is based on this working hypothesis.\nMany of the ideas in the book are actually independent of the “mind can be implemented\ndigitally” working hypothesis, and could apply to AGI systems built on analog, quantum or\nother non-digital frameworks – but we will not pursue these possibilities here. For the moment,\noutlining an AGI design for digital computers is hard enough! Regardless of speculations about\nquantum computing in the brain, it seems clear that AGI on quantum computers is part of our\nfuture and will be a powerful thing; but the description of a CogPrime analogue for quantum\ncomputers will be left for a later work.\n1.7 Five Key Words\nAs noted, the CogPrime approach lies squarely in the integrative cognitive architecture camp.\nBut it is not a haphazard or opportunistic combination of algorithms and data structures. At\nbottom it is motivated by the patternist philosophy of mind laid out in Ben Goertzel’s book\nThe Hidden Pattern [Goe06a], which was in large part a summary and reformulation of ideas\npresented in a series of books published earlier by the same author [Goe94], [Goe93a], [Goe93b],\n[Goe97], [Goe01]. A few of the core ideas of this philosophy are laid out in Chapter 3, though\nthat chapter is by no means a thorough summary.\nOne way to summarize some of the most important yet commonsensical parts of the patternist\nphilosophy of mind, in an AGI context, is to list five words: perception, memory, prediction,\naction, goals.\nIn a phrase: “A mind uses perception and memory to make predictions about\nwhich actions will help it achieve its goals.”\n10 1 Introduction\nThis ties in with the ideas of many other thinkers, including Jeff Hawkins’ “memory/prediction”\ntheory [HB06], and it also speaks directly to the formal characterization of intelligence\npresented in Chapter 7: general intelligence as “the ability to achieve complex goals in complex\nenvironments.”\nNaturally the goals involved in the above phrase may be explicit or implicit to the intelligent\nagent, and they may shift over time as the agent develops.\nPerception is taken to mean pattern recognition: the recognition of (novel or familiar) patterns\nin the environment or in the system itself. Memory is the storage of already-recognized\npatterns, enabling recollection or regeneration of these patterns as needed. Action is the formation\nof patterns in the body and world. Prediction is the utilization of temporal patterns to\nguess what perceptions will be seen in the future, and what actions will achieve what effects in\nthe future – in essence, prediction consists of temporal pattern recognition, plus the (implicit\nor explicit) assumption that the universe possesses a \"habitual tendency\" according to which\npreviously observed patterns continue to apply.\n1.7.1 Memory and Cognition in CogPrime\nEach of these five concepts has a lot of depth to it, and we won’t say too much about them in\nthis brief introductory overview; but we will take a little time to say something about memory\nin particular.\nAs we’ll see in Chapter 7, one of the things that the mathematical theory of general intelligence\nmakes clear is that, if you assume your AI system has a huge amount of computational\nresources, then creating general intelligence is not a big trick. Given enough computing power,\na very brief and simple program can achieve any computable goal in any computable environment,\nquite effectively. Marcus Hutter’s AIXI tl design [Hut05] gives one way of doing this,\nbacked up by rigorous mathematics. Put informally, what this means is: the problem of AGI is\nreally a problem of coping with inadequate compute resources, just as the problem of natural\nintelligence is really a problem of coping with inadequate energetic resources.\nOne of the key ideas underlying CogPrime is a principle called cognitive synergy, which\nexplains how real-world minds achieve general intelligence using limited resources, by appropriately\norganizing and utilizing their memories.\nThis principle says that there are many different kinds of memory in the mind: sensory,\nepisodic, procedural, declarative, attentional, intentional. Each of them has certain learning\nprocesses associated with it; for example, reasoning is associated with declarative memory.\nSynergy arises here in the way the learning processes associated with each kind of memory have\ngot to help each other out when they get stuck, rather than working at cross-purposes.\nCognitive synergy is a fundamental principle of general intelligence – it doesn’t tend to play\na central role when you’re building narrow-AI systems.\nIn the CogPrime approach all the different kinds of memory are linked together in a single\nmeta-representation, a sort of combined semantic/neural network called the AtomSpace. It\nrepresents everything from perceptions and actions to abstract relationships and concepts and\neven a system’s model of itself and others. When specialized representations are used for other\ntypes of knowledge (e.g. program trees for procedural knowledge, spatiotemporal hierarchies\nfor perceptual knowledge) then the knowledge stored outside the AtomSpace is represented via\n1.8 Virtually and Robotically Embodied AI 11\ntokens (Atoms) in the AtomSpace, allowing it to be located by various cognitive processes, and\nassociated with other memory items of any type.\nSo for instance an OpenCog AI system has an AtomSpace, plus some specialized knowledge\nstores linked into the AtomSpace; and it also has specific algorithms acting on the AtomSpace\nand appropriate specialized stores corresponding to each type of memory. Each of these algorithms\nis complex and has its own story; for instance (an incomplete list, for more detail see\nthe following section of this Introduction):\n• Declarative knowledge is handled using Probabilistic Logic Networks, described in Chapter\n34 and others;\n• Procedural knowledge is handled using MOSES, a probabilistic evolutionary learning algorithm\ndescribed in Chapter 21 and others;\n• Attentional knowledge is handled by ECAN (economic attention allocation), described in\nChapter 23 and others;\n• OpenCog contains a language comprehension system called RelEx that takes English sentences\nand turns them into nodes and links in the AtomSpace. It’s currently being extended\nto handle Chinese. RelEx handles mostly declarative knowledge but also involves\nsome procedural knowledge for linguistic phenomena like reference resolution and semantic\ndisambiguation.\nBut the crux of the CogPrime cognitive architecture is not any particular cognitive process,\nbut rather the way they all work together using cognitive synergy.\n1.8 Virtually and Robotically Embodied AI\nAnother issue that will arise frequently in these pages is embodiment. There’s a lot of debate in\nthe AI community over whether embodiment is necessary for advanced AGI or not. Personally,\nwe doubt it’s necessary but we think it’s extremely convenient, and are thus considerably\ninterested in both virtual world and robotic embodiment. The CogPrime architecture itself is\nneutral on the issue of embodiment, and it could be used to build a mathematical theorem\nprover or an intelligent chat bot just as easily as an embodied AGI system. However, most of\nour attention has gone into figuring out how to use CogPrime to control embodied agents in\nvirtual worlds, or else (to a lesser extent) physical robots. For instance, during 2011-2012 we\nare involved in a Hong Kong government funded project using OpenCog to control video game\nagents in a simple game world modeled on the game Minecraft [GPC + 11].\nCurrent virtual world technology has significant limitations that make them far less than\nideal from an AGI perspective, and in Chapter 16 we will discuss how they can be remedied.\nHowever, for the medium-term future virtual worlds are not going to match the natural world\nin terms of richness and complexity – and so there’s also something to be said for physical\nrobots that interact with all the messiness of the real world.\nWith this in mind, in the Artificial Brain Lab at Xiamen University in 2009-2010, we conducted\nsome experiments using OpenCog to control the Nao humanoid robot [GD09]. The goal\nof that work was to take the same code that controls the virtual dog and use it to control the\nphysical robot. But it’s harder because in this context we need to do real vision processing\nand real motor control. A similar project is being undertaken in Hong Kong at time of writing,\ninvolving a collaboration between OpenCog AI developers and David Hanson’s robotics\n12 1 Introduction\ngroup. One of the key ideas involved in this project is explicit integration of subsymbolic and\nmore symbolic subsystems. For instance, one can use a purely subsymbolic, hierarchical pattern\nrecognition network for vision processing, and then link its internal structures into the nodes\nand links in the AtomSpace that represent concepts. So the subsymbolic and symbolic systems\ncan work harmoniously and productively together, a notion we will review in more detail in\nChapter 26.\n1.9 Language Learning\nOne of the subtler aspects of our current approach to teaching CogPrime is language learning.\nThree relatively crisp and simple approaches to language learning would be:\n• Build a language processing system using hand-coded grammatical rules, based on linguistic\ntheory;\n• Train a language processing system using supervised, unsupervised or semisupervised learning,\nbased on computational linguistics;\n• Have an AI system learn language via experience, based on imitation and reinforcement and\nexperimentation, without any built-in distinction between linguistic behaviors and other\nbehaviors.\nWhile the third approach is conceptually appealing, our current approach in CogPrime (described\nin a series of chapters in Part 2) is none of the above, but rather a combination of the\nabove. OpenCog contains a natural language processing system built using a combination of\nthe rule-based and statistical approaches, which has reasonably adequate functionality; and our\nplan is to use it as an initial condition for ongoing adaptive improvement based on embodied\ncommunicative experience.\n1.10 AGI Ethics\nWhen discussing AGI work with the general public, ethical concerns often arise. Science fiction\nfilms like the Terminator series have raised public awareness of the possible dangers of\nadvanced AGI systems without correspondingly advanced ethics. Non-profit organizations like\nthe Singularity Institute for AI ((http://singinst.org) have arisen specifically to raise attention\nabout, and foster research on, these potential dangers.\nOur main focus here is on how to create AGI, not how to teach an AGI human ethical\nprinciples. However, we will address the latter issue explicitly in Chapter 12, and we do think it’s\nimportant to emphasize that AGI ethics has been at the center of the design process throughout\nthe conception and development of CogPrime and OpenCog.\nBroadly speaking there are (at least) two major threats related to advanced AGI. One is\nthat people might use AGIs for bad ends; and the other is that, even if an AGI is made with\nthe best intentions, it might reprogram itself in a way that causes it to do something terrible.\nIf it’s smarter than us, we might be watching it carefully while it does this, and have no idea\nwhat’s going on.\n1.12 Key Claims of the Book 13\nThe best way to deal with this second “bad AGI” problem is to build ethics into your AGI\narchitecture – and we have done this with CogPrime, via creating a goal structure that explicitly\nsupports ethics-directed behavior, and via creating an overall architecture that supports “ethical\nsynergy” along with cognitive synergy. In short, the notion of ethical synergy is that there are\ndifferent kinds of ethical thinking associated with the different kinds of memory and you want\nto be sure your AGI has all of them, and that it uses them together effectively.\nIn order to create AGI that is not only intelligent but beneficial to other sentient beings,\nethics has got to be part of the design and the roadmap. As we teach our AGI systems, we need\nto lead them through a series of instructional and evaluative tasks that move from a primitive\nlevel to the mature human level – in intelligence, but also in ethical judgment.\n1.11 Structure of the Book\nThe book is divided into two parts. The technical particulars of CogPrime are discussed in Part\n2; what we deal with in Part 1 are important preliminary and related matters such as:\n• The nature of real-world general intelligence, both conceptually and from the perspective\nof formal modeling (Section I).\n• The nature of cognitive and ethical development for humans and AGIs (Section III).\n• The high-level properties of CogPrime, including the overall architecture and the various\nsorts of memory involved (Section IV).\n• What kind of path may viably lead us from here to AGI, with focus laid on preschool-type\nenvironments that easily foster humanlike cognitive development. Various advanced aspects\nof AGI systems, such as the network and algebraic structures that may emerge from them,\nthe ways in which they may self-modify, and the degree to which their initial design may\nconstrain or guide their future state even after long periods of radical self-improvement\n(Section V).\nOne point made repeatedly throughout Part 1, which is worth emphasizing here, is the current\nlack of a really rigorous and thorough general technical theory of general intelligence. Such a\ntheory, if complete, would be incredibly helpful for understanding complex AGI architectures\nlike CogPrime. Lacking such a theory, we must work on CogPrime and other such systems using\na combination of theory, experiment and intuition. This is not a bad thing, but it will be very\nhelpful if the theory and practice of AGI are able to grow collaboratively together.\n1.12 Key Claims of the Book\nWe will wrap up this Introduction with a systematic list of some of the key claims to be argued\nfor in these pages. Not all the terms and ideas in these claims have been mentioned in the\npreceding portions of this Introduction, but we hope they will be reasonably clear to the reader\nanyway, at least in a general sense. This list of claims will be revisited in Chapter 49 near the\nend of Part 2, where we will look back at the ideas and arguments that have been put forth in\nfavor of them, in the intervening chapters.\n14 1 Introduction\nIn essence this is a list of claims such that, if the reader accepts these claims, they should\nprobably accept that the CogPrime approach to AGI is a viable one. On the other hand if the\nreader rejects one or more of these claims, they may find one or more aspects of CogPrime\nunacceptable for some reason.\nWithout further ado, now, the claims:\n1. General intelligence (at the human level and ultimately beyond) can be achieved via creating\na computational system that seeks to achieve its goals, via using perception and memory\nto predict which actions will achieve its goals in the contexts in which it finds itself.\n2. To achieve general intelligence in the context of human-intelligence-friendly environments\nand goals using feasible computational resources, it’s important that an AGI system can\nhandle different kinds of memory (declarative, procedural, episodic, sensory, intentional,\nattentional) in customized but interoperable ways.\n3. Cognitive synergy: It’s important that the cognitive processes associated with different kinds\nof memory can appeal to each other for assistance in overcoming bottlenecks in a manner\nthat enables each cognitive process to act in a manner that is sensitive to the particularities\nof each others’ internal representations, and that doesn’t impose unreasonable delays on\nthe overall cognitive dynamics.\n4. As a general principle, neither purely localized nor purely global memory is sufficient for\ngeneral intelligence under feasible computational resources; “glocal” memory will be required.\n5. To achieve human-like general intelligence, it’s important for an intelligent agent to have\nsensory data and motoric affordances that roughly emulate those available to humans.\nWe don’t know exactly how close this emulation needs to be, which means that our AGI\nsystems and platforms need to support fairly flexible experimentation with virtual-world\nand/or robotic infrastructures.\n6. To work toward adult human-level, roughly human-like general intelligence, one fairly easily\ncomprehensible path is to use environments and goals reminiscent of human childhood, and\nseek to advance one’s AGI system along a path roughly comparable to that followed by\nhuman children.\n7. It is most effective to teach an AGI system aimed at roughly human-like general intelligence\nvia a mix of spontaneous learning and explicit instruction, and to instruct it via a\ncombination of imitation, reinforcement and correction, and a combination of linguistic and\nnonlinguistic instruction.\n8. One effective approach to teaching an AGI system human language is to supply it with\nsome in-built linguistic facility, in the form of rule-based and statistical-linguistics-based\nNLP systems, and then allow it to improve and revise this facility based on experience.\n9. An AGI system with adequate mechanisms for handling the key types of knowledge mentioned\nabove, and the capability to explicitly recognize large-scale patterns in itself, should,\nupon sustained interaction with an appropriate environment in pursuit of appropriate\ngoals, emerge a variety of complex structures in its internal knowledge network,\nincluding, but not limited to:\n• a hierarchical network, representing both a spatiotemporal hierarchy and an approximate\n“default inheritance” hierarchy, cross-linked\n• a heterarchical network of associativity, roughly aligned with the hierarchical network\n• a self network which is an approximate micro image of the whole network\n1.12 Key Claims of the Book 15\n• inter-reflecting networks modeling self and others, reflecting a “mirrorhouse” design\npattern\n10. Given the strengths and weaknesses of current and near-future digital computers,\na. A (loosely) neural-symbolic network is a good representation for directly storing many\nkinds of memory, and interfacing between those that it doesn’t store directly;\nb. Uncertain logic is a good way to handle declarative knowledge. To deal with the problems\nfacing a human-level AGI, an uncertain logic must integrate imprecise probability\nand fuzziness with a broad scope of logical constructs. PLN is one good realization.\nc. Programs are a good way to represent procedures (both cognitive and physical-action,\nbut perhaps not including low-level motor-control procedures).\nd. Evolutionary program learning is a good way to handle difficult program learning problems.\nProbabilistic learning on normalized programs is one effective approach to evolutionary\nprogram learning. MOSES is one good realization of this approach.\ne. Multistart hill-climbing, with a strong Occam prior, is a good way to handle relatively\nstraightforward program learning problems.\nf. Activation spreading and Hebbian learning comprise a reasonable way to handle attentional\nknowledge (though other approaches, with greater overhead cost, may provide\nbetter accuracy and may be appropriate in some situations).\n• Artificial economics is an effective approach to activation spreading and Hebbian\nlearning in the context of neural-symbolic networks;\n• ECAN is one good realization of artificial economics;\n• A good trade-off between comprehensiveness and efficiency is to focus on two kinds\nof attention: processor attention (represented in CogPrime by ShortTermImportance)\nand memory attention (represented in CogPrime by LongTermImportance).\ng. Simulation is a good way to handle episodic knowledge (remembered and imagined).\nRunning an internal world simulation engine is an effective way to handle simulation.\nh. Hybridization of one’s integrative neural-symbolic system with a spatiotemporally hierarchical\ndeep learning system is an effective way to handle representation and learning\nof low-level sensorimotor knowledge. DeSTIN is one example of a deep learning system\nof this nature that can be effective in this context.\ni. One effective way to handle goals is to represent them declaratively, and allocate attention\namong them economically. CogPrime’s PLN/ECAN based framework for handling\nintentional knowledge is one good realization.\n11. It is important for an intelligent system to have some way of recognizing large-scale patterns\nin itself, and then embodying these patterns as new, localized knowledge items in\nits memory. Given the use of a neural-symbolic network for knowledge representation, a\ngraph-mining based “map formation” heuristic is one good way to do this.\n12. Occam’s Razor: Intelligence is closely tied to the creation of procedures that achieve goals\nin environments in the simplest possible way. Each of an AGI system’s cognitive algorithms\nshould embody a simplicity bias in some explicit or implicit form.\n13. An AGI system, if supplied with a commonsensically ethical goal system and an intentional\ncomponent based on rigorous uncertain inference, should be able to reliably achieve a much\nhigher level of commonsensically ethical behavior than any human being.\n14. Once sufficiently advanced, an AGI system with a logic-based declarative knowledge approach\nand a program-learning-based procedural knowledge approach should be able to\n16 1 Introduction\nradically self-improve via a variety of methods, including supercompilation and automated\ntheorem-proving.\nSection I\nArtificial and Natural General Intelligence\n\nChapter 2\nWhat Is Human-Like General Intelligence?\n2.1 Introduction\nCogPrime, the AGI architecture on which the bulk of this book focuses, is aimed at the creation\nof artificial general intelligence that is vaguely human-like in nature, and possesses capabilities\nat the human level and ultimately beyond.\nObviously this description begs some foundational questions, such as, for starters: What is\n\"general intelligence\"? What is \"human-like general intelligence\"? What is \"intelligence\" at all?\nPerhaps in the future there will exist a rigorous theory of general intelligence which applies\nusefully to real-world biological and digital intelligences. In later chapters we will give some\nideas in this direction. But such a theory is currently nascent at best. So, given the present\nstate of science, these two questions about intelligence must be handled via a combination of\nformal and informal methods. This brief, informal chapter attempts to explain our view on the\nnature of intelligence in sufficient detail to place the discussion of CogPrime in appropriate\ncontext, without trying to resolve all the subtleties.\nPsychologists sometimes define human general intelligence using IQ tests and related instruments\n– so one might wonder: why not just go with that? But these sorts of intelligence testing\napproaches have difficulty even extending to humans from diverse cultures [HHPO12] [Fis01].\nSo it’s clear that to ground AGI approaches that are not based on precise modeling of human\ncognition, one requires a more fundamental understanding of the nature of general intelligence.\nOn the other hand, if one conceives intelligence too broadly and mathematically, there’s a risk\nof leaving the real human world too far behind. In this chapter (followed up in Chapters 9 and\n7 with more rigor), we present a highly abstract understanding of intelligence-in-general, and\nthen portray human-like general intelligence as a (particularly relevant) special case.\n2.1.1 What Is General Intelligence?\nMany attempts to characterize general intelligence have been made; Legg and Hutter [LH07a]\nreview over 70! Our preferred abstract characterization of intelligence is: the capability of a\nsystem to choose actions maximizing its goal-achievement, based on its perceptions\nand memories, and making reasonably efficient use of its computational resources\n19\n20 2 What Is Human-Like General Intelligence?\n[Goe10c]. A general intelligence is then understood as one that can do this for a variety of\ncomplex goals in a variety of complex environments.\nHowever, apart from positing definitions, it is difficult to say anything nontrivial about general\nintelligence in general. Marcus Hutter [Hut05] has demonstrated, using a characterization\nof general intelligence similar to the one above, that a very simple algorithm called AIXI tl can\ndemonstrate arbitrarily high levels of general intelligence, if given sufficiently immense computational\nresources. This is interesting because it shows that (if we assume the universe can\neffectively be modeled as a computational system) general intelligence is basically a problem of\ncomputational efficiency. The particular structures and dynamics that characterize real-world\ngeneral intelligences like humans arise because of the need to achieve reasonable levels of intelligence\nusing modest space and time resources.\nThe “patternist” theory of mind presented in [Goe06a] and briefly summarized in Chapter\n3 below presents a number of emergent structures and dynamics that are hypothesized to\ncharacterize pragmatic general intelligence, including such things as system-wide hierarchical\nand heterarchical knowledge networks, and a dynamic and self-maintaining self-model. Much of\nthe thinking underlying CogPrime has centered on how to make multiple learning components\ncombine to give rise to these emergent structures and dynamics.\n2.1.2 What Is Human-like General Intelligence?\nGeneral principles like “complex goals in complex environments” and patternism are not sufficient\nto specify the nature of human-like general intelligence. Due to the harsh reality of\ncomputational resource restrictions, real-world general intelligences are necessarily biased to\nparticular classes of environments. Human intelligence is biased toward the physical, social and\nlinguistic environments in which humanity evolved, and if AI systems are to possess humanlike\ngeneral intelligence they must to some extent share these biases.\nBut what are these biases, specifically? This is a large and complex question, which we seek\nto answer in a theoretically grounded way in Chapter 9. However, before turning to abstract\ntheory, one may also approach the question in a pragmatic way, by looking at the categories of\nthings that humans do to manifest their particular variety of general intelligence. This is the\ntask of the following section.\n2.2 Commonly Recognized Aspects of Human-like Intelligence\nIt would be nice if we could give some sort of “standard model of human intelligence” in this\nchapter, to set the context for our approach to artificial general intelligence – but the truth is\nthat there isn’t any. What the cognitive science field has produced so far is better described as:\na broad set of principles and platitudes, plus a long, loosely-organized list of ideas and results.\nChapter 5 below constitutes an attempt to present an integrative architecture diagram for\nhuman-like general intelligence, synthesizing the ideas of a number of different AGI and cognitive\ntheorists. However, though the diagram given there attempts to be inclusive, it nonetheless\ncontains many features that are accepted by only a plurality of the research community.\n2.2 Commonly Recognized Aspects of Human-like Intelligence 21\nThe following list of key aspects of human-like intelligence has a better claim at truly being\ngeneric and representing the consensus understanding of contemporary science. It was produced\nby a very simple method: starting with the Wikipedia page for cognitive psychology, and then\nadding a few items onto it based on scrutinizing the tables of contents of some top-ranked\ncognitive psychology textbooks. There is some redundancy among list items, and perhaps also\nsome minor omissions (depending on how broadly one construes some of the items), but the\npoint is to give a broad indication of human mental functions as standardly identified in the\npsychology field:\n• Perception\n– General perception\n– Psychophysics\n– Pattern recognition (the ability to correctly interpret ambiguous sensory information)\n– Object and event recognition\n– Time sensation (awareness and estimation of the passage of time)\n• Motor Control\n– Motor planning\n– Motor execution\n– Sensorimotor integration\n• Categorization\n– Category induction and acquisition\n– Categorical judgement and classification\n– Category representation and structure\n– Similarity\n• Memory\n– Aging and memory\n– Autobiographical memory\n– Constructive memory\n– Emotion and memory\n– False memories\n– Memory biases\n– Long-term memory\n– Episodic memory\n– Semantic memory\n– Procedural memory\n– Short-term memory\n– Sensory memory\n– Working memory\n• Knowledge representation\n– Mental imagery\n– Propositional encoding\n– Imagery versus propositions as representational mechanisms\n22 2 What Is Human-Like General Intelligence?\n– Dual-coding theories\n– Mental models\n• Language\n– Grammar and linguistics\n– Phonetics and phonology\n– Language acquisition\n• Thinking\n– Choice\n– Concept formation\n– Judgment and decision making\n– Logic, formal and natural reasoning\n– Problem solving\n– Planning\n– Numerical cognition\n– Creativity\n• Consciousness\n– Attention and Filtering (the ability to focus mental effort on specific stimuli whilst\nexcluding other stimuli from consideration)\n– Access consciousness\n– Phenomenal consciousness\n• Social Intelligence\n– Distributed Cognition\n– Empathy\nIf there’s nothing surprising to you in the above list, I’m not surprised! If you’ve read a\nbit in the modern cognitive science literature, the list may even seem trivial. But it’s worth\nreflecting that 50 years ago, no such list could have been produced with the same level of broad\nacceptance. And less than 100 years ago, the Western world’s scientific understanding of the\nmind was dominated by Freudian thinking; and not too long after that, by behaviorist thinking,\nwhich argued that theorizing about what went on inside the mind made no sense, and science\nshould focus entirely on analyzing external behavior. The progress of cognitive science hasn’t\nmade as many headlines as contemporaneous progress in neuroscience or computing hardware\nand software, but it’s certainly been dramatic. One of the reasons that AGI is more achievable\nnow than in the 1950s and 60s when the AI field began, is that now we understand the structures\nand processes characterizing human thinking a lot better.\nIn spite of all the theoretical and empirical progress in the cognitive science field, however,\nthere is still no consensus among experts on how the various aspects of intelligence in the above\n“human intelligence feature list” are achieved and interrelated. In these pages, however, for\nthe purpose of motivating CogPrime, we assume a broad integrative understanding roughly as\nfollows:\n• Perception: There is significant evidence that human visual perception occurs using a\nspatiotemporal hierarchy of pattern recognition modules, in which higher-level modules\n2.2 Commonly Recognized Aspects of Human-like Intelligence 23\ndeal with broader spacetime regions, roughly as in the DeSTIN AGI architecture discussed\nin Chapter 4. Further, there is evidence that each module carries out temporal predictive\npattern recognition as well as static pattern recognition. Audition likely utilizes a similar\nhierarchy. Olfaction may use something more like a Hopfield attractor neural network, as\ndescribed in Chapter 13. The networks corresponding to different sense modalities have\nmultiple cross-linkages, more at the upper levels than the lower, and also link richly into\nthe parts of the mind dealing with other functions.\n• Motor Control: This appears to be handled by a spatiotemporal hierarchy as well, in which\neach level of the hierarchy corresponds to higher-level (in space and time) movements. The\nhierarchy is very tightly linked in with the perceptual hierarchies, allowing sensorimotor\nlearning and coordination.\n• Memory: There appear to be multiple distinct but tightly cross-linked memory systems,\ncorresponding to different sorts of knowledge such as declarative (facts and beliefs), procedural,\nepisodic, sensorimotor, attentional and intentional (goals).\n• Knowledge Representation: There appear to be multiple base-level representational\nsystems; at least one corresponding to each memory system, but perhaps more than that.\nAdditionally there must be the capability to dynamically create new context-specific representational\nsystems founded on the base representational system.\n• Language: While there is surely some innate biasing in the human mind toward learning\ncertain types of linguistic structure, it’s also notable that language shares a great deal of\nstructure with other aspects of intelligence like social roles [CB00] and the physical world\n[Cas07]. Language appears to be learned based on biases toward learning certain types of\nrelational role systems; and language processing seems a complex mix of generic reasoning\nand pattern recognition processes with specialized acoustic and syntactic processing\nroutines.\n• Consciousness is pragmatically well-understood using Baars’ “global workspace” theory,\nin which a small subset of the mind’s content is summoned at each time into a “working\nmemory” aka “workspace” aka “attentional focus” where it is heavily processed and used to\nguide action selection.\n• Thinking is a diverse combination of processes encompassing things like categorization,\n(crisp and uncertain) reasoning, concept creation, pattern recognition, and others; these\nprocesses must work well with all the different types of memory and must effectively integrate\nknowledge in the global workspace with knowledge in long-term memory.\n• Social Intelligence seems closely tied with language and also with self-modeling; we model\nourselves in large part using the same specialized biases we use to help us model others.\nNone of the points in the above bullet list is particularly controversial, but neither are any\nof them universally agreed-upon by experts. However, in order to make any progress on AGI\ndesign one must make some commitments to particular cognition-theoretic understandings, at\nthis level and ultimately at more precise levels as well. Further, general philosophical analyses\nlike the patternist philosophy to be reviewed in the following chapter only provide limited\nguidance here. Patternism provides a filter for theories about specific cognitive functions – it\nrules out assemblages of cognitive-function-specific theories that don’t fit together to yield a\nmind that could act effectively as a pattern-recognizing, goal-achieving system with the right\ninternal emergent structures. But it’s not a precise enough filter to serve as a sole guide for\ncognitive theory even at the high level.\nThe above list of points leads naturally into the integrative architecture diagram presented\nin Chapter 5. But that generic architecture diagram is fairly involved, and before presenting\n24 2 What Is Human-Like General Intelligence?\nit, we will go through some more background regarding human-like intelligence (in the rest\nof this chapter), philosophy of mind (in Chapter 3) and contemporary AGI architectures (in\nChapter4).\n2.3 Further Characterizations of Humanlike Intelligence\nWe now present a few complementary approaches to characterizing the key aspects of humanlike\nintelligence, drawn from different perspectives in the psychology and AI literature. These\ndifferent approaches all overlap substantially, which is good, yet each gives a slightly different\nslant.\n2.3.1 Competencies Characterizing Human-like Intelligence\nFirst we give a list of key competencies characterizing human level intelligence resulting from\nthe the AGI Roadmap Workshop held at the University of Knoxville in October 2008 1 , which\nwas organized by Ben Goertzel and Itamar Arel. In this list, each broad competency area is\nlisted together with a number of specific competencies sub-areas within its scope:\n1. Perception: vision, hearing, touch, proprioception, crossmodal\n2. Actuation: physical skills, navigation, tool use\n3. Memory: episodic, declarative, behavioral\n4. Learning: imitation, reinforcement, interactive verbal instruction, written media, experimentation\n5. Reasoning: deductive, abductive, inductive, causal, physical, associational, categorization\n6. Planning: strategic, tactical, physical, social\n7. Attention: visual, social, behavioral\n8. Motivation: subgoal creation, affect-based motivation, control of emotions\n9. Emotion: expressing emotion, understanding emotion\n10. Self: self-awareness, self-control, other-awareness\n11. Social: empathy, appropriate social behavior, social communication, social inference, group\nplay, theory of mind\n12. Communication: gestural, pictorial, verbal, language acquisition, cross-modal\n13. Quantitative: counting, grounded arithmetic, comparison, measurement\n14. Building/Creation: concept formation, verbal invention, physical construction, social\ngroup formation\nClearly this list is getting at the same things as the textbook headings given in Section 2.2,\nbut with a different emphasis due to its origin among AGI researchers rather than cognitive\n1 See http://www.ece.utk.edu/~itamar/AGI_Roadmap.html; participants included: Sam Adams, IBM\nResearch; Ben Goertzel, Novamente LLC; Itamar Arel, University of Tennessee; Joscha Bach, Institute of Cognitive\nScience, University of Osnabruck, Germany; Robert Coop, University of Tennessee; Rod Furlan, Singularity\nInstitute; Matthias Scheutz, Indiana University; J. Storrs Hall, Foresight Institute; Alexei Samsonovich, George\nMason University; Matt Schlesinger, Southern Illinois University; John Sowa, Vivomind Intelligence, Inc.; Stuart\nC. Shapiro, University at Buffalo\n2.3 Further Characterizations of Humanlike Intelligence 25\npsychologists. As part of the AGI Roadmap project, specific tasks were created corresponding\nto each of the sub-areas in the above list; we will describe some of these tasks in Chapter 17.\n2.3.2 Gardner’s Theory of Multiple Intelligences\nThe diverse list of human-level “competencies” given above is reminiscent of Gardner’s [Gar99]\nmultiple intelligences (MI) framework – a psychological approach to intelligence assessment\nbased on the idea that different people have mental strengths in different high-level domains,\nso that intelligence tests should contain aspects that focus on each of these domains separately.\nMI does not contradict the “complex goals in complex environments” view of intelligence, but\nrather may be interpreted as making specific commitments regarding which complex tasks and\nwhich complex environments are most important for roughly human-like intelligence.\nMI does not seek an extreme generality, in the sense that it explicitly focuses on domains\nin which humans have strong innate capability as well as general-intelligence capability; there\ncould easily be non-human intelligences that would exceed humans according to both the commonsense\nhuman notion of “general intelligence” and the generic “complex goals in complex\nenvironments” or Hutter/Legg-style definitions, yet would not equal humans on the MI criteria.\nThis strong anthropocentrism of MI is not a problem from an AGI perspective so long as\none uses MI in an appropriate way, i.e. only for assessing the extent to which an AGI system\ndisplays specifically human-like general intelligence. This restrictiveness is the price one pays\nfor having an easily articulable and relatively easily implementable evaluation framework.\nTable ?? summarizes the types of intelligence included in Gardner’s MI theory.\nIntelligence Type\nLinguistic\nLogical-Mathematical\nMusical\nBodily-Kinesthetic\nSpatial-Visual\nInterpersonal\nAspects\nWords and language, written and spoken; retention, interpretation\nand explanation of ideas and information via language;\nunderstands relationship between communication\nand meaning\nLogical thinking, detecting patterns, scientific reasoning\nand deduction; analyse problems, perform mathematical\ncalculations, understands relationship between cause and\neffect towards a tangible outcome\nMusical ability, awareness, appreciation and use of sound;\nrecognition of tonal and rhythmic patterns, understands\nrelationship between sound and feeling\nBody movement control, manual dexterity, physical agility\nand balance; eye and body coordination\nVisual and spatial perception; interpretation and creation\nof images; pictorial imagination and expression; understands\nrelationship between images and meanings, and between\nspace and effect\nPerception of other people’s feelings; relates to others; interpretation\nof behaviour and communications; understands\nrelationships between people and their situations\nTable 2.1: Types of Intelligence in Gardner’s Multiple Intelligence Theory\n26 2 What Is Human-Like General Intelligence?\n2.3.3 Newell’s Criteria for a Human Cognitive Architecture\nFinally, another related perspective is given by Alan Newell’s “functional criteria for a human\ncognitive architecture” [New90], which require that a humanlike AGI system should:\n1. Behave as an (almost) arbitrary function of the environment\n2. Operate in real time\n3. Exhibit rational, i.e., effective adaptive behavior\n4. Use vast amounts of knowledge about the environment\n5. Behave robustly in the face of error, the unexpected, and the unknown\n6. Integrate diverse knowledge\n7. Use (natural) language\n8. Exhibit self-awareness and a sense of self\n9. Learn from its environment\n10. Acquire capabilities through development\n11. Arise through evolution\n12. Be realizable within the brain\nIn our view, Newell’s criterion 1 is poorly-formulated, for while universal Turing computing\npower is easy to come by, any finite AI system must inevitably be heavily adapted to some\nparticular class of environments for straightforward mathematical reasons [Hut05, GPI + 10].\nOn the other hand, his criteria 11 and 12 are not relevant to the CogPrime approach as we are\nnot doing biological modeling but rather AGI engineering. However, Newell’s criteria 2-10 are\nessential in our view, and all will be covered in the following chapters.\n2.3.4 intelligence and Creativity\nCreativity is a key aspect of intelligence. While sometimes associated especially with geniuslevel\nintelligence in science or the arts, actually creativity is pervasive throughout intelligence,\nat all levels. When a child makes a flying toy car by pasting paper bird wings on his toy car, and\nwhen a bird figures out how to use a curved stick to get a piece of food out of a difficult corner\n– this is creativity, just as much as the invention of a new physics theory or the design of a new\nfashion line. The very nature of intelligence – achieving complex goals in complex environments\n– requires creativity for its achievement, because the nature of complex environments and goals\nis that they are always unveiling new aspects, so that dealing with them involves inventing\nthings beyond what worked for previously known aspects.\nCogPrime contains a number of cognitive dynamics that are especially effective at creating\nnew ideas, such as: concept creation (which synthesizes new concepts via combining aspects\nof previous ones), probabilistic evolutionary learning (which simulates evolution by natural\nselection, creating new procedures via mutation, combination and probabilistic modeling based\non previous ones), and analogical inference (an aspect of the Probabilistic Logic Networks\nsubsystems). But ultimately creativity is about how a system combines all the processes at its\ndisposal to synthesize novel solutions to the problems posed by its goals in its environment.\nThere are times, of course, when the same goal can be achieved in multiple ways – some\nmore creative than others. In CogPrime this relates to the existence of multiple top-level goals,\none of which may be novelty. A system with novelty as one of its goals, alongside other more\n2.4 Preschool as a View into Human-like General Intelligence 27\nspecific goals, will have a tendency to solve other problems in creative ways, thus fulfilling its\nnovelty goal along with its other goals. This can be seen at the level of childlike behaviors, and\nalso at a much more advanced level. Salvador Dali wanted to depict his thoughts and feelings,\nbut he also wanted to do so in a striking and unusual way; this combination of aspirations\nspurred him to produce his amazing art. A child who is asked to draw a house, but has a\ngoal of novelty, may draw a tower with a swimming pool on the roof rather than a typical\nColonial structure. A physical motivated by novelty will seek a non-obvious solution to the\nequation at hand, rather than just applying tried and true methods, and perhaps discover\nsome new phenomenon. Novelty can be measured formally in terms of information-theoretic\nsurprisingness based upon a given basis of knowledge and experience [Sch06]; something that\nis novel and creative to a child may be familiar to the adult world, and a solution that seems\nnovel and creative to a brilliant scientist today, may seem like cliche’ elementary school level\nwork 100 years from now.\nMeasuring creativity is even more difficult and subjective than measuring intelligence. Qualitatively,\nhowever, we humans can recognize it; and we suspect that the qualitative emergence\nof dramatic, multidisciplinary computational creativity will be one of the things that makes the\nhuman population feel emotionally that advanced AGI has finally arrived.\n2.4 Preschool as a View into Human-like General Intelligence\nOne issue that arises when pursuing the grand goal of human-level general intelligence is how\nto measure partial progress. The classic Turing Test of imitating human conversation remains\ntoo difficult to usefully motivate immediate-term AI research (see [HF95] [Fre90] for arguments\nthat it has been counterproductive for the AI field). The same holds true for comparable alternatives\nlike the Robot College Test of creating a robot that can attend a semester of university\nand obtain passing grades. However, some researchers have suggested intermediary goals, that\nconstitute partial progress toward the grand goal and yet are qualitatively different from the\nhighly specialized problems to which most current AI systems are applied.\nIn this vein, Sam Adams and his team at IBM have outlined a so-called “Toddler Turing\nTest,” in which one seeks to use AI to control a robot qualitatively displaying similar cognitive\nbehaviors to a young human child (say, a 3 year old) [AABL02]. In fact this sort of idea has a\nlong and venerable history in the AI field – Alan Turing’s original 1950 paper on AI [Tur50],\nwhere he proposed the Turing Test, contains the suggestion that\n\"Instead of trying to produce a programme to simulate the adult mind,\nwhy not rather try to produce one which simulates the child’s?\"\nWe find this childlike cognition based approach promising for many reasons, including its integrative\nnature: what a young child does involves a combination of perception, actuation, linguistic\nand pictorial communication, social interaction, conceptual problem solving and creative\nimagination. Specifically, inspired by these ideas, in Chapter 16 we will suggest the approach\nof teaching and testing early-stage AGI systems in environments that emulate the preschools\nused for teaching human children.\nHuman intelligence evolved in response to the demands of richly interactive environments,\nand a preschool is specifically designed to be a richly interactive environment with the capability\nto stimulate diverse mental growth. So, we are currently exploring the use of CogPrime to control\n28 2 What Is Human-Like General Intelligence?\nvirtual agents in preschool-like virtual world environments, as well as commercial humanoid\nrobot platforms such as the Nao (see Figure 2.1) or Robokind (2.2) in physical preschool-like\nrobot labs.\nAnother advantage of focusing on childlike cognition is that child psychologists have created\na variety of instruments for measuring child intelligence. In Chapter 17, we will discuss an\napproach to evaluating the general intelligence of human childlike AGI systems via combining\ntests typically used to measure the intelligence of young human children, with additional tests\ncrafted based on cognitive science and the standard preschool curriculum.\nTo put it differently: While our long-term goal is the creation of genius machines with general\nintelligence at the human level and beyond, we believe that every young child has a certain\ngenius; and by beginning with this childlike genius, we can built a platform capable of developing\ninto a genius machine with far more dramatic capabilities.\n2.4.1 Design for an AGI Preschool\nMore precisely, we don’t suggest to place a CogPrime system in an environment that is an\nexact imitation of a human preschool – this would be inappropriate since current robotic or\nvirtual bodies are very differently abled than the body of a young human child. But we aim to\nplace CogPrime in an environment emulating the basic diversity and educational character of\na typical human preschool. We stress this now, at this early point in the book, because we will\nuse running examples throughout the book drawn from the preschool context.\nThe key notion in modern preschool design is the “learning center,” an area designed and\noutfitted with appropriate materials for teaching a specific skill. Learning centers are designed to\nencourage learning by doing, which greatly facilitates learning processes based on reinforcement,\nimitation and correction; and also to provide multiple techniques for teaching the same skills,\nto accommodate different learning styles and prevent overfitting and overspecialization in the\nlearning of new skills.\nCenters are also designed to cross-develop related skills. A “manipulatives center,” for example,\nprovides physical objects such as drawing implements, toys and puzzles, to facilitate\ndevelopment of motor manipulation, visual discrimination, and (through sequencing and classification\ngames) basic logical reasoning. A “dramatics center” cross-trains interpersonal and\nempathetic skills along with bodily-kinesthetic, linguistic, and musical skills. Other centers,\nsuch as art, reading, writing, science and math centers are also designed to train not just one\narea, but to center around a primary intelligence type while also cross-developing related areas.\nFor specific examples of the learning centers associated with particular contemporary preschools,\nsee [Nei98]. In many progressive, student-centered preschools, students are left largely to their\nown devices to move from one center to another throughout the preschool room. Generally,\neach center will be staffed by an instructor at some points in the day but not others, providing\na variety of learning experiences.\nTo imitate the general character of a human preschool, we will create several centers in our\nrobot lab. The precise architecture will be adapted via experience but initial centers will likely\nbe:\n• a blocks center: a table with blocks on it\n• a language center: a circle of chairs, intended for people to sit around and talk with the\nrobot\n2.5 Integrative and Synergetic Approaches to Artificial General Intelligence 29\n• a manipulatives center, with a variety of different objects of different shapes and sizes,\nintended to teach visual and motor skills\n• a ball play center: where balls are kept in chests and there is space for the robot to kick\nthe balls around\n• a dramatics center where the robot can observe and enact various movements\nOne Running Example\nAs we proceed through the various component structures and dynamics of CogPrime in the\nfollowing chapters, it will be useful to have a few running examples to use to explain how the\nvarious parts of the system are supposed to work. One example we will use fairly frequently is\ndrawn from the preschool context: the somewhat open-ended task of Build me something\nout of blocks, that you haven’t built for me before, and then tell me what it is. This\nis a relatively simple task that combines multiple aspects of cognition in a richly interconnected\nway, and is the sort of thing that young children will naturally do in a preschool setting.\n2.5 Integrative and Synergetic Approaches to Artificial General\nIntelligence\nIn Chapter 1 we characterized CogPrime as an integrative approach. And we suggest that the\nnaturalness of integrative approaches to AGI follows directly from comparing above lists of\ncapabilities and criteria to the array of available AI technologies. No single known algorithm\nor data structure appears easily capable of carrying out all these functions, so if one wants\nto proceed now with creating a general intelligence that is even vaguely humanlike, one must\nintegrate various AI technologies within some sort of unifying architecture.\nFor this reason and others, an increasing amount of work in the AI community these days\nis integrative in one sense or another. Estimation of Distribution Algorithms integrate probabilistic\nreasoning with evolutionary learning [Pel05]. Markov Logic Networks [RD06] integrate\nformal logic and probabilistic inference, as does the Probabilistic Logic Networks framework\n[GIGH08] utilized in CogPrime and explained further in the book, and other works in the\n“Progic” area such as [WW06]. Leslie Pack Kaelbling has synthesized low-level robotics methods\n(particle filtering) with logical inference [ZPK07]. Dozens of further examples could be given.\nThe construction of practical robotic systems like the Stanley system that won the DARPA\nGrand Challenge [Tea06] involve the integration of numerous components based on different\nprinciples. These algorithmic and pragmatic innovations provide ample raw materials for the\nconstruction of integrative cognitive architectures and are part of the reason why childlike AGI\nis more approachable now than it was 50 or even 10 years ago.\nFurther, many of the cognitive architectures described in the current AI literature are “integrative”\nin the sense of combining multiple, qualitatively different, interoperating algorithms.\nChapter 4 gives a high-level overview of existing cognitive architectures, dividing them into\nsymbolic, emergentist (e.g. neural network) and hybrid architectures. The hybrid architectures\ngenerally integrate symbolic and neural components, often with multiple subcomponents within\neach of these broad categories. However, we believe that even these excellent architectures are\nnot integrative enough, in the sense that they lack sufficiently rich and nuanced interactions\n30 2 What Is Human-Like General Intelligence?\nbetween the learning components associated with different kinds of memory, and hence are unlikely\nto give rise to the emergent structures and dynamics characterizing general intelligence.\nOne of the central ideas underlying CogPrime is that with an integrative cognitive architecture\nthat combines multiple aspects of intelligence, achieved by diverse structures and algorithms,\nwithin a common framework designed specifically to support robust synergetic interactions\nbetween these aspects.\nThe simplest way to create an integrative AI architecture is to loosely couple multiple components\ncarrying out various functions, in such a way that the different components pass inputs\nand outputs amongst each other but do not interfere with or modulate each others’ internal\nfunctioning in real-time. However, the human brain appears to be integrative in a much tighter\nsense, involving rich real-time dynamical coupling between various components with distinct\nbut related functions. In [Goe09a] we have hypothesized that the brain displays a property of\ncognitive synergy, according to which multiple learning processes can not only dispatch\nsubproblems to each other, but also share contextual understanding in real-time, so\nthat each one can get help from the others in a contextually savvy way. By imbuing AI architectures\nwith cognitive synergy, we hypothesize, one can get past the bottlenecks that have\nplagued AI in the past. Part of the reasoning here, as elaborated in Chapter 9 and [Goe09b], is\nthat real physical and social environments display a rich dynamic interconnection between their\nvarious aspects, so that richly dynamically interconnected integrative AI architectures will be\nable to achieve goals within them more effectively.\nAnd this brings us to the patternist perspective on intelligent systems, alluded to above and\nfleshed out further in Chapter 3 with its focus on the emergence of hierarchically and heterarchically\nstructured networks of patterns, and pattern-systems modeling self and others. Ultimately\nthe purpose of cognitive synergy in an AGI system is to enable the various AI algorithms and\nstructures composing the system to work together effectively enough to give rise to the right\nsystem-wide emergent structures characterizing real-world general intelligence. The underlying\ntheory is that intelligence is not reliant on any particular structure or algorithm, but is reliant\non the emergence of appropriately structured networks of patterns, which can then be used to\nguide ongoing dynamics of pattern recognition and creation. And the underlying hypothesis is\nthat the emergence of these structures cannot be achieved by a loosely interconnected assemblage\nof components, no matter how sensible the architecture; it requires a tightly connected,\nsynergetic system.\nIt is possible to make these theoretical ideas about cognition mathematically rigorous; for\ninstance, Appendix ?? briefly presents a formal definition of cognitive synergy that has been\nanalyzed as part of an effort to prove theorems about the importance of cognitive synergy for\ngiving rise to emergent system properties associated with general intelligence. However, while\nwe have found such formal analyses valuable for clarifying our designs and understanding their\nqualitative properties, we have concluded that, for the present, the best way to explore our\nhypotheses about cognitive synergy and human-like general intelligence is empirically – via\nbuilding and testing systems like CogPrime.\n2.5.1 Achieving Humanlike Intelligence via Cognitive Synergy\nSumming up: at the broadest level, there are four primary challenges in constructing an integrative,\ncognitive synergy based approach to AGI:\n2.5 Integrative and Synergetic Approaches to Artificial General Intelligence 31\n1. choosing an overall cognitive architecture that possesses adequate richness and flexibility\nfor the task of achieving childlike cognition.\n2. Choosing appropriate AI algorithms and data structures to fulfill each of the functions\nidentified in the cognitive architecture (e.g. visual perception, audition, episodic memory,\nlanguage generation, analogy,...)\n3. Ensuring that these algorithms and structures, within the chosen cognitive architecture,\nare able to cooperate in such a way as to provide appropriate coordinated, synergetic\nintelligent behavior (a critical aspect since childlike cognition is an integrated functional\nresponse to the world, rather than a loosely coupled collection of capabilities.)\n4. Embedding one’s system in an environment that provides sufficiently rich stimuli and\ninteractions to enable the system to use this cooperation to ongoingly, creatively develop\nan intelligent internal world-model and self-model.\nWe argue that CogPrime\nprovides a viable way to address these challenges.\n32 2 What Is Human-Like General Intelligence?\nFig. 2.1: The Nao humanoid robot\n2.5 Integrative and Synergetic Approaches to Artificial General Intelligence 33\nFig. 2.2: The Nao humanoid robot\n\nChapter 3\nA Patternist Philosophy of Mind\n3.1 Introduction\nIn the last chapter we discussed human intelligence from a fairly down-to-earth perspective,\nlooking at the particular intelligent functions that human beings carry out in their everyday\nlives. And we strongly feel this practical perspective is important: Without this concreteness, it’s\ntoo easy for AGI research to get distracted by appealing (or frightening) abstractions of various\nsorts. However, it’s also important to look at the nature of mind and intelligence from a more\ngeneral and conceptual perspective, to avoid falling into an approach that follows the particulars\nof human capability but ignores the deeper structures and dynamics of mind that ultimately\nallow human minds to be so capable. In this chapter we very briefly review some ideas from the\npatternist philosophy of mind, a general conceptual framework on intelligence which has\nbeen inspirational for many key aspects of the CogPrime design, and which has been ongoingly\ndeveloped by one of the authors (Ben Goertzel) during the last two decades (in a series of\npublications beginning in 1991, most recently The Hidden Pattern [Goe06a]). Some of the ideas\ndescribed are quite broad and conceptual, and are related to CogPrime only via serving as\ngeneral inspirations; others are more concrete and technical, and are actually utilized within\nthe design itself.\nCogPrime is an integrative design formed via the combination of a number of different\nphilosophical, scientific and engineering ideas. The success or failure of the design doesn’t depend\non any particular philosophical understanding of intelligence. In that sense, the more abstract\nnotions presented in this chapter should be considered “optional” rather than critical in a\nCogPrime context. However, due to the core role patternism has played in the development of\nCogPrime, understanding a few things about general patternist philosophy will be helpful for\nunderstanding CogPrime, even for those readers who are not philosophically inclined. Those\nreaders who are philosophically inclined, on the other hand, are urged to read The Hidden\nPattern and then interpret the particulars of CogPrime in this light.\n3.2 Some Patternist Principles\nThe patternist philosophy of mind is a general approach to thinking about intelligent systems.\nIt is based on the very simple premise that mind is made of pattern – and that a mind is a\n35\n36 3 A Patternist Philosophy of Mind\nsystem for recognizing patterns in itself and the world, critically including patterns regarding\nwhich procedures are likely to lead to the achievement of which goals in which contexts.\nPattern as the basis of mind is not in itself is a very novel idea; this concept is present, for\ninstance, in the 19th-century philosophy of Charles Peirce [Pei34], in the writings of contemporary\nphilosophers Daniel Dennett [Den91] and Douglas Hofstadter [Hof79, Hof96], in Benjamin\nWhorf’s [Who64] linguistic philosophy and Gregory Bateson’s [Bat79] systems theory of mind\nand nature. Bateson spoke of the Metapattern: “that it is pattern which connects.” In Goertzel’s\nwritings on philosophy of mind, an effort has been made to pursue this theme more thoroughly\nthan has been done before, and to articulate in detail how various aspects of human mind and\nmind in general can be well-understood by explicitly adopting a patternist perspective. 1\nIn the patternist perspective, \"pattern\" is generally defined as \"representation as something\nsimpler.\" Thus, for example, if one measures simplicity in terms of bit-count, then a program\ncompressing an image would be a pattern in that image. But if one uses a simplicity measure\nincorporating run-time as well as bit-count, then the compressed version may or may not be a\npattern in the image, depending on how one’s simplicity measure weights the two factors. This\ndefinition encompasses simple repeated patterns, but also much more complex ones. While\npattern theory has typically been elaborated in the context of computational theory, it is not\nintrinsically tied to computation; rather, it can be developed in any context where there is a\nnotion of \"representation\" or \"production\" and a way of measuring simplicity. One just needs\nto be able to assess the extent to which f represents or produces X, and then to compare the\nsimplicity of f and X; and then one can assess whether f is a pattern in X. A formalization of\nthis notion of pattern is given in [Goe06a] and briefly summarized at the end of this chapter.\nNext, in patternism the mind of an intelligent system is conceived as the (fuzzy) set of\npatterns in that system, and the set of patterns emergent between that system and other\nsystems with which it interacts. The latter clause means that the patternist perspective is\ninclusive of notions of distributed intelligence [Hut96]. Basically, the mind of a system is the\nfuzzy set of different simplifying representations of that system that may be adopted.\nIntelligence is conceived, similarly to in Marcus Hutter’s [Hut05] recent work (and as elaborated\ninformally in Chapter 2 above, and formally in Chapter 7 below), as the ability to achieve\ncomplex goals in complex environments; where complexity itself may be defined as the possession\nof a rich variety of patterns. A mind is thus a collection of patterns that is associated\nwith a persistent dynamical process that achieves highly-patterned goals in highly-patterned\nenvironments.\nAn additional hypothesis made within the patternist philosophy of mind is that reflection is\ncritical to intelligence. This lets us conceive an intelligent system as a dynamical system that\nrecognizes patterns in its environment and itself, as part of its quest to achieve complex goals.\nWhile this approach is quite general, it is not vacuous; it gives a particular structure to the\ntasks of analyzing and synthesizing intelligent systems. About any would-be intelligent system,\nwe are led to ask questions such as:\n• How are patterns represented in the system? That is, how does the underlying infrastructure\nof the system give rise to the displaying of a particular pattern in the system’s behavior?\n• What kinds of patterns are most compactly represented within the system?\n• What kinds of patterns are most simply learned?\n1 In some prior writings the term “psynet model of mind” has been used to refer to the application of patternist\nphilosophy to cognitive theory, but this term has been \"deprecated\" in recent publications as it seemed to\nintroduce more confusion than clarification.\n3.2 Some Patternist Principles 37\n• What learning processes are utilized for recognizing patterns?\n• What mechanisms are used to give the system the ability to introspect (so that it can\nrecognize patterns in itself)?\nNow, these same sorts of questions could be asked if one substituted the word “pattern” with\nother words like “knowledge” or “information”. However, we have found that asking these questions\nin the context of pattern leads to more productive answers, avoiding unproductive byways\nand also tying in very nicely with the details of various existing formalisms and algorithms for\nknowledge representation and learning.\nAmong the many kinds of patterns in intelligent systems, semiotic patterns are particularly\ninteresting ones. Peirce decomposed these into three categories:\n• iconic patterns, which are patterns of contextually important internal similarity between\ntwo entities (e.g. an iconic pattern binds a picture of a person to that person)\n• indexical patterns, which are patterns of spatiotemporal co-occurrence (e.g. an indexical\npattern binds a wedding dress and a wedding)\n• symbolic patterns, which are patterns indicating that two entities are often involved in\nthe same relationships (e.g. a symbolic pattern between the number “5” (the symbol) and\nvarious sets of 5 objects (the entities that the symbol is taken to represent))\nOf course, some patterns may span more than one of these semiotic categories; and there\nare also some patterns that don’t fall neatly into any of these categories. But the semiotic\npatterns are particularly important ones; and symbolic patterns have played an especially large\nrole in the history of AI, because of the radically different approaches different researchers have\ntaken to handling them in their AI systems. Mathematical logic and related formalisms provide\nsophisticated mechanisms for combining and relating symbolic patterns (“symbols”), and some\nAI approaches have focused heavily on these, sometimes more so than on the identification of\nsymbolic patterns in experience or the use of them to achieve practical goals. We will look fairly\ncarefully at these differences in Chapter 4.\nPursuing the patternist philosophy in detail leads to a variety of particular hypotheses and\nconclusions about the nature of mind. Following from the view of intelligence in terms of\nachieving complex goals in complex environments, comes a view in which the dynamics of\na cognitive system are understood to be governed by two main forces:\n• self-organization, via which system dynamics cause existing system patterns to give rise to\nnew ones\n• goal-oriented behavior, which will be defined more rigorously in Chapter 7, but basically\namounts to a system interacting with its environment in a way that appears like an attempt\nto maximize some reasonably simple function\nSelf-organized and goal-oriented behavior must be understood as cooperative aspects. If an\nagent is asked to build a surprising structure out of blocks and does so, this is goal-oriented.\nBut the agent’s ability to carry out this goal-oriented task will be greater if it has previously\nplayed around with blocks a lot in an unstructured, spontaneous way. And the “nudge toward\ncreativity” given to it by asking it to build a surprising blocks structure may cause it to explore\nsome novel patterns, which then feed into its future unstructured blocks play.\nBased on these concepts, as argued in detail in [Goe06a], several primary dynamical principles\nmay be posited, including:\n38 3 A Patternist Philosophy of Mind\n• Evolution , conceived as a general process via which patterns within a large population\nthereof are differentially selected and used as the basis for formation of new patterns, based\non some “fitness function” that is generally tied to the goals of the agent\n– Example: If trying to build a blocks structure that will surprise Bob, an agent may\nsimulate several procedures for building blocks structures in its “mind’s eye”, assessing\nfor each one the expected degree to which it might surprise Bob. The search through\nprocedure space could be conducted as a form of evolution, via an algorithm such as\nMOSES.\n• Autopoiesis: the process by which a system of interrelated patterns maintains its integrity,\nvia a dynamic in which whenever one of the patterns in the system begins to decrease in\nintensity, some of the other patterns increase their intensity in a manner that causes the\ntroubled pattern to increase in intensity again\n– Example: An agent’s set of strategies for building the base of a tower, and its set of\nstrategies for building the middle part of a tower, are likely to relate autopoietically. If\nthe system partially forgets how to build the base of a tower, then it may regenerate\nthis missing knowledge via using its knowledge about how to build the middle part\n(i.e., it knows it needs to build the base in a way that will support good middle parts).\nSimilarly if it partially forgets how to build the middle part, then it may regenerate this\nmissing knowledge via using its knowledge about how to build the base (i.e. it knows a\ngood middle part should fit in well with the sorts of base it knows are good).\n– This same sort of interdependence occurs between pattern-sets containing more than\ntwo elements\n– Sometimes (as in the above example) autopoietic interdependence in the mind is tied\nto interdependencies in the physical world, sometimes not.\n• Association. Patterns, when given attention, spread some of this attention to other patterns\nthat they have previously been associated with in some way. Furthermore, there is\nPeirce’s law of mind [Pei34], which could be paraphrased in modern terms as stating that\nthe mind is an associative memory network, whose dynamics dictate that every idea in\nthe memory is an active agent, continually acting on those ideas with which the memory\nassociates it.\n– Example: Building a blocks structure that resembles a tower, spreads attention to memories\nof prior towers the agents has seen, and also to memories of people the agent knows\nhave seen towers, and structures it has built at the same time as towers, structures that\nresemble towers in various respects, etc.\n• Differential attention allocation / credit assignment. Patterns that have been valuable\nfor goal-achievement are given more attention, and are encouraged to participate in\ngiving rise to new patterns.\n– Example: Perhaps in a prior instance of the task “build me a surprising structure out of\nblocks,” searching through memory for non-blocks structures that the agent has played\nwith has proved a useful cognitive strategy. In that case, when the task is posed to the\nagent again, it should tend to allocate disproportionate resources to this strategy.\n• Pattern creation. Patterns that have been valuable for goal-achievement are mutated and\ncombined with each other to yield new patterns.\n3.2 Some Patternist Principles 39\n– Example: Building towers has been useful in a certain context, but so has building\nstructures with a large number of triangles. Why not build a tower out of triangles?\nOr maybe a vaguely tower-like structure that uses more triangles than a tower easily\ncould?\n– Example: Building an elongated block structure resembling a table was successful in the\npast, as was building a structure resembling a very flat version of a chair. Generalizing,\nmaybe building distorted versions of furniture is good. Or maybe it is building distorted\nversion of any previously perceived objects that is good. Or maybe both, to different\ndegrees....\nNext, for a variety of reasons outlined in [Goe06a] it becomes appealing to hypothesize that the\nnetwork of patterns in an intelligent system must give rise to the following large-scale emergent\nstructures\n• Hierarchical network. Patterns are habitually in relations of control over other patterns that\nrepresent more specialized aspects of themselves.\n– Example: The pattern associated with “tall building” has some control over the pattern\nassociated with “tower”, as the former represents a more general concept ... and “tower”\nhas some control over “Eiffel tower”, etc.\n• Heterarchical network. The system retains a memory of which patterns have previously\nbeen associated with each other in any way.\n– Example: “Tower” and “snake” are distant in the natural pattern hierarchy, but may be\nassociatively/heterarchically linked due to having a common elongated structure. This\nheterarchical linkage may be used for many things, e.g. it might inspire the creative\nconstruction of a tower with a snake’s head.\n• Dual network. Hierarchical and heterarchical structures are combined, with the dynamics\nof the two structures working together harmoniously. Among many possible ways to hierarchically\norganize a set of patterns, the one used should be one that causes hierarchically\nnearby patterns to have many meaningful heterarchical connections; and of course, there\nshould be a tendency to search for heterarchical connections among hierarchically nearby\npatterns.\n– Example: While the set of patterns hierarchically nearby “tower” and the set of patterns\nheterarchically nearby “tower” will be quite different, they should still have more overlap\nthan random pattern-sets of similar sizes. So, if looking for something else heterarchically\nnear “tower”, using the hierarchical information about “tower” should be of some use,\nand vice versa.\n– In PLN, hierarchical relationships correspond to Atoms A and B so that InheritanceAB\nand InheritanceBA have highly dissimilar strength; and heterarchical relationships correspond\nto IntensionalSimilarity relationships. The dual network structure then arises\nwhen intensional and extensional inheritance approximately correlate with each other,\nso that inference about either kind of inheritance assists with figuring out about the\nother kind.\n• Self structure. A portion of the network of patterns forms into an approximate image of the\noverall network of patterns.\n40 3 A Patternist Philosophy of Mind\n– Example: Each time the agent builds a certain structure, it observes itself building\nthe structure, and its role as “builder of a tall tower” (or whatever the structure is)\nbecomes part of its self-model. Then when it is asked to build something new, it may\nconsult its self-model to see if it believes itself capable of building that sort of thing (for\ninstance, if it is asked to build something very large, its self-model may tell it that it\nlacks persistence for such projects, so it may reply “I can try, but I may wind up not\nfinishing it”).\nAs we proceed through the CogPrime design in the following pages, we will see how each\nof these abstract concepts arises concretely from CogPrime’s structures and algorithms. If the\ntheory of [Goe06a] is correct, then the success of CogPrime as a design will depend largely on\nwhether these high-level structures and dynamics can be made to emerge from the synergetic\ninteraction of CogPrime’s representation and algorithms, when they are utilized to control an\nappropriate agent in an appropriate environment.\n3.3 Cognitive Synergy\nNow we dig a little deeper and present a different sort of “general principle of feasible general\nintelligence”, already hinted in earlier chapters: the cognitive synergy principle 2 , which is both\na conceptual hypothesis about the structure of generally intelligent systems in certain classes of\nenvironments, and a design principle used to guide the design of CogPrime. Chapter 8 presents\na mathematical formalization of the notion of cognitive synergy; here we present the conceptual\nidea informally, which makes it more easily digestible but also more vague-sounding.\nWe will focus here on cognitive synergy specifically in the case of “multi-memory systems,”\nwhich we define as intelligent systems whose combination of environment, embodiment and\nmotivational system make it important for them to possess memories that divide into partially\nbut not wholly distinct components corresponding to the categories of:\n• Declarative memory\n– Examples of declarative knowledge: Towers on average are taller than buildings. I generally\nam better at building structures I imagine, than at imitating structures I’m shown\nin pictures.\n• Procedural memory (memory about how to do certain things)\n– Examples of procedural knowledge: Practical know-how regarding how to pick up an\nelongated rectangular block, or a square one. Know-how regarding when to approach\na problem by asking “What would one of my teachers do in this situation” versus by\nthinking through the problem from first principles.\n• Sensory and episodic memory\n– Example of sensory knowledge: memory of Bob’s face; memory of what a specific tall\nblocks tower looked like\n2 While these points are implicit in the theory of mind given in [Goe06a], they are not articulated in this\nspecific form there. So the material presented in this section is a new development within patternist philosophy,\ndeveloped since [Goe06a] in a series of conference papers such as [Goe09a].\n3.3 Cognitive Synergy 41\n– Example of episodic knowledge: memory of the situation in which the agent first met\nBob; memory of a situation in which a specific tall blocks tower was built\n• Attentional memory (knowledge about what to pay attention to in what contexts)\n– Example of attentional knowledge: When involved with a new person, it’s useful to pay\nattention to whatever that person looks at\n• Intentional memory (knowledge about the system’s own goals and subgoals)\n– Example of intentional knowledge: If my goal is to please some person whom I don’t\nknow that well, then a subgoal may be figuring out what makes that person smile.\nIn Chapter 9 below we present a detailed argument as to how the requirement for a multimemory\nunderpinning for general intelligence emerges from certain underlying assumptions\nregarding the measurement of the simplicity of goals and environments. Specifically we argue\nthat each of these memory types corresponds to certain modes of communication, so that intelligent\nagents which have to efficiently handle a sufficient variety of types of communication with\nother agents, are going to have to handle all these types of memory. These types of communication\noverlap and are often used together, which implies that the different memories and their\nassociated cognitive processes need to work together. The points made in this section do not\nrely on that argument regarding the relation of multiple memory types to the environmental\nsituation of multiple communication types. What they do rely on is the assumption that, in\nthe intelligence agent in question, the different components of memory are significantly but not\nwholly distinct. That is, there are significant “family resemblances” between the memories of a\nsingle type, yet there are also thoroughgoing connections between memories of different types.\nRepeating the above points in a slightly more organized manner and then extending them, the\nessential idea of cognitive synergy, in the context of multi-memory systems, may be expressed\nin terms of the following points\n1. Intelligence, relative to a certain set of environments, may be understood as the capability\nto achieve complex goals in these environments.\n2. With respect to certain classes of goals and environments, an intelligent system requires a\n“multi-memory” architecture, meaning the possession of a number of specialized yet interconnected\nknowledge types, including: declarative, procedural, attentional, sensory, episodic\nand intentional (goal-related). These knowledge types may be viewed as different sorts of\npatterns that a system recognizes in itself and its environment.\n3. Such a system must possess knowledge creation (i.e. pattern recognition / formation) mechanisms\ncorresponding to each of these memory types. These mechanisms are also called\n“cognitive processes.”\n4. Each of these cognitive processes, to be effective, must have the capability to recognize when\nit lacks the information to perform effectively on its own; and in this case, to dynamically\nand interactively draw information from knowledge creation mechanisms dealing with other\ntypes of knowledge\n5. This cross-mechanism interaction must have the result of enabling the knowledge creation\nmechanisms to perform much more effectively in combination than they would if operated\nnon-interactively. This is “cognitive synergy.”\nInteractions as mentioned in Points 4 and 5 in the above list are the real conceptual meat\nof the cognitive synergy idea. One way to express the key idea here, in an AI context, is that\n42 3 A Patternist Philosophy of Mind\nmost AI algorithms suffer from combinatorial explosions: the number of possible elements to\nbe combined in a synthesis or analysis is just too great, and the algorithms are unable to\nfilter through all the possibilities, given the lack of intrinsic constraint that comes along with\na “general intelligence” context (as opposed to a narrow-AI problem like chess-playing, where\nthe context is constrained and hence restricts the scope of possible combinations that needs\nto be considered). In an AGI architecture based on cognitive synergy, the different learning\nmechanisms must be designed specifically to interact in such a way as to palliate each others’\ncombinatorial explosions - so that, for instance, each learning mechanism dealing with a certain\nsort of knowledge, must synergize with learning mechanisms dealing with the other sorts of\nknowledge, in a way that decreases the severity of combinatorial explosion.\nOne prerequisite for cognitive synergy to work is that each learning mechanism must recognize\nwhen it is “stuck,” meaning it’s in a situation where it has inadequate information to\nmake a confident judgment about what steps to take next. Then, when it does recognize that\nit’s stuck, it may request help from other, complementary cognitive mechanisms.\n3.4 The General Structure of Cognitive Dynamics: Analysis and\nSynthesis\nWe have discussed the need for synergetic interrelation between cognitive processes corresponding\nto different types of memory ... and the general high-level cognitive dynamics that a mind\nmust possess (evolution, autopoiesis). The next step is to dig further into the nature of the cognitive\nprocesses associated with different memory types and how they give rise to the needed\nhigh-level cognitive dynamics. In this section we present a general theory of cognitive processes\nbased on a decomposition of cognitive processes into the two categories of analysis and synthesis,\nand a general formulation of each of these categories 3 .\nSpecifically we focus here on what we call focused cognitive processes; that is, cognitive\nprocesses that selectively focus attention on a subset of the patterns making up a mind. In\ngeneral these are not the only kind, there may also be global cognitive processes that act on\nevery pattern in a mind. An example of a global cognitive process in CogPrime is the basic\nattention allocation process, which spreads “importance” among all knowledge in the system’s\nmemory. Global cognitive processes are also important, but focused cognitive processes are\nsubtler to understand which is why we spend more time on them here.\n3.4.1 Component-Systems and Self-Generating Systems\nWe begin with autopoesis – and, more specifically, with the concept of a “component-system”,\nas described in George Kampis’s book Self-Modifying Systems in Biology and Cognitive Science\n[Kam91], and as modified into the concept of a “self-generating system” or SGS in Goertzel’s\nbook Chaotic Logic [Goe94]. Roughly speaking, a Kampis-style component-system consists of\na set of components that combine with each other to form other compound components. The\n3 While these points are highly compatible with theory of mind given in [Goe06a], they are not articulated there.\nThe material presented in this section is a new development within patternist philosophy, presented previously\nonly in the article [GPPG06].\n3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 43\nmetaphor Kampis uses is that of Lego blocks, combining to form bigger Lego structures. Compound\nstructures may in turn be combined together to form yet bigger compound structures.\nA self-generating system is basically the same concept as a component-system, but understood\nto be computable, whereas Kampis claims that component-systems are uncomputable.\nNext, in SGS theory there is also a notion of reduction (not present in the Lego metaphor):\nsometimes when components are combined in a certain way, a “reaction” happens, which may\nlead to the elimination of some of the components. One relevant metaphor here is chemistry.\nAnother is abstract algebra: for instance, if we combine a component f with its “inverse” component\nf −1 , both components are eliminated. Thus, we may think about two stages in the\ninteraction of sets of components: combination, and reduction. Reduction may be thought of\nas algebraic simplification, governed by a set of rules that apply to a newly created compound\ncomponent, based on the components that are assembled within it.\nFormally, suppose C 1 , C 2 , ... is the set of components present in a discrete-time componentsystem\nat time t. Then, the components present at time t+1 are a subset of the set of components\nof the form\nReduce(Join(C i (1), ..., C i (r)))\nwhere Join is a joining operation, and Reduce is a reduction operator. The joining operation\nis assumed to map tuples of components into components, and the reduction operator is assumed\nto map the space of components into itself. Of course, the specific nature of a component system\nis totally dependent on the particular definitions of the reduction and joining operators; in\nfollowing chapters we will specify these for the CogPrime system, but for the purpose of the\nbroader theoretical discussion in this section they may be left general.\nWhat is called the “cognitive equation” in Chaotic Logic [Goe94] is the case of a SGS where\nthe patterns in the system at time t have a tendency to correspond to components of the system\nat future times t + s. So, part of the action of the system is to transform implicit knowledge\n(patterns among system components) into explicit knowledge (specific system components). We\nwill see one version of this phenomenon in Chapter 14 where we model implicit knowledge using\nmathematical structures called “derived hypergraphs”; and we will also later review several ways\nin which CogPrime’s dynamics explicitly encourage cognitive-equation type dynamics, e.g.:\n• inference, which takes conclusions implicit in the combination of logical relationships, and\nmakes them implicit by deriving new logical relationships from them\n• map formation, which takes concepts that have often been active together, and creates new\nconcepts grouping them\n• association learning, which creates links representing patterns of association between entities\n• probabilistic procedure learning, which creates new models embodying patterns regarding\nwhich procedures tend to perform well according to particular fitness functions\n3.4.2 Analysis and Synthesis\nNow we move on to the main point of this section: the argument that all or nearly all focused\ncognitive processes are expressible using two general process-schemata we call synthesis and\n44 3 A Patternist Philosophy of Mind\nanalysis 4 . The notion of “focused cognitive process” will be exemplified more thoroughly below,\nbut in essence what is meant is a cognitive process that begins with a small number of items\n(drawn from memory) as its focus, and has as its goal discovering something about these\nitems, or discovering something about something else in the context of these items or in a way\nstrongly biased by these items. This is different from a global cognitive process whose goal is\nmore broadly-based and explicitly involves all or a large percentage of the knowledge in an\nintelligent system’s memory store.\nAmong the focused cognitive processes are those governed by the so-called cognitive schematic\nimplication\nContext ∧ P rocedure → Goal\nwhere the Context involves sensory, episodic and/or declarative knowledge; and attentional\nknowledge is used to regulate how much resource is given to each such schematic implication in\nmemory. Synergy among the learning processes dealing with the context, the procedure and the\ngoal is critical to the adequate execution of the cognitive schematic using feasible computational\nresources. This sort of explicitly goal-driven cognition plays a significant though not necessarily\ndominant role in CogPrime, and is also related to production rules systems and other traditional\nAI systems, as will be articulated in Chapter 4.\nThe synthesis and analysis processes as we conceive them, in the general framework of SGS\ntheory, are as follows. First, synthesis, as shown in Figure 3.1, is defined as\nsynthesis: Iteratively build compounds from the initial component pool using the combinators,\ngreedily seeking compounds that seem likely to achieve the goal.\nOr in more detail:\n1. Begin with some initial components (the initial “current pool”), an additional set of components\nidentified as “combinators” (combination operators), and a goal function\n2. Combine the components in the current pool, utilizing the combinators, to form product\ncomponents in various ways, carrying out reductions as appropriate, and calculating relevant\nquantities associated with components as needed\n3. Select the product components that seem most promising according to the goal function,\nand add these to the current pool (or else simply define these as the current pool)\n4. Return to Step 2\nAnd analysis, as shown in Figure 3.2, is defined as\nanalysis: Iteratively search (the system’s long-term memory) for component-sets that combine\nusing the combinators to form the initial component pool (or subsets thereof), greedily\nseeking component-sets that seem likely to achieve the goal\nor in more detail:\n1. Begin with some components (the initial “current pool”) and a goal function\n2. Seek components so that, if one combines them to form product components using the\ncombinators and then performs appropriate reductions, one obtains (as many as possible\nof) the components in the current pool\n4 In [GPPG06], what is here called “analysis” was called “backward synthesis”, a name which has some advantages\nsince it indicated that what’s happening is a form of creation; but here we have opted for the more traditional\nanalysis/synthesis terminology\n3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 45\nFig. 3.1: The General Process of Synthesis\n3. Use the newly found constructions of the components in the current pool, to update the\nquantitative properties of the components in the current pool, and also (via the current\npool) the quantitative properties of the components in the initial pool\n4. Out of the components found in Step 2, select the ones that seem most promising according\nto the goal function, and add these to the current pool (or else simply define these as the\ncurrent pool)\n5. Return to Step 2\nMore formally, synthesis may be specified as follows. Let X denote the set of combinators,\nand let Y 0 denote the initial pool of components (the initial focus of the cognitive process).\nGiven Y i , let Z i denote the set\nReduce(Join(C i (1), ..., C i (r)))\nwhere the C i are drawn from Y i or from X. We may then say\nY i+1 = F ilter(Z i )\nwhere F ilter is a function that selects a subset of its arguments.\nAnalysis, on the other hand, begins with a set W of components, and a set X of combinators,\nand tries to find a series Y i so that according to the process of synthesis, Y n =W .\nIn practice, of course, the implementation of a synthesis process need not involve the explicit\nconstruction of the full set Z i . Rather, the filtering operation takes place implicitly during the\nconstruction of Y i+1 . The result, however, is that one gets some subset of the compounds producible\nvia joining and reduction from the set of components present in Y i plus the combinators\nX.\n46 3 A Patternist Philosophy of Mind\nFig. 3.2: The General Process of Analysis\nConceptually one may view synthesis as a very generic sort of “growth process,” and analysis\nas a very generic sort of “figuring out how to grow something.” The intuitive idea underlying\nthe present proposal is that these forward-going and backward-going “growth processes” are\namong the essential foundations of cognitive control, and that a conceptually sound design for\ncognitive control should explicitly make use of this fact. To abstract away from the details,\nwhat these processes are about is:\n• taking the general dynamic of compound-formation and reduction as outlined in Kampis\nand Chaotic Logic\n• introducing goal-directed pruning (“filtering”) into this dynamic so as to account for the\nlimitations of computational resources that are a necessary part of pragmatic intelligence\n3.4.3 The Dynamic of Iterative Analysis and Synthesis\nWhile synthesis and analysis are both very useful on their own, they achieve their greatest power\nwhen harnessed together. It is my hypothesis that the dynamic pattern of alternating synthesis\nand analysis has a fundamental role in cognition. Put simply, synthesis creates new mental\nforms by combining existing ones. Then, analysis seeks simple explanations for the forms in the\nmind, including the newly created ones; and, this explanation itself then comprises additional\nnew forms in the mind, to be used as fodder for the next round of synthesis. Or, to put it yet\nmore simply:\n3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 47\n⇒ Combine ⇒ Explain ⇒ Combine ⇒ Explain ⇒ Combine ⇒\nIt is not hard to express this alternating dynamic more formally, as well.\n• Let X denote any set of components.\n• Let F(X) denote a set of components which is the result of synthesis on X.\n• Let B(X) denote a set of components which is the result of analysis of X. We assume also\na heuristic biasing the synthesis process toward simple constructs.\n• Let S(t) denote a set of components at time t, representing part of a system’s knowledge\nbase.\n• Let I(t) denote components resulting from the external environment at time t.\nThen, we may consider a dynamical iteration of the form\nS(t + 1) = B(F (S(t) + I(t)))\nThis expresses the notion of alternating synthesis and analysis formally, as a dynamical\niteration on the space of sets of components. We may then speak about attractors of this\niteration: fixed points, limit cycles and strange attractors. One of the key hypotheses I wish\nto put forward here is that some key emergent cognitive structures are strange attractors of\nthis equation. The iterative dynamic of combination and explanation leads to the emergence\nof certain complex structures that are, in essence, maintained when one recombines their parts\nand then seeks to explain the recombinations. These structures are built in the first place\nthrough iterative recombination and explanation, and then survive in the mind because they\nare conserved by this process. They then ongoingly guide the construction and destruction of\nvarious other temporary mental structures that are not so conserved.\n3.4.4 Self and Focused Attention as Approximate Attractors of the\nDynamic of Iterated Forward-Analysis\nAs noted above, patternist philosophy argues that two key aspects of intelligence are emergent\nstructures that may be called the “self” and the “attentional focus.” These, it is suggested, are\naspects of intelligence that may not effectively be wired into the infrastructure of an intelligent\nsystem, though of course the infrastructure may be configured in such a way as to encourage\ntheir emergence. Rather, these aspects, by their nature, are only likely to be effective if they\nemerge from the cooperative activity of various cognitive processes acting within a broad base\nof knowledge.\nAbove we have described the pattern of ongoing habitual oscillation between synthesis and\nanalysis as a kind of “dynamical iteration.” Here we will argue that both self and attentional\nfocus may be viewed as strange attractors of this iteration. The mode of argument is relatively\ninformal. The essential processes under consideration are ones that are poorly understood from\nan empirical perspective, due to the extreme difficulty involved in studying them experimentally.\nFor understanding self and attentional focus, we are stuck in large part with introspection, which\nis famously unreliable in some contexts, yet still dramatically better than having no information\nat all. So, the philosophical perspective on self and attentional focus given here is a synthesis of\nempirical and introspective notions, drawn largely from the published thinking and research of\n48 3 A Patternist Philosophy of Mind\nothers but with a few original twists. From a CogPrime perspective, its use has been to guide\nthe design process, to provide a grounding for what otherwise would have been fairly arbitrary\nchoices.\n3.4.4.1 Self\nAnother high-level intelligent system pattern mentioned above is the “self”, which we here will tie\nin with analysis and synthesis processes. The term “self” as used here refers to the “phenomenal\nself” [Met04] or “self-model”. That is, the self is the model that a system builds internally,\nreflecting the patterns observed in the (external and internal) world that directly pertain to\nthe system itself. As is well known in everyday human life, self-models need not be completely\naccurate to be useful; and in the presence of certain psychological factors, a more accurate\nself-model may not necessarily be advantageous. But a self-model that is too badly inaccurate\nwill lead to a badly-functioning system that is unable to effectively act toward the achievement\nof its own goals.\nThe value of a self-model for any intelligent system carrying out embodied agentive cognition\nis obvious. And beyond this, another primary use of the self is as a foundation for metaphors\nand analogies in various domains. Patterns recognized pertaining to the self are analogically\nextended to other entities. In some cases this leads to conceptual pathologies, such as the anthropomorphization\nof trees, rocks and other such objects that one sees in some precivilized\ncultures. But in other cases this kind of analogy leads to robust sorts of reasoning - for instance,\nin reading Lakoff and Nunez’s [LN00] intriguing explorations of the cognitive foundations of\nmathematics, it is pretty easy to see that most of the metaphors on which they hypothesize\nmathematics to be based, are grounded in the mind’s conceptualization of itself as a spatiotemporally\nembedded entity, which in turn is predicated on the mind’s having a conceptualization\nof itself (a self) in the first place.\nA self-model can in many cases form a self-fulfilling prophecy (to make an obvious doubleentendre’!).\nActions are generated based on one’s model of what sorts of actions one can and/or\nshould take; and the results of these actions are then incorporated into one’s self-model. If a\nself-model proves a generally bad guide to action selection, this may never be discovered, unless\nsaid self-model includes the knowledge that semi-random experimentation is often useful.\nIn what sense, then, may it be said that self is an attractor of iterated analysis? Analysis\ninfers the self from observations of system behavior. The system asks: What kind of system\nmight I be, in order to give rise to these behaviors that I observe myself carrying out? Based\non asking itself this question, it constructs a model of itself, i.e. it constructs a self. Then, this\nself guides the system’s behavior: it builds new logical relationships its self-model and various\nother entities, in order to guide its future actions oriented toward achieving its goals. Based on\nthe behaviors newly induced via this constructive, forward-synthesis activity, the system may\nthen engage in analysis again and ask: What must I be now, in order to have carried out these\nnew actions? And so on.\nOur hypothesis is that after repeated iterations of this sort, in infancy, finally during early\nchildhood a kind of self-reinforcing attractor occurs, and we have a self-model that is resilient\nand doesn’t change dramatically when new instances of action- or explanation-generation occur.\nThis is not strictly a mathematical attractor, though, because over a long period of time the self\nmay well shift significantly. But, for a mature self, many hundreds of thousands or millions of\nforward-analysis cycles may occur before the self-model is dramatically modified. For relatively\n3.4 The General Structure of Cognitive Dynamics: Analysis and Synthesis 49\nlong periods of time, small changes within the context of the existing self may suffice to allow\nthe system to control itself intelligently.\nHumans can also develop what are known as subselves [Row90]. A subself is a partially\nautonomous self-network focused on particular tasks, environments or interactions. It contains\na unique model of the whole organism, and generally has its own set of episodic memories,\nconsisting of memories of those intervals during which it was the primary dynamic mode controlling\nthe organism. One common example is the creative subself – the subpersonality that\ntakes over when a creative person launches into the process of creating something. In these\ntimes, a whole different personality sometimes emerges, with a different sort of relationship\nto the world. Among other factors, creativity requires a certain open-ness that is not always\nproductive in an everyday life context, so it’s natural for the self-system of a highly creative\nperson to bifurcate into one self-system for everyday life, and another for the protected context\nof creative activity. This sort of phenomenon might emerge naturally in CogPrime systems as\nwell if they were exposed to appropriate environments and social situations.\nFinally, it is interesting to speculate regarding how self may differ in future AI systems as\nopposed to in humans. The relative stability we see in human selves may not exist in AI systems\nthat can self-improve and change more fundamentally and rapidly than humans can. There may\nbe a situation in which, as soon as a system has understood itself decently, it radically modifies\nitself and hence violates its existing self-model. Thus: intelligence without a long-term stable self.\nIn this case the “attractor-ish” nature of the self holds only over much shorter time scales than\nfor human minds or human-like minds. But the alternating process of synthesis and analysis\nfor self-construction is still critical, even though no reasonably stable self-constituting attractor\never emerges. The psychology of such intelligent systems will almost surely be beyond human\nbeings’ capacity for comprehension and empathy.\n3.4.4.2 Attentional Focus\nFinally, we turn to the notion of an “attentional focus” similar to Baars’ [Baa97] notion of a\nGlobal Workspace, which will be reviewed in more detail in Chapter 4: a collection of mental\nentities that are, at a given moment, receiving far more than the usual share of an intelligent\nsystem’s computational resources. Due to the amount of attention paid to items in the attentional\nfocus, at any given moment these items are in large part driving the cognitive processes\ngoing on elsewhere in the mind as well - because the cognitive processes acting on the items in\nthe attentional focus are often involved in other mental items, not in attentional focus, as well\n(and sometimes this results in pulling these other items into attentional focus). An intelligent\nsystem must constantly shift its attentional focus from one set of entities to another based on\nchanges in its environment and based on its own shifting discoveries.\nIn the human mind, there is a self-reinforcing dynamic pertaining to the collection of entities\nin the attentional focus at any given point in time, resulting from the observation that: If A\nis in the attentional focus, and A and B have often been associated in the past, then odds\nare increased that B will soon be in the attentional focus. This basic observation has been\nrefined tremendously via a large body of cognitive psychology work; and neurologically it follows\nnot only from Hebb’s [Heb49] classic work on neural reinforcement learning, but also from\nnumerous more modern refinements [SB98]. But it implies that two items A and B, if both in\nthe attentional focus, can reinforce each others’ presence in the attentional focus, hence forming\na kind of conspiracy to keep each other in the limelight. But of course, this kind of dynamic\n50 3 A Patternist Philosophy of Mind\nmust be counteracted by a pragmatic tendency to remove items from the attentional focus if\ngiving them attention is not providing sufficient utility in terms of the achievement of system\ngoals.\nThe synthesis and analysis perspective provides a more systematic perspective on this selfreinforcing\ndynamic. Synthesis occurs in the attentional focus when two or more items in the\nfocus are combined to form new items, new relationships, new ideas. This happens continually,\nas one of the main purposes of the attentional focus is combinational. On the other hand,\nAnalysis then occurs when a combination that has been speculatively formed is then linked\nin with the remainder of the mind (the “unconscious”, the vast body of knowledge that is not\nin the attentional focus at the given moment in time). Analysis basically checks to see what\nsupport the new combination has within the existing knowledge store of the system. Thus,\nforward-analysis basically comes down to “generate and test”, where the testing takes the form\nof attempting to integrate the generated structures with the ideas in the unconscious longterm\nmemory. One of the most obvious examples of this kind of dynamic is creative thinking\n(Boden, 2003; Goertzel, 1997), where the attentional focus continually combinationally creates\nnew ideas, which are then tested via checking which ones can be validated in terms of (built up\nfrom) existing knowledge.\nThe analysis stage may result in items being pushed out of the attentional focus, to be\nreplaced by others. Likewise may the synthesis stage: the combinations may overshadow and\nthen replace the things combined. However, in human minds and functional AI minds, the\nattentional focus will not be a complete chaos with constant turnover: Sometimes the same set of\nideas – or a shifting set of ideas within the same overall family of ideas – will remain in focus for a\nwhile. When this occurs it is because this set or family of ideas forms an approximate attractor\nfor the dynamics of the attentional focus, in particular for the forward-analysis dynamic of\nspeculative combination and integrative explanation. Often, for instance, a small “core set” of\nideas will remain in the attentional focus for a while, but will not exhaust the attentional focus:\nthe rest of the attentional focus will then, at any point in time, be occupied with other ideas\nrelated to the ones in the core set. Often this may mean that, for a while, the whole of the\nattentional focus will move around quasi-randomly through a “strange attractor” consisting of\nthe set of ideas related to those in the core set.\n3.4.5 Conclusion\nThe ideas presented above (the notions of synthesis and analysis, and the hypothesis of self and\nattentional focus as attractors of the iterative forward-analysis dynamic) are quite generic and\nare hypothetically proposed to be applicable to any cognitive system, natural or artificial. Later\nchapters will discuss the manifestation of the above ideas in the context of CogPrime. We have\nfound that the analysis/synthesis approach is a valuable tool for conceptualizing CogPrime’s\ncognitive dynamics, and we conjecture that a similar utility may be found more generally.\nNext, so as not to end the section on too blasé of a note, we will also make a stronger\nhypothesis: that, in order for a physical or software system to achieve intelligence that is roughly\nhuman-level in both capability and generality, using computational resources on the same order\nof magnitude as the human brain, this system must\n• manifest the dynamic of iterated synthesis and analysis, as modes of an underlying “selfgenerating\nsystem” dynamic\n3.5 Perspectives on Machine Consciousness 51\n• do so in such a way as to lead to self and attentional focus as emergent structures that serve\nas approximate attractors of this dynamic, over time periods that are long relative to the\nbasic “cognitive cycle time” of the system’s forward-analysis dynamics\nTo prove the truth of a hypothesis of this nature would seem to require mathematics fairly\nfar beyond anything that currently exists. Nonetheless, however, we feel it is important to\nformulate and discuss such hypotheses, so as to point the way for future investigations both\ntheoretical and pragmatic.\n3.5 Perspectives on Machine Consciousness\nFinally, we can’t let a chapter on philosophy – even a brief one – end without some discussion\nof the thorniest topic in the philosophy of mind: consciousness. Rather than seeking to resolve\nor comprehensively review this most delicate issue, we will restrict ourselves to giving it in\nAppendix ?? an overview of many of the common views on the subject; and here in the main text\ndiscussing the relationship between consciousness theory and patternist philosophy of cognition,\nthe practical work of designing and building AGI.\nOne fairly concrete idea about consciousness, that relates closely to certain aspects of the\nCogPrime design, is that the subjective experience of being conscious of some entity X, is correlated\nwith the presence of a very intense pattern in one’s overall mind-state, corresponding to X.\nThis simple idea is also the essence of neuroscientist Susan Greenfield’s theory of consciousness\n[Gre01] (but in her theory, \"overall mind-state\" is replaced with \"brain-state\"), and has much\ndeeper historical roots in philosophy of mind which we shall not venture to unravel here.\nThis observation relates to the idea of \"moving bubbles of awareness\" in intelligent systems.\nIf an intelligent system consists of multiple processing or data elements, and during each (sufficiently\nlong) interval of time some of these elements get much more attention than others,\nthen one may view the system as having a certain \"attentional focus\" during each interval. The\nattentional focus is itself a significant pattern in the system (the pattern being \"these elements\nhabitually get more processor and memory\", roughly speaking). As the attentional focus shifts\nover time one has a \"moving bubble of pattern\" which then corresponds experientially to a\n\"moving bubble of awareness.\"\nThis notion of a \"moving bubble of awareness\" ties in very closely to global workspace\ntheory [Baa97] (briefly mentioned above), a cognitive theory that has broad support from\nneuroscience and cognitive science and has also served as the motivation for Stan Franklin’s\nLIDA AI system [BF09], to be discussed in Chapter ??. The global workspace theory views the\nmind as consisting of a large population of small, specialized processes – a society of agents.\nThese agents organize themselves into coalitions, and coalitions that are relevant to contextually\nnovel phenomena, or contextually important goals, are pulled into the global workspace (which\nis identified with consciousness). This workspace broadcasts the message of the coalition to all\nthe unconscious agents, and recruits other agents into consciousness. Various sorts of contexts\n– e.g. goal contexts, perceptual contexts, conceptual contexts and cultural contexts – play a\nrole in determining which coalitions are relevant, and form the unconscious \"background\" of\nthe conscious global workspace. New perceptions are often, but not necessarily, pushed into the\nworkspace. Some of the agents in the global workspace are concerned with action selection, i.e.\nwith controlling and passing parameters to a population of possible actions. The contents of\nthe workspace at any given time have a certain cohesiveness and interdependency, the so-called\n52 3 A Patternist Philosophy of Mind\n\"unity of consciousness.\" In essence the contents of the global workspace form a moving bubble\nof attention or awareness.\nIn CogPrime, this moving bubble is achieved largely via economic attention network (ECAN)\nequations [GPI + 10] that propagate virtual currency between nodes and links representing elements\nof memories, so that the attentional focus consists of the wealthiest nodes and links.\nFigures 3.3 and 3.4 illustrate the existence and flow of attentional focus in OpenCog. On the\nother hand, in Hameroff’s recent model of the brain [Ham10], the brain’s moving bubble of\nattention is achieved through dendro-dendritic connections and the emergent dendritic web.\nFig. 3.3: Graphical depiction of the momentary bubble of attention in the memory of an\nOpenCog AI system. Circles and lines represent nodes and links in OpenCogPrimes memory,\nand stars denote those nodes with a high level of attention (represented in OpenCog by\nthe ShortTermImportance node variable) at the particular point in time.\nIn this perspective, self, free will and reflective consciousness are specific phenomena occurring\nwithin the moving bubble of awareness. They are specific ways of experiencing awareness,\ncorresponding to certain abstract types of physical structures and dynamics, which we shall\nendeavor to identify in detail in Appendix ??.\n3.6 Postscript: Formalizing Pattern 53\nFig. 3.4: Graphical depiction of the momentary bubble of attention in the memory of an\nOpenCog AI system, a few moments after the bubble shown in Figure 3.3, indicating the moving\nof the bubble of attention. Depictive conventions are the same as in Figure 1. This shows\nan idealized situation where the declarative knowledge remains invariant from one moment to\nthe next but only the focus of attention shifts. In reality both will evolve together.\n3.6 Postscript: Formalizing Pattern\nFinally, before winding up our very brief tour through patternist philosophy of mind, we will\nbriefly visit patternism’s more formal side. Many of the key aspects of patternism have been\nrigorously formalized. Here we give only a few very basic elements of the relevant mathematics,\nwhich will be used later on in the exposition of CogPrime. (Specifically, the formal definition of\npattern emerges in the CogPrime design in the definition of a fitness function for “pattern mining”\nalgorithms and Occam-based concept creation algorithms, and the definition of intensional\ninheritance within PLN.)\nWe give some definitions, drawn from Appendix 1 of [Goe06a]:\nDefinition 1 Given a metric space (M, d), and two functions c : M → [0, ∞] (the “simplicity\nmeasure”) and F : M → M (the “production relationship”), we say that P ∈ M is a pattern\nin X ∈ M to the degree\n54 3 A Patternist Philosophy of Mind\nι P X =\n((\n1 −\nd(F (P), X)\nc(X)\n) c(X) − c(P)\nc(X)\nThis degree is called the pattern intensity of P in X. It quantifies the extent to which P\nis a pattern in X. Supposing that F (P) = X, then the first factor in the definition equals 1,\nand we are left with only the second term, which measures the degree of compression obtained\nvia representing X as the result of P rather than simply representing X directly. The greater\nthe compression ratio obtained via using P to represent X, the greater the intensity of P as a\npattern in X. The first time, in the case F (P) ≠ X, adjusts the pattern intensity downwards to\naccount for the amount of error with which F (P) approximates ≠ X. If one holds the second\nfactor fixed and thinks about varying the first factor, then: The greater the error, the lossier\nthe compression, and the lower the pattern intensity.\nFor instance, if one wishes one may take c to denote algorithmic information measured on\nsome reference Turing machine, and F (X) to denote what appears on the second tape of a\ntwo-tape Turing machine t time-steps after placing X on its first tape. Other more naturalistic\ncomputational models are also possible here and are discussed extensively in Appendix 1 of\n[Goe06a].\n) +\nDefinition 2 The structure of X ∈ M is the fuzzy set St X\nfunction\nχ StX (P) = ι P X\ndefined via the membership\nThis lets us formalize our definition of “mind” alluded to above: the mind of X as the set\nof patterns associated with X. We can formalize this, for instance, by considering P to belong\nto the mind of X if it is a pattern in some Y that includes X. There are then two numbers\nto look at: ι P X and P (Y |X) (the percentage of Y that is also contained in X). To define the\ndegree to which P belongs to the mind of X we can then combine these two numbers using\nsome function f that is monotone increasing in both arguments. This highlights the somewhat\narbitrary semantics of “of” in the phrase “the mind of X.” Which of the patterns binding X to\nits environment are part of X’s mind, and which are part of the world? This isn’t necessarily\na good question, and the answer seems to depend on what perspective you choose, represented\nformally in the present framework by what combination function f you choose (for instance if\nf(a, b) = a r b 2−r then it depends on the choice of 0 < r < 1).\nNext, we can formalize the notion of a “pattern space” by positing a metric on patterns, thus\nmaking pattern space a metric space, which will come in handy in some places in later chapters:\nDefinition 3 Assuming M is a countable space, the structural distance is a metric d St\ndefined on M via\nd St (X, Y ) = T (χ StX , χ StY )\nwhere T is the Tanimoto distance.\nThe Tanimoto distance between two real vectors A and B is defined as\nT (A, B) =\nA · B\n‖A‖ 2 + ‖B‖ 2 − A · B\nand since M is countable this can be applied to fuzzy sets such as St X via considering the\nlatter as vectors. (As an aside, this can be generalized to uncountable M as well, but we will\nnot require this here.)\n3.6 Postscript: Formalizing Pattern 55\nUsing this definition of pattern, combined with the formal theory of intelligence given in\nChapter 7, one may formalize the various hypotheses made in the previous section, regarding\nthe emergence of different kinds of networks and structures as patterns in intelligent systems.\nHowever, it appears quite difficult to prove the formal versions of these hypotheses given current\nmathematical tools, which renders such formalizations of limited use.\nFinally, consider the case where the metric space M has a partial ordering < on it; we may\nthen define\nDefinition 3.1. R ∈ M is a subpattern in X ∈ M to the degree\n∫\nκ R P∈M\nX =\ntrue(R < P )dιP X\n∫\nP∈M dιP X\nThis degree is called the subpattern intensity of P in X.\nRoughly speaking, the subpattern intensity measures the percentage of patterns in X that\ncontain R (where \"containment\" is judged by the partial ordering <). But the percentage is\nmeasured using a weighted average, where each pattern is weighted by its intensity as a pattern\nin X. A subpattern may or may not be a pattern on its own. A nonpattern that happens to\noccur within many patterns may be an intense subpattern.\nWhether the subpatterns in X are to be considered part of the \"mind\" of X is a somewhat\nsuperfluous question of semantics. Here we choose to extend the definition of mind given in\n[Goe06a] to include subpatterns as well as patterns, because this makes it simpler to describe\nthe relationship between hypersets and minds, as we will do in Appendix ??.\n\nChapter 4\nBrief Survey of Cognitive Architectures\n4.1 Introduction\nWhile we believe CogPrime is the most thorough attempt at an architecture for advanced AGI,\nto date, we certainly recognize there have been many valuable attempts in the past with similar\naims; and we also have great respect for other AGI efforts occurring in parallel with Cog-\nPrime development, based on alternative, sometimes overlapping, theoretical presuppositions\nand practical choices. In most of this book we will ignore these other current and historical\nefforts except where they are directly useful for CogPrime – there are many literature reviews\nalready published, and this is a research treatise not a textbook. In this chapter, however, we\nwill break from this pattern and give a rough high-level overview of the various AGI architectures\nat play in the field today. The overview definitely has a bias toward other work with\nsome direct relevance to CogPrime, but not an overwhelming bias; we also discuss a number of\napproaches that are unrelated to, and even in some cases conceptually orthogonal to, our own.\nCogPrime builds on prior AI efforts in a variety of ways. Most of the specific algorithms\nand structures in CogPrime have their roots in prior AI work; and in addition, the CogPrime\ncognitive architecture has been heavily inspired by some other holistic cognitive architectures,\nespecially (but not exclusively) MicroPsi [Bac09], LIDA [BF09] and DeSTIN [ARK09a, ARC09].\nIn this chapter we will briefly review some existing cognitive architectures, with especial but\nnot exclusive emphasis on the latter three.\nWe will articulate some rough mappings between elements of these other architectures and\nelements of CogPrime – some in this chapter, and some in Chapter 5. However, these mappings\nwill mostly be left informal and very incompletely specified. The articulation of detailed interarchitecture\nmappings is an important project, but would be a substantial additional project\ngoing well beyond the scope of this book. We will not give a thorough review of the similarities\nand differences between CogPrime and each of these architectures, but only mention some of\nthe highlights.\nThe reader desiring a more thorough review of cognitive architectures is referred to Wlodek\nDuch’s review paper from the AGI-08 conference [DOP08]; and also to Alexei Samsonovich’s\nreview paper [Sam10], which compares a number of cognitive architectures in terms of a feature\nchecklist, and was created collaboratively with the creators of the architectures.\nDuch, in his survey of cognitive architectures [DOP08], divides existing approaches into three\nparadigms – symbolic, emergentist and hybrid – as broadly indicated in Figure 4.1. Drawing on\nhis survey and updating slightly, we give here some key examples of each, and then explain why\n57\n58 4 Brief Survey of Cognitive Architectures\nCogPrime represents a significantly more effective approach to embodied human-like general\nintelligence. In our treatment of emergentist architectures, we pay particular attention to developmental\nrobotics architectures, which share considerably with CogPrime in terms of underlying\nphilosophy, but differ via not integrating a symbolic “language and inference” component such\nas CogPrime includes.\nIn brief, we believe that the hybrid approach is the most pragmatic one given the current state\nof AI technology, but that the emergentist approach gets something fundamentally right, by\nfocusing on the emergence of complex dynamics and structures from the interactions of simple\ncomponents. So CogPrime is a hybrid architecture which (according to the cognitive synergy\nprinciple) binds its components together very tightly dynamically, allowing the emergence of\ncomplex dynamics and structures in the integrated system. Most other hybrid architectures are\nless tightly coupled and hence seem ill-suited to give rise to the needed emergent complexity. The\nother hybrid architectures that do possess the needed tight coupling, such as MicroPsi [Bac09],\nstrike us as underdeveloped and founded on insufficiently powerful learning algorithms.\nFig. 4.1: Duch’s simplified taxonomy of cognitive architectures. CogPrime falls into the “hybrid”\ncategory, but differs from other hybrid architectures in its focus on synergetic interactions\nbetween components and their potential to give rise to appropriate system-wide emergent structures\nenabling general intelligence.\n4.2 Symbolic Cognitive Architectures\nA venerable tradition in AI focuses on the physical symbol system hypothesis [New90], which\nstates that minds exist mainly to manipulate symbols that represent aspects of the world or\nthemselves. A physical symbol system has the ability to input, output, store and alter symbolic\nentities, and to execute appropriate actions in order to reach its goals. Generally, symbolic\ncognitive architectures focus on “working memory” that draws on long-term memory as needed,\nand utilize a centralized control over perception, cognition and action. Although in principle\nsuch architectures could be arbitrarily capable (since symbolic systems have universal repre-\n4.2 Symbolic Cognitive Architectures 59\nsentational and computational power, in theory), in practice symbolic architectures tend to be\nweak in learning, creativity, procedure learning, and episodic and associative memory. Decades\nof work in this tradition have not resolved these issues, which has led many researchers to\nexplore other options. A few of the more important symbolic cognitive architectures are:\n• SOAR [LRN87], a classic example of expert rule-based cognitive architecture designed to\nmodel general intelligence. It has recently been extended to handle sensorimotor functions,\nthough in a somewhat cognitively unnatural way; and is not yet strong in areas such as\nepisodic memory, creativity, handling uncertain knowledge, and reinforcement learning.\n• ACT-R [AL03] is fundamentally a symbolic system, but Duch classifies it as a hybrid system\nbecause it incorporates connectionist-style activation spreading in a significant role; and\nthere is an experimental thoroughly connectionist implementation to complement the primary\nmainly-symbolic implementation. Its combination of SOAR-style “production rules”\nwith large-scale connectionist dynamics allows it to simulate a variety of human psychological\nphenomena, but abstract reasoning, creativity and transfer learning are still missing.\n• EPIC [RCK01], a cognitive architecture aimed at capturing human perceptual, cognitive\nand motor activities through several interconnected processors working in parallel. The\nsystem is controlled by production rules for cognitive processors and a set of perceptual\n(visual, auditory, tactile) and motor processors operating on symbolically coded features\nrather than raw sensory data. It has been connected to SOAR for problem solving, planning\nand learning,\n• ICARUS [Lan05], an integrated cognitive architecture for physical agents, with knowledge\nspecified in the form of reactive skills, each denoting goal-relevant reactions to a class of\nproblems. The architecture includes a number of modules: a perceptual system, a planning\nsystem, an execution system, and several memory systems. Concurrent processing is absent,\nattention allocation is fairly crude, and uncertain knowledge is not thoroughly handled.\n• SNePS (Semantic Network Processing System) [SE07] is a logic, frame and network-based\nknowledge representation, reasoning, and acting system that has undergone over three\ndecades of development. While it has been used for some interesting prototype experiments\nin language processing and virtual agent control, it has not yet been used for any\nlarge-scale or real-world application.\n• Cyc [LG90] is an AGI architecture based on predicate logic as a knowledge representation,\nand using logical reasoning techniques to answer questions and derive new knowledge from\nold. It has been connected to a natural language engine, and designs have been created\nfor the connection of Cyc with Albus’s 4D-RCS [AM01]. Cyc’s most unique aspect is the\nlarge database of commonsense knowledge that Cycorp has accumulated (millions of pieces\nof knowledge, entered by specially trained humans in predicate logic format); part of the\nphilosophy underlying Cyc is that once a sufficient quantity of knowledge is accumulated in\nthe knowledge base, the problem of creating human-level general intelligence will become\nmuch less difficult due to the ability to leverage this knowledge.\nWhile these architectures contain many valuable ideas and have yielded some interesting results,\nwe feel they are incapable on their own of giving rise to the emergent structures and dynamics\nrequired to yield humanlike general intelligence using feasible computational resources. However,\nwe are more sanguine about the possibility of ideas and components from symbolic architectures\nplaying a role in human-level AGI via incorporation in hybrid architectures.\nWe now review a few symbolic architectures in slightly more detail.\n60 4 Brief Survey of Cognitive Architectures\n4.2.1 SOAR\nThe cognitive architectures best known among AI academics are probably Soar and ACT-R,\nboth of which are explicitly being developed with the dual goals of creating human-level AGI\nand modeling all aspects of human psychology. Neither the Soar nor ACT-R communities feel\nthemselves particularly near these long-term goals, yet they do take them seriously.\nSoar is based on IF-THEN rules, otherwise known as “production rules.” On the surface this\nmakes it similar to old-style expert systems, but Soar is much more than an expert system; it’s\nat minimum a sophisticated problem-solving engine. Soar explicitly conceives problem solving\nas a search through solution space for a “goal state” representing a (precise or approximate)\nproblem solution. It uses a methodology of incremental search, where each step is supposed to\nmove the system a little closer to its problem-solving goal, and each step involves a potentially\ncomplex “decision cycle.”\nIn the simplest case, the decision cycle has two phases:\n• Gathering appropriate information from the system’s long-term memory (LTM) into its\nworking memory (WM)\n• A decision procedure that uses the gathered information to decide an action\nIf the knowledge available in LTM isn’t enough to solve the problem, then the decision\nprocedure invokes search heuristics like hill-climbing, which try to create new knowledge (new\nproduction rules) that will help move the system closer to a solution. If a solution is found by\nchaining together multiple production rules, then a chunking mechanism is used to combine\nthese rules together into a single rule for future use. One could view the chunking mechanism\nas a way of converting explicit knowledge into implicit knowledge, similar to “map formation”\nin CogPrime (see Chapter 42 of Part 2), but in the current Soar design and implementation it\nis a fairly crude mechanism.\nIn recent years Soar has acquired a number of additional methods and modalities, including\nsome visual reasoning methods and some mechanisms for handling episodic and procedural\nknowledge. These expand the scope of the system but the basic production rule and chunking\nmechanisms as briefly described above remain the core “cognitive algorithm” of the system.\nFrom a CogPrime perspective, what Soar offers is certainly valuable, e.g.\n• heuristics for transferring knowledge from LTM into WM\n• chaining and chunking of implications\n• methods for interfacing between other forms of knowledge and implications\nHowever, a very short and very partial list of the major differences between Soar and Cog-\nPrime would include\n• CogPrime contains a variety of other core cognitive mechanisms beyond the management\nand chunking of implications\n• the variety of “chunking” type methods in CogPrime goes far beyond the sort of localized\nchunking done in Soar\n• CogPrime is committed to representing uncertainty at the base level whereas Soar’s production\nrules are crisp\n• The mechanisms for LTM-WM interaction are rather different in CogPrime, being based\non complex nonlinear dynamics as represented in Economic Attention Allocation (ECAN)\n• Currently Soar does not contain creativity-focused heuristics like blending or evolutionary\nlearning in its core cognitive dynamic.\n4.2 Symbolic Cognitive Architectures 61\n4.2.2 ACT-R\nIn the grand scope of cognitive architectures, ACT-R is quite similar to Soar, but there are\nmany micro-level differences. ACT-R is defined in terms of declarative and procedural knowledge,\nwhere procedural knowledge takes the form of Soar-like production rules, and declarative\nknowledge takes the form of chunks. It contains a variety of mechanisms for learning new rules\nand chunks from old; and also contains sophisticated probabilistic equations for updating the\nactivation levels associated with items of knowledge (these equations being roughly analogous\nin function to, though quite different from, the ECAN equations in CogPrime).\nFigure 4.2 displays the current architecture of ACT-R. The flow of cognition in the system is\nin response to the current goal, currently active information from declarative memory, information\nattended to in perceptual modules (vision and audition are implemented), and the current\nstate of motor modules (hand and speech are implemented). The early work with ACT-R was\nbased on comparing system performance to human behavior, using only behavioral measures,\nsuch as the timing of keystrokes or patterns of eye movements. Using such measures, it was not\npossible to test detailed assumptions about which modules were active in the performance of\na task. More recently the ACT-R community has been engaged in a process of using imaging\ndata to provide converging data on module activity. Figure 4.3 illustrates the associations they\nhave made between the modules in Figure 4.2 and brain regions. Coordination among all of\nthese components occurs through actions of the procedural module, which is mapped to the\nbasal ganglia.\nFig. 4.2: High-level architecture of ACT-R\nIn practice ACT-R, even more so than Soar, seems to be used more as a programming\nframework for cognitive modeling than as an AI system. One can fairly easily use ACT-R\nto program models of specific human mental behaviors, which may then be matched against\n62 4 Brief Survey of Cognitive Architectures\nFig. 4.3: Conjectured Mapping Between ACT-R and the Brain\npsychological data. Opinions differ as to whether this sort of modeling is valuable for achieving\nAGI goals. CogPrime is not designed to support this kind of modeling, as it intentionally does\nmany things very differently from humans.\nACT-R in its original form did not say much about perceptual and motor operations, but\nrecent versions have incorporated EPIC, an independent cognitive architecture focused on modeling\nthese aspects of human behavior.\n4.2.3 Cyc and Texai\nOur review of cognitive architectures would be incomplete without mentioning Cyc [LG90],\none of the best known and best funded AGI-oriented projects in history. While the main focus\nof the Cyc project has been on the hand-coding of large amounts of declarative knowledge,\nthere is also a cognitive architecture of sorts there. The center of Cyc is an engine for logical\ndeduction, acting on knowledge represented in predicate logic. A natural language engine has\nbeen associated with the logic engine, which enables one to ask English questions and get\nEnglish replies.\nStephen Reed, while an engineer at Cycorp, designed a perceptual-motor front end for Cyc\nbased on James Albus’ Reference Model Architecture; the ensuing system, called Cognitive-\nCyc, would have been the first full-fledged cognitive architecture based on Cyc, but was not\nimplemented. Reed left Cycorp and is now building a system called Texai, which has many\nsimilarities to Cyc (and relies upon the OpenCyc knowledge base, a subset of Cyc’s overall\nknowledge base), but incorporates a CognitiveCyc style cognitive architecture.\n4.2 Symbolic Cognitive Architectures 63\n4.2.4 NARS\nPei Wang’s NARS logic [Wan06] played a large role in the development of PLN, CogPrime’s\nuncertain logic component, a relationship that is discussed in depth in [GMIH08] and won’t\nbe re-emphasized here. However, NARS is more than just an uncertain logic, it is also an\noverall cognitive architecture (which is centered on NARS logic, but also includes other aspects).\nCogPrime bears little relation to NARS except in the specific similarities between PLN logic\nand NARS logic, but, the other aspects of NARS are worth briefly recounting here.\nNARS is formulated as a system for processing tasks, where a task consists of a question or a\npiece of new knowledge. The architecture is focused on declarative knowledge, but some pieces\nof knowledge may be associated with executable procedures, which allows NARS to carry out\ncontrol activities (in roughly the same way that a Prolog program can).\nAt any given time a NARS system contains\n• working memory: a small set of tasks which are active, kept for a short time, and closely\nrelated to new questions and new knowledge\n• long-term memory: a huge set of knowledge which is passive, kept for a long time, and not\nnecessarily related to current questions and knowledge\nThe working and long term memory spaces of NARS may each be thought of as a set of\nchunks, where each chunk consists of a set of tasks and a set of knowledge. NARS’s basic\ncognitive process is:\n1. choose a chunk\n2. choose a task from that chunk\n3. choose a piece of knowledge from that chunk\n4. use the task and knowledge to do inference\n5. send the new tasks to corresponding chunks\nDepending on the nature of the task and knowledge, the inference involved may be one of\nthe following:\n• if the task is a question, and the knowledge happens to be an answer to the question, a\ncopy of the knowledge is generated as a new task\n• backward inference\n• revision (merging two pieces of knowledge with the same form but different truth value)\n• forward inference\n• execution of a procedure associated with a piece of knowledge\nUnlike many other systems, NARS doesn’t decide what type of inference is used to process\na task when the task is accepted, but works in a data-driven way – that is, it is the task and\nknowledge that dynamically determine what type of inference will be carried out\nThe “choice” processes mentioned above are done via assigning relative priorities to\n• chunks (where they are called activity)\n• tasks (where they are called urgency)\n• knowledge (where they are called importance)\n64 4 Brief Survey of Cognitive Architectures\nand then distributing the system’s resources accordingly, based on a probabilistic algorithm.\n(It’s interesting to note that while NARS uses probability theory as part of its control mechanism,\nthe logic it uses to represent its own knowledge about the world is nonprobabilistic. This\nis considered conceptually consistent, in the context of NARS theory, because system control\nis viewed as a domain where the system’s knowledge is more complete, thus more amenable to\nprobabilistic reasoning.)\n4.2.5 GLAIR and SNePS\nAnother logic-focused cognitive architecture, very different from NARS in detail, is Stuart\nShapiro’s GLAIR cognitive architecture, which is centered on the SNePS paraconsistent logic\n[SE07].\nLike NARS, the core “cognitive loop” of GLAIR is based on reasoning: either thinking about\nsome percept (e.g. linguistic input, or sense data from the virtual or physical world), or answering\nsome question. This inference based cognition process is turned into an intelligent agent\ncontrol process via coupling it with an acting component, which operates according to a set of\npolicies, each one of which tells the system when to take certain internal or external actions\n(including internal reasoning actions) in response to its observed internal and external situation.\nGLAIR contains multiple layers:\n• the Knowledge Layer (KL), which contains the beliefs of the agent, and is where reasoning,\nplanning, and act selection are performed\n• the Sensori-Actuator Layer (SAL), contains the controllers of the sensors and effectors of\nthe hardware or software robot.\n• the Perceptuo-Motor Layer (PML), which grounds the KL symbols in perceptual structures\nand subconscious actions, contains various registers for providing the agent’s sense of situatedness\nin the environment, and handles translation and communication between the KL\nand the SAL.\nThe logical Knowledge Layer incorporates multiple memory types using a common representation\n(including declarative, procedural, episodic, attentional and intentional knowledge, and\nmeta-knowledge). To support this broad range of knowledge types, a broad range of logical inference\nmechanisms are used, so that the KL may be variously viewed as predicate logic based,\nframe based, semantic network based, or from other perspectives.\nWhat makes GLAIR more robust than most logic based AI approaches is the novel paraconsistent\nlogical formalism used in the knowledge base, which means (among other things)\nthat uncertain, speculative or erroneous knowledge may exist in the system’s memory without\nleading the system to create a broadly erroneous view of the world or carry out egregiously\nunintelligent actions. CogPrime is not thoroughly logic-focused like GLAIR is, but in its logical\naspect it seeks a similar robustness through its use of PLN logic, which embodies properties\nrelated to paraconsistency.\nCompared to CogPrime, we see that GLAIR has a similarly integrative approach, but that\nthe integration of different sorts of cognition is done more strictly within the framework of\nlogical knowledge representation.\n4.3 Emergentist Cognitive Architectures 65\n4.3 Emergentist Cognitive Architectures\nAnother species of cognitive architecture expects abstract symbolic processing to emerge from\nlower-level “subsymbolic” dynamics, which sometimes (but not always) are designed to simulate\nneural networks or other aspects of human brain function. These architectures are typically\nstrong at recognizing patterns in high-dimensional data, reinforcement learning and associative\nmemory; but no one has yet shown how to achieve high-level functions such as abstract reasoning\nor complex language processing using a purely subsymbolic approach. A few of the more\nimportant subsymbolic, emergentist cognitive architectures are:\n• DeSTIN [ARK09a, ARC09], which is part of CogPrime, may also be considered as an\nautonomous AGI architecture, in which case it is emergentist and contains mechanisms\nto encourage language, high-level reasoning and other abstract aspects of intelligent to\nemerge from hierarchical pattern recognition and related self-organizing network dynamics.\nIn CogPrime DeSTIN is used as part of a hybrid architecture, which greatly reduces the\nreliance on DeSTIN’s emergent properties.\n• Hierarchical Temporal Memory (HTM) [HB06] is a hierarchical temporal pattern\nrecognition architecture, presented as both an AI approach and a model of the cortex. So\nfar it has been used exclusively for vision processing and we will discuss its shortcomings\nlater in the context of our treatment of DeSTIN.\n• SAL [JL08], based on the earlier and related IBCA (Integrated Biologically-based Cognitive\nArchitecture) is a large-scale emergent architecture that seeks to model distributed\ninformation processing in the brain, especially the posterior and frontal cortex and the\nhippocampus. So far the architectures in this lineage have been used to simulate various\nhuman psychological and psycholinguistic behaviors, but haven’t been shown to give rise to\nhigher-level behaviors like reasoning or subgoaling.\n• NOMAD (Neurally Organized Mobile Adaptive Device) automata and its successors\n[KE06] are based on Edelman’s “Neural Darwinism” model of the brain, and feature large\nnumbers of simulated neurons evolving by natural selection into configurations that carry\nout sensorimotor and categorization tasks. The emergence of higher-level cognition from\nthis approach seems rather unlikely.\n• Ben Kuipers and his colleagues [MK07, MK08, MK09]have pursued an extremely innovative\nresearch program which combines qualitative reasoning and reinforcement learning to enable\nan intelligent agent to learn how to act, perceive and model the world. Kuipers’ notion of\n“bootstrap learning” involves allowing the robot to learn almost everything about its world,\nincluding for instance the structure of 3D space and other things that humans and other\nanimals obtain via their genetic endowments. Compared to Kuipers’ approach, CogPrime\nfalls in line with most other approaches which provide more “hard-wired” structure, following\nthe analogy to biological organisms that are born with more innate biases.\nThere is also a set of emergentist architectures focused specifically on developmental robotics,\nwhich we will review below in a separate subsection, as all of these share certain common\ncharacteristics.\nOur general perspective on the emergentist approach is that it is philosophically correct\nbut currently pragmatically inadequate. Eventually, some emergentist approach could surely\nsucceed at giving rise to humanlike general intelligence – the human brain, after all, is plainly\nan emergentist system. However, we currently lack understanding of how the brain gives rise\nto abstract reasoning and complex language, and none of the existing emergentist systems\n66 4 Brief Survey of Cognitive Architectures\nseem remotely capable of giving rise to such phenomena. It seems to us that the creation of\na successful emergentist AGI will have to wait for either a detailed understanding of how the\nbrain gives rise to abstract thought, or a much more thorough mathematical understanding of\nthe dynamics of complex self-organizing systems.\nThe concept of cognitive synergy is more relevant to emergentist than to symbolic architectures.\nIn a complex emergentist architecture with multiple specialized components, much of\nthe emergence is expected to arise via synergy between different richly interacting components.\nSymbolic systems, at least in the forms currently seen in the literature, seem less likely to give\nrise to cognitive synergy as their dynamics tend to be simpler. And hybrid systems, as we shall\nsee, are somewhat diverse in this regard: some rely heavily on cognitive synergies and others\nconsist of more loosely coupled components.\nWe now review the DeSTIN emergentist architecture in more detail, and then turn to the\ndevelopmental robotics architectures.\n4.3.1 DeSTIN: A Deep Reinforcement Learning Approach to AGI\nThe DeSTIN architecture, created by Itamar Arel and his colleagues, addresses the problem\nof general intelligence using hierarchical spatiotemporal networks designed to enable scalable\nperception, state inference and reinforcement-learning-guided action in real-world environments.\nDeSTIN has been developed with the plan of gradually extending it into a complete system for\nhumanoid robot control, founded on the same qualitative information-processing principles as\nthe human brain (though without striving for detailed biological realism). However, the practical\nwork with DeSTIN to date has focused on visual and auditory processing; and in the context of\nthe present proposal, the intention is to utilize DeSTIN for perception and actuation oriented\nprocessing, hybridizing it with CogPrime which will handle abstract cognition and language.\nHere we will discuss DeSTIN primarily in the perception context, only briefly mentioning the\napplication to actuation which is conceptually similar.\nIn DeSTIN (see Figure 4.4), perception is carried out by a deep spatiotemporal inference\nnetwork, which is connected to a similarly architected critic network that provides feedback on\nthe inference network’s performance, and an action network that controls actuators based on the\nactivity in the inference network (Figure 4.5 depicts a standard action hierarchy, of which the\nhierarchy in DeSTIN is an example). The nodes in these networks perform probabilistic pattern\nrecognition according to algorithms to be described below; and the nodes in each of the networks\nmay receive states of nodes in the other networks as inputs, providing rich interconnectivity\nand synergetic dynamics.\n4.3.1.1 Deep versus Shallow Learning for Perceptual Data Processing\nThe most critical feature of DeSTIN is its uniquely robust approach to modeling the world\nbased on perceptual data. Mimicking the efficiency and robustness by which the human brain\nanalyzes and represents information has been a core challenge in AI research for decades. For\ninstance, humans are exposed to massive amounts of visual and auditory data every second\nof every day, and are somehow able to capture critical aspects of it in a way that allows for\nappropriate future recollection and action selection. For decades, it has been known that the\n4.3 Emergentist Cognitive Architectures 67\nFig. 4.4: High-level architecture of DeSTIN\nbrain is a massively parallel fabric, in which computation processes and memory storage are\nhighly distributed. But massive parallelism is not in itself a solution – one also needs the right\narchitecture; which DeSTIN provides, building on prior work in the area of deep learning.\nHumanlike intelligence is heavily adapted to the physical environments in which humans\nevolved; and one key aspect of sensory data coming from our physical environments is its\nhierarchical structure. However, most machine learning and pattern recognition systems are\n“shallow” in structure, not explicitly incorporating the hierarchical structure of the world in\ntheir architecture. In the context of perceptual data processing, the practical result of this is\nthe need to couple each shallow learner with a pre-processing stage, wherein high-dimensional\nsensory signals are reduced to a lower-dimension feature space that can be understood by the\nshallow learner. The hierarchical structure of the world is thus crudely captured in the hierarchy\nof “preprocessor plus shallow learner.” In this sort of approach, much of the intelligence of the\nsystem shifts to the feature extraction process, which is often imperfect and always applicationdomain\nspecific.\nDeep machine learning has emerged as a more promising framework for dealing with complex,\nhigh-dimensional real-world data. Deep learning systems possess a hierarchical structure that\nintrinsically biases them to recognize the hierarchical patterns present in real-world data. Thus,\nthey hierarchically form a feature space that is driven by regularities in the observations, rather\nthan by hand-crafted techniques. They also offer robustness to many of the distortions and\ntransformations that characterize real-world signals, such as noise, displacement, scaling, etc.\nDeep belief networks [HOT06] and Convolutional Neural Networks [LBDE90] have been\ndemonstrated to successfully address pattern inference in high dimensional data (e.g. images).\nThey owe their success to their underlying paradigm of partitioning large data structures into\nsmaller, more manageable units, and discovering the dependencies that may or may not exist\n68 4 Brief Survey of Cognitive Architectures\nFig. 4.5: A standard, general-purpose hierarchical control architecture. DeSTIN’s control hierarchy\nexemplifies this architecture, with the difference lying mainly in the DeSTIN control\nhierarchy’s tight integration with the state inference (perception) and critic (reinforcement)\nhierarchies.\nbetween such units. However, this paradigm has its limitations; for instance, these approaches\ndo not represent temporal information with the same ease as spatial structure. Moreover, some\nkey constraints are imposed on the learning schemes driving these architectures, namely the\nneed for layer-by-layer training, and oftentimes pre-training. DeSTIN overcomes the limitations\nof prior deep learning approaches to perception processing, and also extends beyond perception\nto action and reinforcement learning.\n4.3.1.2 DeSTIN for Perception Processing\nThe hierarchical architecture of DeSTIN’s spatiotemporal inference network comprises an arrangement\ninto multiple layers of “nodes” comprising multiple instantiations of an identical\ncortical circuit. Each node corresponds to a particular spatiotemporal region, and uses a statistical\nlearning algorithm to characterize the sequences of patterns that are presented to it by\nnodes in the layer beneath it. More specifically,\n• At the very lowest layer of the hierarchy nodes receive as input raw data (e.g. pixels of an\nimage) and continuously construct a belief state that attempts to characterize the sequences\nof patterns viewed.\n4.3 Emergentist Cognitive Architectures 69\n• The second layer, and all those above it, receive as input the belief states of nodes at their\ncorresponding lower layers, and attempt to construct belief states that capture regularities\nin their inputs.\n• Each node also receives as input the belief state of the node above it in the hierarchy (which\nconstitutes “contextual” information)\nFig. 4.6: Small-scale instantiation of the DeSTIN perceptual hierarchy. Each box represents a\nnode, which corresponds to a spatiotemporal region (nodes higher in the hierarchy corresponding\nto larger regions). O denotes the current observation in the region, C is the state of the higherlayer\nnode, and S and S ′ denote state variables pertaining to two subsequent time steps. In\neach node, a statistical learning algorithm is used to predict subsequent states based on prior\nstates, current observations, and the state of the higher-layer node.\nMore specifically, each of the DeSTIN nodes, referring to a specific spacetime region, contains\na set of state variables conceived as clusters, each corresponding to a set of previously-observed\nsequences of events. These clusters are characterized by centroids (and are hence assumed\nroughly spherical in shape), and each of them comprises a certain \"spatiotemporal form\" recognized\nby the system in that region. Each node then contains the task of predicting the likelihood\nof a certain centroid being most apropos in the near future, based on the past history of observations\nin the node. This prediction may be done by simple probability tabulation, or via\n70 4 Brief Survey of Cognitive Architectures\napplication of supervised learning algorithms such as recurrent neural networks. These clustering\nand prediction processes occur separately in each node, but the nodes are linked together\nvia bidirectional dynamics: each node feeds input to its parents, and receives \"advice\" from its\nparents that is used to condition its probability calculations in a contextual way.\nThese processes are executed formally by the following basic belief update rule, which governs\nthe learning process and is identical for every node in the architecture. The belief state is a\nprobability mass function over the sequences of stimuli that the nodes learns to represent.\nConsequently, each node is allocated a predefined number of state variables each denoting a\ndynamic pattern, or sequence, that is autonomously learned. The DeSTIN update rule maps\nthe current observation (o), belief state (b), and the belief state of a higher-layer node or context\n(c), to a new (updated) belief state (b ′ ), such that\nalternatively expressed as\nb ′ (s ′ ) = Pr (s ′ |o, b, c) = Pr (s′ ∩ o ∩ b ∩ c)\n, (4.1)\nPr (o ∩ b ∩ c)\nb ′ (s ′ ) = Pr(o|s′ , b, c) Pr (s ′ |b, c) Pr (b, c)\n. (4.2)\nPr (o|b, c) Pr (b, c)\nUnder the assumption that observations depend only on the true state, or Pr(o|s ′ , b, c) =\nPr(o|s ′ ), we can further simplify the expression such that\nb ′ (s ′ ) = Pr(o|s′ ) Pr (s ′ |b, c)\n, (4.3)\nPr (o|b, c)\nwhere Pr (s ′ |b, c) = ∑ Pr (s ′ |s, c) b (s), yielding the belief update rule\ns∈S\nb ′ (s ′ ) =\nPr (o|s ′ ) ∑ Pr (s ′ |s, c) b (s)\ns∈S\n∑\nPr (o|s ′′ ) ∑ Pr (s ′′ |s, c) b (s) , (4.4)\ns ′′ ∈S\nwhere S denotes the sequence set (i.e. belief dimension) such that the denominator term is a\nnormalization factor.\nOne interpretation of eq. (4.4) would be that the static pattern similarity metric, Pr (o|s ′ ) ,\nis modulated by a construct that reflects the system dynamics, Pr (s ′ |s, c). As such, the belief\nstate inherently captures both spatial and temporal information. In our implementation, the\nbelief state of the parent node, c, is chosen using the selection rule\ns∈S\nc = arg max b p (s), (4.5)\ns\nwhere b p is the belief distribution of the parent node.\nA close look at eq. (4.4) reveals that there are two core constructs to be learned, Pr(o|s ′ )\nand Pr(s ′ |s, c). In the current DeSTIN design, the former is learned via online clustering while\nthe latter is learned based on experience by inductively learning a rule that predicts the next\nstate s ′ given the prior state s and c.\nThe overall result is a robust framework that autonomously (i.e. with no human engineered\npre-processing of any type) learns to represent complex data patterns, and thus serves the\n4.3 Emergentist Cognitive Architectures 71\ncritical role of building and maintaining a model of the state of the world. In a vision processing\ncontext, for example, it allows for powerful unsupervised classification. If shown a variety of\nreal-world scenes, it will automatically form internal structures corresponding to the various\nnatural categories of objects shown in the scenes, such as trees, chairs, people, etc.; and also\nthe various natural categories of events it sees, such as reaching, pointing, falling. And, as will\nbe discussed below, it can use feedback from DeSTIN’s action and critic networks to further\nshape its internal world-representation based on reinforcement signals.\nBenefits of DeSTIN for Perception Processing\nDeSTIN’s perceptual network offers multiple key attributes that render it more powerful than\nother deep machine learning approaches to sensory data processing:\n1. The belief space that is formed across the layers of the perceptual network inherently\ncaptures both spatial and temporal regularities in the data. Given that many applications\nrequire that temporal information be discovered for robust inference, this is a key advantage\nover existing schemes.\n2. Spatiotemporal regularities in the observations are captured in a coherent manner (rather\nthan being represented via two separate mechanisms)\n3. All processing is both top-down and bottom-up, and both hierarchical and heterarchical,\nbased on nonlinear feedback connections directing activity and modulating learning in multiple\ndirections through DeSTIN’s cortical circuits\n4. Support for multi-modal fusing is intrinsic within the framework, yielding a powerful state\ninference system for real-world, partially-observable settings.\n5. Each node is identical, which makes it easy to map the design to massively parallel platforms,\nsuch as graphics processing units.\nPoints 2-4 in the above list describe how DeSTIN’s perceptual network displays its own\n“cognitive synergy” in a way that fits naturally into the overall synergetic dynamics of the overall\nCogPrime architecture. Using this cognitive synergy, DeSTIN’s perceptual network addresses\na key aspect of general intelligence: the ability to robustly infer the state of the world, with\nwhich the system interacts, in an accurate and timely manner.\n4.3.1.3 DeSTIN for Action and Control\nDeSTIN’s perceptual network performs unsupervised world-modeling, which is a critical aspect\nof intelligence but of course is not the whole story. DeSTIN’s action network, coupled with the\nperceptual network, orchestrates actuator commands into complex movements, but also carries\nout other functions that are more cognitive in nature.\nFor instance, people learn to distinguish between cups and bowls in part via hearing other\npeople describe some objects as cups and others as bowls. To emulate this kind of learning,\nDeSTIN’s critic network provides positive or negative reinforcement signals based on whether\nthe action network has correctly identified a given object as a cup or a bowl, and this signal\nthen impacts the nodes in the action network. The critic network takes a simple external “degree\nof success or failure” signal and turns it into multiple reinforcement signals to be fed into the\nmultiple layers of the action network. The result is that the action network self-organizes so\n72 4 Brief Survey of Cognitive Architectures\nas to include an implicit “cup versus bowl” classifier, whose inputs are the outputs of some of\nthe nodes in the higher levels of the perceptual network. This classifier belongs in the action\nnetwork because it is part of the procedure by which the DeSTIN system carries out the action\nof identifying an object as a cup or a bowl.\nThis example illustrates how the learning of complex concepts and procedures is divided\nfluidly between the perceptual network, which builds a model of the world in an unsupervised\nway, and the action network, which learns how to respond to the world in a manner that will\nreceive positive reinforcement from the critic network.\n4.3.2 Developmental Robotics Architectures\nA particular subset of emergentist cognitive architectures are sufficiently important that we\nconsider them separately here: these are developmental robotics architectures, focused on controlling\nrobots without significant “hard-wiring” of knowledge or capabilities, allowing robots\nto learn (and learn how to learn, etc.) via their engagement with the world. A significant focus\nis often placed here on “intrinsic motivation,” wherein the robot explores the world guided by\ninternal goals like novelty or curiosity, forming a model of the world as it goes along, based\non the modeling requirements implied by its goals. Many of the foundations of this research\narea were laid by Juergen Schmidhuber’s work in the 1990s [Sch91b, Sch91a, Sch95, Sch02], but\nnow with more powerful computers and robots the area is leading to more impressive practical\ndemonstrations.\nWe mention here a handful of the important initiatives in this area:\n• Juyang Weng’s Dav [HZT + 02] and SAIL [WHZ + 00] projects involve mobile robots that\nexplore their environments autonomously, and learn to carry out simple tasks by building up\ntheir own world-representations through both unsupervised and teacher-driven processing\nof high-dimensional sensorimotor data. The underlying philosophy is based on human child\ndevelopment [WH06], the knowledge representations involved are neural network based,\nand a number of novel learning algorithms are involved, especially in the area of vision\nprocessing.\n• FLOWERS [BO09], an initiative at the French research institute INRIA, led by Pierre-\nYves Oudeyer, is also based on a principle of trying to reconstruct the processes of development\nof the human child’s mind, spontaneously driven by intrinsic motivations. Kaplan\n[Kap08] has taken this project in a direction closely related to our own via the creation\nof a “robot playroom.” Experiential language learning has also been a focus of the project\n[OK06], driven by innovations in speech understanding.\n• IM-CLEVER 1 , a new European project coordinated by Gianluca Baldassarre and conducted\nby a large team of researchers at different institutions, is focused on creating software\nenabling an iCub [MSV + 08] humanoid robot to explore the environment and learn to carry\nout human childlike behaviors based on its own intrinsic motivations. As this project is the\nclosest to our own we will discuss it in more depth below.\nLike CogPrime, IM-CLEVER is a humanoid robot intelligence architecture guided by intrinsic\nmotivations, and using hierarchical architectures for reinforcement learning and sensory ab-\n1 http://im-clever.noze.it/project/project-description\n4.4 Hybrid Cognitive Architectures 73\nstraction. IM-CLEVER’s motivational structure is based in part on Schmidhuber’s informationtheoretic\nmodel of curiosity [Sch06]; and CogPrime’s Psi-based motivational structure utilizes\nprobabilistic measures of novelty, which are mathematically related to Schmidhuber’s measures.\nOn the other hand, IM-CLEVER’s use of reinforcement learning follows Schmidhuber’s\nearlier work RL for cognitive robotics [BS04, BZGS06], Barto’s work on intrinsically motivated\nreinforcement learning [SB06, SM05], and Lee’s [LMC07b, LMC07a] work on developmental\nreinforcement learning; whereas CogPrime’s assemblage of learning algorithms is more diverse,\nincluding probabilistic logic, concept blending and other symbolic methods (in the OCP component)\nas well as more conventional reinforcement learning methods (in the DeSTIN component).\nIn many respects IM-CLEVER bears a moderately strong resemblance to DeSTIN, whose\nintegration with CogPrime is discussed in Chapter 26 of Part 2 (although IM-CLEVER has\nmuch more focus on biological realism than DeSTIN). Apart from numerous technical differences,\nthe really big distinction between IM-CLEVER and CogPrime is that in the latter we\nare proposing to hybridize a hierarchical-abstraction/reinforcement-learning system (such as\nDeSTIN) with a more abstract symbolic cognition engine that explicitly handles probabilistic\nlogic and language. IM-CLEVER lacks the aspect of hybridization with a symbolic system, taking\nmore of a pure emergentist strategy. Like DeSTIN considered as a standalone architecture\nIM-CLEVER does entail a high degree of cognitive synergy, between components dealing with\nperception, world-modeling, action and motivation. However, the “emergentist versus hybrid”\nis a large qualitative difference between the two approaches.\nIn all, while we largely agree with the philosophy underlying developmental robotics, our\nintuition is that the learning and representational mechanisms underlying the current systems\nin this area are probably not powerful enough to lead to human child level intelligence. We\nexpect that these systems will develop interesting behaviors but fall short of robust preschool\nlevel competency, especially in areas like language and reasoning where symbolic systems have\ntypically proved more effective. This intuition is what impels us to pursue a hybrid approach,\nsuch as CogPrime. But we do feel that eventually, once the mechanisms underlying brains are\nbetter understood and robotic bodies are richer in sensation and more adept in actuation, some\nsort of emergentist, developmental-robotics approach can be successful at creating humanlike,\nhuman-level AGI.\n4.4 Hybrid Cognitive Architectures\nIn response to the complementary strengths and weaknesses of the symbolic and emergentist\napproaches, in recent years a number of researchers have turned to integrative, hybrid architectures,\nwhich combine subsystems operating according to the two different paradigms. The\ncombination may be done in many different ways, e.g. connection of a large symbolic subsystem\nwith a large subsymbolic system, or the creation of a population of small agents each of which\nis both symbolic and subsymbolic in nature.\nNils Nilsson expressed the motivation for hybrid AGI systems very clearly in his article at\nthe AI-50 conference (which celebrated the 50’th anniversary of the AI field) [Nil09]. While\naffirming the value of the Physical Symbol System Hypothesis that underlies symbolic AI, he\nargues that “the PSSH explicitly assumes that, whenever necessary, symbols will be grounded\nin objects in the environment through the perceptual and effector capabilities of a physical\nsymbol system.” Thus, he continues,\n74 4 Brief Survey of Cognitive Architectures\n“I grant the need for non-symbolic processes in some intelligent systems, but I think they supplement\nrather than replace symbol systems. I know of no examples of reasoning, understanding\nlanguage, or generating complex plans that are best understood as being performed by systems\nusing exclusively non-symbolic processes....\nAI systems that achieve human-level intelligence will involve a combination of symbolic and\nnon-symbolic processing.”\nA few of the more important hybrid cognitive architectures are:\n• CLARION [SZ04] is a hybrid architecture that combines a symbolic component for reasoning\non “explicit knowledge” with a connectionist component for managing “implicit knowledge.”\nLearning of implicit knowledge may be done via neural net, reinforcement learning,\nor other methods. The integration of symbolic and subsymbolic methods is powerful, but a\ngreat deal is still missing such as episodic knowledge and learning and creativity. Learning\nin the symbolic and subsymbolic portions is carried out separately rather than dynamically\ncoupled, minimizing “cognitive synergy” effects.\n• DUAL [NK04] is the most impressive system to come out of Marvin Minsky’s “Society of\nMind” paradigm. It features a population of agents, each of which combines symbolic and\nconnectionist representation, self-organizing to collectively carry out tasks such as perception,\nanalogy and associative memory. The approach seems innovative and promising, but\nit is unclear how the approach will scale to high-dimensional data or complex reasoning\nproblems due to the lack of a more structured high-level cognitive architecture.\n• LIDA [BF09] is a comprehensive cognitive architecture heavily based on Bernard Baars’\n“Global Workspace Theory”. It articulates a “cognitive cycle” integrating various forms of\nmemory and intelligent processing in a single processing loop. The architecture ties in well\nwith both neuroscience and cognitive psychology, but it deals most thoroughly with “lower\nlevel” aspects of intelligence, handling more advanced aspects like language and reasoning\nonly somewhat sketchily. There is a clear mapping between LIDA structures and processes\nand corresponding structures and processing in OCP; so that it’s only a mild stretch to view\nCogPrime as an instantiation of the general LIDA approach that extends further both in\nthe lower level (to enable robot action and sensation via DeSTIN) and the higher level (to\nenable advanced language and reasoning via OCP mechanisms that have no direct LIDA\nanalogues).\n• MicroPsi [Bac09] is an integrative architecture based on Dietrich Dorner’s Psi model of motivation,\nemotion and intelligence. It has been tested on some practical control applications,\nand also on simulating artificial agents in a simple virtual world. MicroPsi’s comprehensiveness\nand basis in neuroscience and psychology are impressive, but in the current version\nof MicroPsi, learning and reasoning are carried out by algorithms that seem unlikely to\nscale. OCP incorporates the Psi model for motivation and emotion, so that MicroPsi and\nCogPrime may be considered very closely related systems. But similar to LIDA, MicroPsi\ncurrently focuses on the “lower level” aspects of intelligence, not yet directly handling advanced\nprocesses like language and abstract reasoning.\n• PolyScheme [Cas07] integrates multiple methods of representation, reasoning and inference\nschemes for general problem solving. Each Polyscheme “specialist” models a different\naspect of the world using specific representation and inference techniques, interacting with\nother specialists and learning from them. Polyscheme has been used to model infant reasoning\nincluding object identity, events, causality, and spatial relations. The integration of\n4.4 Hybrid Cognitive Architectures 75\nreasoning methods is powerful, but the overall cognitive architecture is simplistic compared\nto other systems and seems focused more on problem-solving than on the broader problem\nof intelligent agent control.\n• Shruti [SA93] is a fascinating biologically-inspired model of human reflexive inference,\nwhich represents in connectionist architecture relations, types, entities and causal rules\nusing focal-clusters. However, much like Hofstadter’s earlier Copycat architecture [Hof95],\nShruti seems more interesting as a prototype exploration of ideas than as a practical AGI\nsystem; at least, after a significant time of development it has not proved significantly\neffective in any applications\n• James Albus’s 4D/RCS robotics architecture shares a great deal with some of the emergentist\narchitectures discussed above, e.g. it has the same hierarchical pattern recognition\nstructure as DeSTIN and HTM, and the same three cross-connected hierarchies as DeSTIN,\nand shares with the developmental robotics architectures a focus on real-time adaptation to\nthe structure of the world. However, 4D/RCS is not foundationally learning-based but relies\non hard-wired architecture and algorithms, intended to mimic the qualitative structure of\nrelevant parts of the brain (and intended to be augmented by learning, which differentiates\nit from emergentist approaches.\nAs our own CogPrime approach is a hybrid architecture, it will come as no surprise that\nwe believe several of the existing hybrid architectures are fundamentally going in the right\ndirection. However, nearly all the existing hybrid architectures have severe shortcomings which\nwe feel will prevent them from achieving robust humanlike AGI.\nMany of the hybrid architectures are in essence “multiple, disparate algorithms carrying out\nseparate functions, encapsulated in black boxes and communicating results with each other.”\nFor instance, PolyScheme, ACT-R and CLARION all display this “modularity” property to a\nsignificant extent. These architectures lack the rich, real-time interaction between the internal\ndynamics of various memory and learning processes that we believe is critical to achieving\nhumanlike general intelligence using realistic computational resources. On the other hand, those\narchitectures that feature richer integration – such as DUAL, Shruti, LIDA and MicroPsi – have\nthe flaw of relying (at least in their current versions) on overly simplistic learning algorithms,\nwhich drastically limits their scalability.\nIt does seem plausible to us that some of these hybrid architectures could be dramatically\nextended or modified so as to produce humanlike general intelligence. For instance, one could\nreplace LIDA’s learning algorithms with others that interrelate with each other in a nuanced\nsynergetic way; or one could replace MicroPsi’s simple learning and reasoning methods with\nmuch more powerful and scalable ones acting on the same data structures. However, making\nthese changes would dramatically alter the cognitive architectures in question on multiple levels.\n4.4.1 Neural versus Symbolic; Global versus Local\nThe “symbolic versus emergentist” dichotomy that we have used to structure our review of cognitive\narchitectures is not absolute nor fully precisely defined; it is more of a heuristic distinction.\nIn this section, before plunging into the details of particular hybrid cognitive architectures, we\nreview two other related dichotomies that are useful for understanding hybrid systems: neural\nversus symbolic systems, and globalist versus localist knowledge representation.\n76 4 Brief Survey of Cognitive Architectures\n4.4.1.1 Neural-Symbolic Integration\nThe distinction between neural and symbolic systems has gotten fuzzier and fuzzier in recent\nyears, with developments such as\n• Logic-based systems being used to control embodied agents (hence using logical terms to\ndeal with data that is apparently perception or actuation-oriented in nature, rather than\nbeing symbolic in the semiotic sense), see [SS03a] and [GMIH08].\n• Hybrid systems combining neural net and logical parts, or using logical or neural net components\ninterchangeably in the same role [LAon].\n• Neural net systems being used for strongly symbolic tasks such as automated grammar\nlearning ([Elm91], [Elm91], plus more recent work.)\nFigure 4.7 presents a schematic diagram of a generic neural-symbolic system, generalizing\nfrom [BH05], a paper that gives an elegant categorization of neural-symbolic AI systems. Figure\n4.8 depicts several broad categories of neural-symbolic architecture.\nFig. 4.7: Generic neural-symbolic architecture\nBader and Hitzler categorize neural-symbolic systems according to three orthogonal axes:\ninterrelation, language and usage. “Language” refers to the type of language used in the symbolic\ncomponent, which may be logical, automata-based, formal grammar-based, etc. “Usage” refers\nto the purpose to which the neural-symbolic interrelation is put. We tend to use “learning” as\nan encompassing term for all forms of ongoing knowledge-creation, whereas Bader and Hitzler\ndistinguish learning from reasoning.\nOf Bader and Hitzler’s three axes the one that interests us most here is “interrelation”, which\nrefers to the way the neural and symbolic components of the architecture intersect with each\nother. They distinguish “hybrid” architectures which contain separate but equal, interacting\nneural and symbolic components; versus “integrative” architectures in which the symbolic component\nessentially rides piggyback on the neural component, extracting information from it and\nhelping it carry out its learning, but playing a clearly derived and secondary role. We prefer\nSun’s (2001) term “monolithic” to Bader and Hitzler’s “integrative” to describe this type of\nsystem, as the latter term seems best preserved in its broader meaning.\n4.4 Hybrid Cognitive Architectures 77\nFig. 4.8: Broad categories of neural-symbolic architecture\nWithin the scope of hybrid neural-symbolic systems, there is another axis which Bader and\nHitzler do not focus on, because the main interest of their review is in monolithic systems. We\ncall this axis \"interactivity\"’, and what we are referring to is the frequency of high-informationcontent,\nhigh-influence interaction between the neural and symbolic components in the hybrid\nsystem. In a low-interaction hybrid system, the neural and symbolic components don’t exchange\nlarge amounts of mutually influential information all that frequently, and basically act like\nindependent system components that do their learning/reasoning/thinking periodically sending\neach other their conclusions. In some cases, interaction may be asymmetric: one component may\nfrequently send a lot of influential information to the other, but not vice versa. However, our\nhypothesis is that the most capable neural-symbolic systems are going to be the symmetrically\nhighly interactive ones.\nIn a symmetric high-interaction hybrid neural-symbolic system, the neural and symbolic\ncomponents exchange influential information sufficiently frequently that each one plays a major\nrole in the other one’s learning/reasoning/thinking processes. Thus, the learning processes of\neach component must be considered as part of the overall dynamic of the hybrid system. The\ntwo components aren’t just feeding their outputs to each other as inputs, they’re mutually\nguiding each others’ internal processing.\nOne can make a speculative argument for the relevance of this kind of architecture to neuroscience.\nIt seems plausible that this kind of neural-symbolic system roughly emulates the kind\nof interaction that exists between the brain’s neural subsystems implementing localist symbolic\nprocessing, and the brain’s neural subsystems implementing globalist, classically “connectionist”\nprocessing. It seems most likely that, in the brain, symbolic functionality emerges from\nan underlying layer of neural dynamics. However, it is also reasonable to conjecture that this\nsymbolic functionality is confined to a functionally distinct subsystem of the brain, which then\n78 4 Brief Survey of Cognitive Architectures\ninteracts with other subsystems in the brain much in the manner that the symbolic and neural\ncomponents of a symmetric high-interaction neural-symbolic system interact.\nNeuroscience speculations aside, however, our key conjecture regarding neural-symbolic integration\nis that this sort of neural-symbolic system presents a promising direction for artificial\ngeneral intelligence research. In Chapter 26 of Volume 2 we will give a more concrete idea of\nwhat a symmetric high-interaction hybrid neural-symbolic architecture might look like, exploring\nthe potential for this sort of hybridization between the OpenCogPrime AGI architecture\n(which is heavily symbolic in nature) and hierarchical attractor neural net based architectures\nsuch as DeSTIN.\n4.5 Globalist versus Localist Representations\nAnother interesting distinction, related to but different from “symbolic versus emergentist”\nand “neural versus symbolic”, may be drawn between cognitive systems (or subsystems) where\nmemory is essentially global, and those where memory is essentially local. In this section\nwe will pursue this distinction in various guises, along with the less familiar notion of glocal\nmemory.\nThis globalist/localist distinction is most easily conceptualized by reference to memories\ncorresponding to categories of entities or events in an external environment. In an AI system\nthat has an internal notion of “activation” – i.e. in which some of its internal elements are more\nactive than others, at any given point in time – one can define the internal image of an external\nevent or entity as the fuzzy set of internal elements that tend to be active when that event or\nentity is presented to the system’s sensors. If one has a particular set S of external entities or\nevents of interest, then, the degree of memory localization of such an AI system relative to S\nmay be conceived as the percentage of the system’s internal elements that have a high degree\nof membership in the internal image of an average element of S.\nOf course, this characterization of localization has its limitations, such as the possibility of\nambiguity regarding what are the “system elements” of a given AI system; and the exclusive\nfocus on internal images of external phenomena rather than representation of internal abstract\nconcepts. However, our goal here is not to formulate an ultimate, rigorous and thorough ontology\nof memory systems, but only to pose a “rough and ready” categorization so as to properly frame\nour discussion of some specific AGI issues relevant to CogPrime. Clearly the ideas pursued here\nwill benefit from further theoretical exploration and elaboration.\nIn this sense, a Hopfield neural net [Ami89] would be considered “globalist” since it has a low\ndegree of memory localization (most internal images heavily involve a large number of system\nelements); whereas Cyc would be considered “localist” as it has a very high degree of memory\nlocalization (most internal images are heavily focused on a small set of system elements).\nHowever, although Hopfield nets and Cyc form handy examples, the “globalist vs. localist”\ndistinction as described above is not identical to the “neural vs. symbolic” distinction. For it is\nin principle quite possible to create localist systems using formal neurons, and also to create\nglobalist systems using formal logic. And “globalist-localist” is not quite identical to “symbolic vs\nemergentist” either, because the latter is about coordinated system dynamics and behavior not\njust about knowledge representation. CogPrime combines both symbolic and (loosely) neural\nrepresentations, and also combines globalist and localist representations in a way that we will\ncall “glocal” and analyze more deeply in Chapter 13; but there are many other ways these various\n4.5 Globalist versus Localist Representations 79\nproperties could be manifested by AI systems. Rigorously studying the corpus of existing (or\nhypothetical!) cognitive architectures using these ideas would be a large task, which we do not\nundertake here.\nIn the next sections we review several hybrid architectures in more detail, focusing most\ndeeply on LIDA and MicroPsi which have been directly inspirational for CogPrime.\n4.5.1 CLARION\nRon Sun’s CLARION architecture (see Figure 4.9) is interesting in its combination of symbolic\nand neural aspects – a combination that is used in a sophisticated way to embody the distinction\nand interaction between implicit and explicit mental processes. From a CLARION perspective,\narchitectures like Soar and ACT-R are severely limited in that they deal only with explicit\nknowledge and associated learning processes.\nCLARION consists of a number of distinct subsystems, each of which contains a dual representational\nstructure, including a “rules and chunks” symbolic knowledge store somewhat\nsimilar to ACT-R, and a neural net knowledge store embodying implicit knowledge. The main\nsubsystems are:\n• An action-centered subsystem to control actions;\n• A non-action-centered subsystem to maintain general knowledge;\n• A motivational subsystem to provide underlying motivations for perception, action, and\ncognition;\n• A meta-cognitive subsystem to monitor, direct, and modify the operations of all the other\nsubsystems.\nFig. 4.9: The CLARION cognitive architecture.\n80 4 Brief Survey of Cognitive Architectures\n4.5.2 The Society of Mind and the Emotion Machine\nIn his influential but controversial book The Society of Mind [Min88], Marvin Minsky described\na model of human intelligence as something that is built up from the interactions of numerous\nsimple agents. He spells out in great detail how various particular cognitive functions may be\nachieved via agents and their interactions. He leaves no room for any central algorithms or\nstructures of thought, famously arguing: “What magical trick makes us intelligent? The trick\nis that there is no trick. The power of intelligence stems from our vast diversity, not from any\nsingle, perfect principle.”\nThis perspective was extended in the more recent work The Emotion Machine [Min07], where\nMinsky argued that emotions are “ways to think” evolved to handle different “problem types”\nthat exist in the world. The brain is posited to have rule-based mechanisms (selectors) that\nturns on emotions to deal with various problems.\nOverall, both of these works serve better as works of speculative cognitive science than as\nworks of AI or cognitive architecture per se. As neurologist Richard Restak said in his review\nof Emotion Machine, “Minsky does a marvelous job parsing other complicated mental activities\ninto simpler elements. ... But he is less effective in relating these emotional functions to what’s\ngoing on in the brain.” As Restak added, he is also not so effective at relating these emotional\nfunctions to straightforwardly implementable algorithms or data structures.\nPush Singh, in his PhD thesis and followup work [SBC05], did the best job so far of creating\na concrete AI design based on Minsky’s ideas. While Singh’s system was certainly interesting,\nit was also noteworthy for its lack of any learning mechanisms, and its exclusive focus on\nexplicit rather than implicit knowledge. Due to Singh’s tragic death, his work was never brought\nanywhere near completion. It seems fair to say that there has not yet been a serious cognitive\narchitecture posed based closely on Minsky’s ideas.\n4.5.3 DUAL\nThe closest thing to a Minsky-ish cognitive architecture is probably DUAL, which takes the\nSociety of Mind concept and adds to it a number of other interesting ideas. DUAL integrates\nsymbolic and connectionist approaches at a deeper level than CLARION, and has been used\nto model various cognitive functions such as perception, analogy and judgment. Computations\nin DUAL emerge from the self-organized interaction of many micro-agents, each of which is\na hybrid symbolic/connectionist device. Each DUAL agent plays the role of a neural network\nnode, with an activation level and activation spreading dynamics; but also plays the role of\na symbol, manipulated using formal rules. The agents exchange messages and activation via\nlinks that can be learned and modified, and they form coalitions which collectively represent\nconcepts, episodes, and facts.\nThe structure of the model is sketchily depicted in Figure 4.10, which covers the application\nof DUAL to a toy environment called TextWorld. The visual input corresponding to a stimulus\nis presented on a two-dimensional visual array representing the front end of the system.\nPerceptual primitives like blobs and terminations are immediately generated by cheap parallel\ncomputations. Attention is controlled at each time by an object which allocates it selectively\nto some area of the stimulus. A detailed symbolic representation is constructed for this area\nwhich tends to fade away as attention is withdrawn from it and allocated to another one. Cate-\n4.5 Globalist versus Localist Representations 81\ngorization of visual memory contents takes place by retrieving object and scene categories from\nDUAL’s semantic memory and mapping them onto current visual memory representations.\nFig. 4.10: The three main components of the DUAL model: the retinotopic visual array (RVA),\nthe visual working memory (VWM) and DUAL’s semantic memory. Attention is allocated to\nan area of the visual array by the object in VWM controlling attention, while scene and object\ncategories corresponding to the contents of VWM are retrieved from the semantic memory.\nIn principle the DUAL framework seems quite powerful; using the language of CogPrime,\nhowever, it seems to us that the learning mechanisms of DUAL have not been formulated in\nsuch a way as to give rise to powerful, scalable cognitive synergy. It would likely be possible\nto create very powerful AGI systems within DUAL, and perhaps some very CogPrime -like\nsystems as well. But the systems that have been created or designed for use within DUAL so\nfar seem not to be that powerful in their potential or scope.\n4.5.4 4D/RCS\nIn a rather different direction, James Albus, while at the National Bureau of Standards, developed\na very thorough and impressive architecture for intelligent robotics called 4D/RCS,\nwhich was implemented in a number of machines including unmanned automated vehicles. This\narchitecture lacks critical aspects of intelligence such as learning and creativity, but combines\nperception, action, planning and world-modeling in a highly effective and tightly-integrated\nfashion.\nThe architecture has three hierarchies of memory/processing units: one for perception, one\nfor action and one for modeling and guidance. Each unit has a certain spatiotemporal scope,\n82 4 Brief Survey of Cognitive Architectures\nand (except for the lowest level) supervenes over children whose spatiotemporal scope is a subset\nof its own. The action hierarchy takes care of decomposing tasks into subtasks; whereas the\nsensation hierarchy takes care of grouping signals into entities and events. The modeling/guidance\nhierarchy mediates interactions between perception and action based on its understanding\nof the world and the system’s goals.\nIn his book [AM01] Albus describes methods for extending 4D/RCS into a complete cognitive\narchitecture, but these extensions have not been elaborated in full detail nor implemented.\nFig. 4.11: Albus’s 4D-RCS architecture for a single vehicle\n4.5.5 PolyScheme\nNick Cassimatis’s PolyScheme architecture [Cas07] shares with GLAIR the use of multiple\nlogical reasoning methods on a common knowledge store. While its underlying ideas are quite\ngeneral, currently PolyScheme is being developed in the context of the “object tracking” domain\n(construed very broadly). As a logic framework PolyScheme is fairly conventional (unlike GLAIR\nor NARS with their novel underlying formalisms), but PolyScheme has some unique conceptual\naspects, for instance its connection with Cassimatis’s theory of mind, which holds that the same\ncore set of logical concepts and relationships underlies both language and physical reasoning\n[Cas04]. This ties in with the use of a common knowledge store for multiple cognitive processes;\nfor instance it suggests that\n• the same core relationships can be used for physical reasoning and parsing, but that each\nof these domains may involve some additional relationships.\n• language processing may be done via physical-reasoning-based cognitive processes, plus the\nadditional activity of some language-specific processes\n4.5 Globalist versus Localist Representations 83\nFig. 4.12: Albus’s perceptual, motor and modeling hierarchies\n4.5.6 Joshua Blue\nSam Adams and his colleagues at IBM have created a cognitive architecture called Joshua Blue\n[AABL02], which has some significant similarities to CogPrime. Similar to our current research\ndirection with CogPrime, Joshua Blue was created with loose emulation of child cognitive\ndevelopment in mind; and, also similar to CogPrime, it features a number of cognitive processes\nacting on a common neural-symbolic knowledge store. The specific cognitive processes involved\nin Joshua Blue and CogPrime are not particularly similar, however. At time of writing (2012)\n84 4 Brief Survey of Cognitive Architectures\nJoshua Blue is not under active development and has not been for some time; however, the\nproject may be reanimated in future.\nJoshua Blue’s core knowledge representation is a semantic network of nodes connected by\nlinks along which activation spreads. Although many of the nodes have specific semantic referents,\nas in a classical semantic net, the spread of activation through the network is designed to\nlead to the emergence of “assemblies” (which could also be thought of as dynamical attractors)\nin a manner more similar to an attractor neural network.\nA major difference from typical semantic or neural network models is the central role that\naffect plays in the system’s dynamics. The weights of the links in the knowledge base are adjusted\ndynamically based on the emotional context – a very direct way of ensuring that cognitive\nprocesses and mental representations are continuously influenced by affect. Qualitatively, this\nmimics the way that particular emotions in the human brain correlate with the dissemination\nthroughout the brain of particular neurotransmitters, which then affect synaptic activity.\nA result of this architecture is that in Joshua Blue, emotion directs attention in a very direct\nway: affective weighting is important in determining which associated objects will become part of\nthe focus of attention, or will be retained from memory. A notable similarity between CogPrime\nand Joshua Blue is that in both systems, nodes are assigned two quantitative attention values,\none governing allocation of current system resources (mainly processor time; this is CogPrime’s\nShortTermImportance) and one governing the long-term allocation of memory (CogPrime’s\nLongTermImportance).\nThe concrete work done with Joshua Blue involved using it to control a simple agent in a simulated\nworld, with the goal that via human interaction, the agent would develop a complex and\nhumanlike emotional and motivational structure from its simple in-built emotions and drives,\nand would then develop complex cognitive capabilities as part of this development process.\n4.5.7 LIDA\nThe LIDA architecture developed by Stan Franklin and his colleagues [BF09] is based on the\nconcept of the “cognitive cycle” - a notion that is important to nearly every BICA (Biologically\nInspired Cognitive Architectures) and also to the brain, but that plays a particularly central\nrole in LIDA. As Franklin says, \"as a matter of principle, every autonomous agent, be it human,\nanimal, or artificial, must frequently sample (sense) its environment, process (make sense of)\nthis input, and select an appropriate response (action). The agent’s “life” can be viewed as\nconsisting of a continual sequence of iterations of these cognitive cycles. Such cycles constitute\nthe indivisible elements of attention, the least sensing and acting to which we can attend. A\ncognitive cycle can be thought of as a moment of cognition, a cognitive \"moment\".\"\n4.5.8 The Global Workspace\nLIDA is heavily based on the “global workspace” concept developed by Bernard Baars. As this\nconcept is also directly relevant to CogPrime it is worth briefly describing here.\nIn essence Baars’ Global Workspace Theory (GWT) is a particular hypothesis about how\nworking memory works and the role it plays in the mind. Baars conceives working memory as the\n4.5 Globalist versus Localist Representations 85\n“inner domain in which we can rehearse telephone numbers to ourselves or, more interestingly,\nin which we carry on the narrative of our lives. It is usually thought to include inner speech\nand visual imagery.” Baars uses the term “consciousness” to refer to the contents of working\nmemory – a theoretical commitment that is not part of the CogPrime design. In this section\nwe will use the term “consciousness” in Baars’ way, but not throughout the rest of the book.\nBaars conceives working memory and consciousness in terms of a “theater metaphor” – according\nto which, in the “theater of consciousness” a “spotlight of selective attention” shines\na bright spot on stage. The bright spot reveals the global workspace – the contents of consciousness,\nwhich may be metaphorically considered as a group of actors moving in and out of\nconsciousness, making speeches or interacting with each other. The unconscious is represented\nby the audience watching the play ... and there is also a role for the director (the mind’s executive\nprocesses) behind the scenes, along with a variety of helpers like stage hands, script\nwriters, scene designers, etc.\nGWT describes a fleeting memory with a duration of a few seconds. This is much shorter\nthan the 10-30 seconds of classical working memory – according to GWT there is a very brief\n“cognitive cycle” in which the global workspace is refreshed, and the time period an item remains\nin working memory generally spans a large number of these elementary “refresh” actions. GWT\ncontents are proposed to correspond to what we are conscious of, and are said to be broadcast\nto a multitude of unconscious cognitive brain processes. Unconscious processes, operating in\nparallel, can form coalitions which can act as input processes to the global workspace. Each\nunconscious process is viewed as relating to certain goals, and seeking to get involved with\ncoalitions that will get enough importance to become part of the global workspace – because\nonce they’re in the global workspace they’ll be allowed to broadcast out across the mind as a\nwhole, which include broadcasting to the internal and external actuators that allow the mind\nto do things. Getting into the global workspace is a process’s best shot at achieving its goals.\nObviously, the theater metaphor used to describe the GWT is evocative but limited; for\ninstance, the unconscious in the mind does a lot more than the audience in a theater. The\nunconscious comes up with complex creative ideas sometimes, which feed into consciousness –\nalmost as if the audience is also the scriptwriter. Baars’ theory, with its understanding of unconscious\ndynamics in terms of coalition-building, fails to describe the subtle dynamics occurring\nwithin the various forms of long-term memory, which result in subtle nonlinear interactions\nbetween long term memory and working memory. But nevertheless, GWT successfully models\na number of characteristics of consciousness, including its role in handling novel situations, its\nlimited capacity, its sequential nature, and its ability to trigger a vast range of unconscious\nbrain processes. It is the framework on which LIDA’s theory of the cognitive cycle is built.\n4.5.9 The LIDA Cognitive Cycle\nThe simplest cognitive cycle is that of an animal, which senses the world, compares sensation to\nmemory, and chooses an action, all in one fluid subjective moment. But the same cognitive cycle\nstructure/process applies to higher-level cognitive processes as well. The LIDA architecture is\nbased on the LIDA model of the cognitive cycle, which posits a particular structure underlying\nthe cognitive cycle that possess the generality to encompass both simple and complex cognitive\nmoments.\n86 4 Brief Survey of Cognitive Architectures\nThe LIDA cognitive cycle itself is a theoretical construct that can be implemented in many\nways, and indeed other BICAs like CogPrime and Psi also manifest the LIDA cognitive cycle\nin their dynamics, though utilizing different particular structures to do so.\nFigure 4.13 shows the cycle pictorially, starting in the upper left corner and proceeding\nclockwise. At the start of a cycle, the LIDA agent perceives its current situation and allocates\nattention differentially to various parts of it. It then broadcasts information about the most\nimportant parts (which constitute the agent’s consciousness), and this information gets features\nextracted from it, when then get passed along to episodic and semantic memory, that interact\nin the “global workspace” to create a model for the agent’s current situation. This model then,\nin interaction with procedural memory, enables the agent to choose an appropriate action and\nexecute it - the critical “action-selection” phase!\nFig. 4.13: The LIDA Cognitive Cycle\nThe LIDA Cognitive Cycle in More Depth\n2\nWe now run through the cognitive cycle in more detail. It begins with sensory stimuli from\nthe agent’s external internal environment. Low-level feature detectors in sensory memory begin\nthe process of making sense of the incoming stimuli. These low-level features are passed to\nperceptual memory where higher-level features, objects, categories, relations, actions, situations,\n2 This section paraphrases heavily from [Fra06]\n4.5 Globalist versus Localist Representations 87\netc. are recognized. These recognized entities, called percepts, are passed to the workspace,\nwhere a model of the agent’s current situation is assembled.\nWorkspace structures serve as cues to the two forms of episodic memory, yielding both short\nand long term remembered local associations. In addition to the current percept, the workspace\ncontains recent percepts that haven’t yet decayed away, and the agent’s model of the thencurrent\nsituation previously assembled from them. The model of the agent’s current situation is\nupdated from the previous model using the remaining percepts and associations. This updating\nprocess will typically require looking back to perceptual memory and even to sensory memory,\nto enable the understanding of relations and situations. This assembled new model constitutes\nthe agent’s understanding of its current situation within its world. Via constructing the model,\nthe agent has made sense of the incoming stimuli.\nNow attention allocation comes into play, because a real agent lacks the computational resources\nto work with all parts of its world-model with maximal mental focus. Portions of the\nmodel compete for attention. These competing portions take the form of (potentially overlapping)\ncoalitions of structures comprising parts the model. Once one such coalition wins the\ncompetition, the agent has decided what to focus its attention on.\nAnd now comes the purpose of all this processing: to help the agent to decide what to do\nnext. The winning coalition passes to the global workspace, the namesake of Global Workspace\nTheory, from which it is broadcast globally. Though the contents of this conscious broadcast\nare available globally, the primary recipient is procedural memory, which stores templates of\npossible actions including their context and possible results.\nProcedural memory also stores an activation value for each such template – a value that\nattempts to measure the likelihood of an action taken within its context producing the expected\nresult. It’s worth noting that LIDA makes a rather specific assumption here. LIDA’s\n“activation” values are like the probabilistic truth values of the implications in CogPrime’s\nContext ∧ Procedure → Goal triples. However, in CogPrime this probability is not the same as\nthe ShortTermImportance “attention value” associated with the Implication link representing\nthat implication. Here LIDA merges together two concepts that in CogPrime are separate.\nTemplates whose contexts intersect sufficiently with the contents of the conscious broadcast\ninstantiate copies of themselves with their variables specified to the current situation. These\ninstantiations are passed to the action selection mechanism, which chooses a single action from\nthese instantiations and those remaining from previous cycles. The chosen action then goes to\nsensorimotor memory, where it picks up the appropriate algorithm by which it is then executed.\nThe action so taken affects the environment, and the cycle is complete.\nThe LIDA model hypothesizes that all human cognitive processing is via a continuing iteration\nof such cognitive cycles. It acknowledges that other cognitive processes may also occur,\nrefining and building on the knowledge used in the cognitive cycle (for instance, the cognitive\ncycle itself doesn’t mention abstract reasoning or creativity). But the idea is that these other\nprocesses occur in the context of the cognitive cycle, which is the main loop driving the internal\nand external activities of the organism.\n4.5.9.1 Avoiding Combinatorial Explosion via Adaptive Attention Allocation\nLIDA avoids combinatorial explosions in its inference processes via two methods, both of which\nare also important in CogPrime :\n• combining reasoning via association with reasoning via deduction\n88 4 Brief Survey of Cognitive Architectures\n• foundational use of uncertainty in reasoning\nOne can create an analogy between LIDA’s workspace structures and codelets and a logicbased\narchitecture’s assertions and functions. However, LIDA’s codelets only operate on the\nstructures that are active in the workspace during any given cycle. This includes recent perceptions,\ntheir closest matches in other types of memory, and structures recently created by other\ncodelets. The results with the highest estimate of success, i.e. activation, will then be selected.\nUncertainty plays a role in LIDA’s reasoning in several ways, most notably through the base\nactivation of its behavior codelets, which depend on the model’s estimated probability of the\ncodelet’s success if triggered. LIDA observes the results of its behaviors and updates the base\nactivation of the responsible codelets dynamically.\nWe note that for this kind of uncertain inference/activation interplay to scale well, some\nlevel of cognitive synergy must be present; and based on our understanding of LIDA it is not\nclear to us whether the particular inference and association algorithms used in LIDA possess\nthe requisite synergy.\n4.5.9.2 LIDA versus CogPrime\nThe LIDA cognitive cycle, broadly construed, exists in CogPrime as in other cognitive architectures.\nTo see how, it suffices to map the key LIDA structures into corresponding CogPrime\nstructures, as is done in Table 4.1. Of course this table does not cover all CogPrime processes,\nas LIDA does not constitute a thorough explanation of CogPrime structure and dynamics. And\nin most cases the corresponding CogPrime and LIDA processes don’t work in exactly the same\nway; for instance, as noted above, LIDA’s action selection relies solely on LIDA’s “activation”\nvalues, whereas CogPrime’s action selection process is more complex, relying on aspects of\nCogPrime that lack LIDA analogues.\n4.5.10 Psi and MicroPsi\nWe have saved for last the architecture that has the most in common with CogPrime : Joscha\nBach’s MicroPsi architecture, closely based on Dietrich Dorner’s Psi theory. CogPrime has\nborrowed substantially from Psi in its handling of emotion and motivation; but Psi also has\nother aspects that differ considerably from CogPrime. Here we will focus more heavily on the\npoints of overlap, but will mention the key points of difference as well.\nThe overall Psi cognitive architecture, which is centered on the Psi model of the motivational\nsystem, is roughly depicted in Figure 4.14.\nPsi’s motivational system begins with Demands, which are the basic factors that motivate\nthe agent. For an animal these would include things like food, water, sex, novelty, socialization,\nprotection of one’s children, and so forth. For an intelligent robot they might include things\nlike electrical power, novelty, certainty, socialization, well-being of others and mental growth.\nPsi also specifies two fairly abstract demands and posits them as psychologically fundamental\n(see Figure 4.15):\n• competence, the effectiveness of the agent at fulfilling its Urges\n• certainty, the confidence of the agent’s knowledge\n4.5 Globalist versus Localist Representations 89\nLIDA\nCogPrime\nDeclarative memory Atomspace\nattentional codelets Schema that adjust importance of Atoms explicitly\ncoalitions\nmaps\nglobal workspace\nattentional focus\nbehavior codelets\nschema\nprocedural memory (scheme net) procedures in ProcedureRepository; and network of\nSchemaNodes in the Atomspace\naction selection (behavior net) propagation of STICurrency from goals to actions, and\naction selection process\ntransient episodic memory perceptual atoms entering AT with high STI, which\nrapidly decreases in most cases\nlocal workspaces\nbubbles of interlinked Atoms with moderate importance,\nfocused on by a subset of MindAgents (defined\nin Chapter 19 of Part 2) for a period of time\nperceptual associative memory HebbianLinks in the AT\nsensory memory\nspaceserver/timeserver, plus auxiliary stores for other\nsenses\nsensorimotor memory Atoms storing record of actions taken, linked in with\nAtoms indexed in sensory memory\nTable 4.1: CogPrime Analogues of Key LIDA Features\nEach demand is assumed to come with a certain “target level” or “target range” (and these\nmay fluctuate over time, or may change as a system matures and develops). An Urge is said to\ndevelop when a demand deviates from its target range: the urge then seeks to return the demand\nto its target range. For instance, in an animal-like agent the demand related to food is more\nclearly described as “fullness,” and there is a target range indicating that the agent is neither too\nhungry nor too full of food. If the agent’s fullness deviates from this range, an Urge to return\nthe demand to its target range arises. Similarly, if an agent’s novelty deviates from its target\nrange, this means the agent’s life has gotten either too boring or too disconcertingly weird, and\nthe agent gets an Urge for either more interesting activities (in the case of below-range novelty)\nor more familiar ones (in the case of above-range novelty).\nThere is also a primitive notion of Pleasure (and its opposite, displeasure), which is considered\nas different from the complex emotion of “happiness.” Pleasure is understood as associated\nwith Urges: pleasure occurs when an Urge is (at least partially) satisfied, whereas displeasure\noccurs when an urge gets increasingly severe. The degree to which an Urge is satisfied is not\nnecessarily defined instantaneously; it may be defined, for instance, as a time-decaying weighted\naverage of the proximity of the demand to its target range over the recent past.\nSo, for instance if an agent is bored and gets a lot of novel stimulation, then it experiences\nsome pleasure. If it’s bored and then the monotony of its stimulation gets even more extreme,\nthen it experiences some displeasure.\nNote that, according to this relatively simplistic approach, any decrease in the amount of\ndissatisfaction causes some pleasure; whereas if everything always continues within its acceptable\nrange, there isn’t any pleasure. This may seem a little counterintuitive, but it’s important\nto understand that these simple definitions of “pleasure” and “displeasure” are not intended to\nfully capture the natural language concepts associated with those words. The natural language\nterms are used here simply as heuristics to convey the general character of the processes in-\n90 4 Brief Survey of Cognitive Architectures\nFig. 4.14: High-Level Architecture of the Psi Model\nvolved. These are very low level processes whose analogues in human experience are largely\nbelow the conscious level.\nA Goal is considered as a statement that the system may strive to make true at some future\ntime. A Motive is an (urge, goal) pair, consisting of a goal whose satisfaction is predicted to\nimply the satisfaction of some urge. In fact one may consider Urges as top-level goals, and the\nagent’s other goals as their subgoals.\nIn Psi an agent has one “ruling motive” at any point in time, but this seems an oversimplification\nmore applicable to simple animals than to human-like or other advanced AI systems.\nIn general one may think of different motives having different weights indicating the amount of\nresources that will be spent on pursuing them.\nEmotions in Psi are considered as complex systemic response-patterns rather than explicitly\nconstructed entities. An emotion is the set of mental entities activated in response to a certain\nset of urges. Dorner conceived theories about how various common emotions emerge from the\ndynamics of urges and motives as described in the Psi model. “Intentions” are also considered as\ncomposite entities: an intention at a given point in time consists of the active motives, together\nwith their related goals, behavior programs and so forth.\n4.5 Globalist versus Localist Representations 91\nThe basic logic of action in Psi is carried out by “triples” that are very similar to CogPrime’s\nContext ∧ Procedure → Goal triples. However, an important role is played by four modulators\nthat control how the processes of perception, cognition and action selection are regulated at a\ngiven time:\n• activation, which determines the degree to which the agent is focused on rapid, intensive\nactivity versus reflective, cognitive activity\n• resolution level, which determines how accurately the system tries to perceive the world\n• certainty, which determines how hard the system tries to achieve definite, certain knowledge\n• selection threshold, which determines how willing the system is to change its choice of which\ngoals to focus on\nThese modulators characterize the system’s emotional and cognitive state at a very abstract\nlevel; they are not emotions per se, but they have a large effect on the agent’s emotions. Their\nintended interaction is depicted in Figure 4.15.\nFig. 4.15: Primary Interrelationships Between Psi Modulators\n4.5.11 The Emergence of Emotion in the Psi Model\nWe now briefly review the specifics of how Psi models the emergence of emotion. The basic idea is\nto define a small set of proto-emotional dimensions in terms of basic Urges and modulators.\nThen, emotions are identified with regions in the space spanned by these dimensions.\nThe simplest approach uses a six-dimensional continuous space:\n1. pleasure\n92 4 Brief Survey of Cognitive Architectures\n2. arousal\n3. resolution level\n4. selection threshold (i.e. degree of dominance of the leading motive)\n5. level of background checks (the rate of the securing behavior)\n6. level of goal-directed behavior\nFigure 4.16 shows how the latter 5 of these dimensions are derived from underlying urges and\nmodulators. Note that these dimensions are not orthogonal; for instance resolution is mainly inversely\nrelated to arousal. Additional dimensions are also discussed, for instance it is postulated\nthat to deal with social emotions one may wish to introduce two more demands corresponding\nto inner and outer obedience to social norms, and then define dimensions in terms of these.\nFig. 4.16: Five Proto-Emotional Dimensions Implicit in the Psi Model\nSpecific emotions are then characterized in terms of these dimensions. According to [Bac09],\nfor instance, “Anger ... is characterized by high arousal, low resolution, strong motive dominance,\nfew background checks and strong goal-orientedness; sadness by low arousal, high resolution,\nstrong dominance, few background-checks and low goal-orientedness.”\nI’m a bit skeptical of the contention that these dimensions fully characterize the relevant\nemotions. Anger for instance seems to have some particular characteristics not implied by the\nabove list of dimensional values. The list of dimensional values associated with anger doesn’t\ntell us that an angry person is more likely to punch someone than to bounce up and down,\nfor example. However, it does seem that the dimensional values associated with an emotion are\n4.5 Globalist versus Localist Representations 93\ninformative about the emotion, so that positioning an emotion on the given dimensions tells\none a lot.\n4.5.12 Knowledge Representation, Action Selection and Planning in\nPsi\nIn addition to the basic motivation/emotion architecture of Psi, which has been adopted (with\nsome minor changes) for use in CogPrime, Psi has a number of other aspects that are somewhat\ndifferent from their CogPrime analogues.\nFirst of all, on the micro level, Psi represents knowledge using structures called “quads.” Each\nquad is a cluster of 5 neurons containing a core neuron, and four other neurons representing\nbefore/after and part-of/has-part relationships in regard to that core neuron. Quads are naturally\nassembled into spatiotemporal hierarchies, though they are not required to form part of\nsuch a structure.\nPsi stores knowledge using quads arranged in three networks, which are conceptually similar\nto the networks in Albus’s 4D/RCS and Arel’s DeSTIN architectures:\n• A sensory network, which stores declarative knowledge: schemas representing images, objects,\nevents and situations as hierarchical structures.\n• A motor network, which contains procedural knowledge by way of hierarchical behavior\nprograms\n• A motivational network handling demands\nPerception in Psi, which is centered in the sensory network, follows principles similar to\nDeSTIN (which are shared also by other systems), for instance the principle of perception as\nprediction. Psi’s “HyPercept” mechanism performs hypothesis-based perception: it attempts to\npredict what is there to be perceived and then attempts to verify these predictions using sensation\nand memory. Furthermore HyPercept is intimately coupled with actions in the external\nworld, according to the concept of “Neisser’s perceptual cycle,” the cycle between exploration\nand representation of reality. Perceptually acquired information is translated into schemas capable\nof guiding behaviors, and these are enacted (sometimes affecting the world in significant\nways) and in the process used to guide further perception. Imaginary perceptions are handled\nvia a “mental stage” analogous to CogPrime’s internal simulation world.\nAction selection in Psi works based on what are called “triplets,” each of which consists of\n• a sensor schema (pre-conditions, “condition schema”; like CogPrime’s “context”)\n• a subsequent motor schema (action, effector; like CogPrime’s “procedure”)\n• a final sensor schema (post-conditions, expectations; like an CogPrime predicate or goal)\nWhat distinguishes these triplets from classic production rules as used in (say) Soar and\nACT-R is that the triplets may be partial (some of the three elements may be missing) and\nmay be uncertain. However, there seems no fundamental difference between these triplets and\nCogPrime’s concept/procedure/goal triplets, at a high level; the difference lies in the underlying\nknowledge representation used for the schemata, and the probabilistic logic used to represent\nthe implication.\nThe work of figuring out what schema to execute to achieve the chosen goal in the current\ncontext is done in Psi using a combination of processes called the “Rasmussen ladder” (named\n94 4 Brief Survey of Cognitive Architectures\nafter Danish psychologist Jens Rasmussen). The Rasmussen ladder describes the organization\nof action as a movement between the stages of skill-based behavior, rule-based behavior and\nknowledge-based behavior, as follows:\n• If a given task amounts to a trained routine, an automatism or skill is activated; it can\nusually be executed without conscious attention and deliberative control.\n• If there is no automatism available, a course of action might be derived from rules; before a\nknown set of strategies can be applied, the situation has to be analyzed and the strategies\nhave to be adapted.\n• In those cases where the known strategies are not applicable, a way of combining the\navailable manipulations (operators) into reaching a given goal has to be explored at first.\nThis stage usually requires a recomposition of behaviors, that is, a planning process.\nThe planning algorithm used in the Psi and MicroPsi implementations is a fairly simple\nhill-climbing planner. While it’s hypothesized that a more complex planner may be needed for\nadvanced intelligence, part of the Psi theory is the hypothesis that most real-life planning an\norganism needs to do is fairly simple, once the organism has the right perceptual representations\nand goals.\n4.5.13 Psi versus CogPrime\nOn a high level, the similarities between Psi and CogPrime are quite strong:\n• interlinked declarative, procedural and intentional knowledge structures, represented using\nneural-symbolic methods (though, the knowledge structures have somewhat different highlevel\nstructures and low-level representational mechanisms in the two systems)\n• perception via prediction and perception/action integration\n• action selection via triplets that resemble uncertain, potentially partial production rules\n• similar motivation/emotion framework, since CogPrime incorporates a variant of Psi for\nthis\nOn the nitty-gritty level there are many differences between the systems, but on the bigpicture\nlevel the main difference lies in the way the cognitive synergy principle is pursued in\nthe two different approaches. Psi and MicroPsi rely on very simple learning algorithms that are\nclosely tied to the “quad” neurosymbolic knowledge representation, and hence interoperate in\na fairly natural way without need for subtle methods of “synergy engineering.” CogPrime uses\nmuch more diverse and sophisticated learning algorithms which thus require more sophisticated\nmethods of interoperation in order to achieve cognitive synergy.\nChapter 5\nA Generic Architecture of Human-Like Cognition\n5.1 Introduction\nWhen writing the first draft of this book, some years ago, we had the idea to explain CogPrime\nby aligning its various structures and processes with the ones in the \"standard architecture\ndiagram\" of the human mind. After a bit of investigation, though, we gradually came to the\nrealization that no such thing existed. There was no standard flowchart or other sort of diagram\nexplaining the modern consensus on how human thought works. Many such diagrams\nexisted, but each one seemed to represent some particular focus or theory, rather than an overall\nintegrative understanding.\nSince there are multiple opinions regarding nearly every aspect of human intelligence, it\nwould be difficult to get two cognitive scientists to fully agree on every aspect of an overall\nhuman cognitive architecture diagram. Prior attempts to outline detailed mind architectures\nhave tended to follow highly specific theories of intelligence, and hence have attracted only\nmoderate interest from researchers not adhering to those theories. An example is Minsky’s work\npresented in The Emotion Machine [Min07], which arguably does constitute an architecture\ndiagram for the human mind, but which is only loosely grounded in current empirical knowledge\nand stands more as a representation of Minsky’s own intuitive understanding.\nBut nevertheless, it seemed to us that a reasonable attempt at an integrative, relatively\ntheory-neutral \"human cognitive architecture diagram\" would be better than nothing. So naturally,\nwe took it on ourselves to create such a diagram. This chapter is the result – it draws on\nthe thinking of a number of cognitive science and AGI researchers, integrating their perspectives\nin a coherent, overall architecture diagram for human, and human-like, general intelligence. The\nspecific architecture diagram of CogPrime, given in Chapter 6 below, may then be understood\nas a particular instantiation of this generic architecture diagram of human-like cognition.\nThere is no getting around the fact that, to a certain extent, the diagram presented here\nreflects our particular understanding of how the mind works. However, it was intentionally\nconstructed with the goal of not being just an abstracted version of the CogPrime architecture\ndiagram! It does not reflect our own idiosyncratic understanding of human intelligence, as much\nas a combination of understandings previously presented by multiple researchers (including\nourselves), arranged according to our own taste in a manner we find conceptually coherent.\nWith this in mind, we call it the \"Integrative Human-Like Cognitive Architecture Diagram,\" or\nfor short \"the integrative diagram.\" We have made an effort to ensure that as many pieces of\nthe integrative diagram as possible are well grounded in psychological and even neuroscientific\n95\n96 5 A Generic Architecture of Human-Like Cognition\ndata, rather than mainly embodying speculative notions; however, given the current state of\nknowledge, this could not be done to a complete extent, and there is still some speculation\ninvolved here and there.\nWhile based on understandings of human intelligence, the integrative diagram is intended to\nserve as an architectural outline for human-like general intelligence more broadly. For example,\nCogPrime is explicitly not intended as a precise emulation of human intelligence, and does many\nthings quite differently than the human mind, yet can still fairly straightforwardly be mapped\ninto the integrative diagram.\nThe integrative diagram focuses on structure, but this should not be taken to represent a\nvaluation of structure over dynamics in our approach to intelligence. Following chapters treat\nvarious dynamical phenomena in depth.\n5.2 Key Ingredients of the Integrative Human-Like Cognitive\nArchitecture Diagram\nThe main ingredients we’ve used in assembling the integrative diagram are as follows:\n• Our own views on the various types of memory critical for human-like cognition, and the\nneed for tight, \"synergetic\" interactions between the cognitive processes focused on these\n• Aaron Sloman’s high-level architecture diagram of human intelligence [Slo01], drawn from\nhis CogAff architecture, which strikes me as a particularly clear embodiment of \"modern\ncommon sense\" regarding the overall architecture of the human mind. We have added only\na couple items to Sloman’s high-level diagram, which we felt deserved an explicit high-level\nrole that he did not give them: emotion, language and reinforcement.\n• The LIDA architecture diagram presented by Stan Franklin and Bernard Baars [BF09].\nWe think LIDA is an excellent model of working memory and what Sloman calls \"reactive\nprocesses\", with well-researched grounding in the psychology and neuroscience literature.\nWe have adapted the LIDA diagram only very slightly for use here, changing some of\nthe terminology on the arrows, and indicating where parts of the LIDA diagram indicate\nprocesses elaborated in more detail elsewhere in the integrative diagram.\n• The architecture diagram of the Psi model of motivated cognition, presented by Joscha\nBach in [Bac09] based on prior work by Dietrich Dorner [Dör02]. This diagram is presented\nwithout significant modification; however it should be noted that Bach and Dorner present\nthis diagram in the context of larger and richer cognitive models, the other aspects of which\nare not all incorporated in the integrative diagram.\n• James Albus’s three-hierarchy model of intelligence [AM01], involving coupled perception,\naction and reinforcement hierarchies. Albus’s model, utilized in the creation of intelligent\nunmanned automated vehicles, is a crisp embodiment of many ideas emergent from the field\nof intelligent control systems.\n• Deep learning networks as a model of perception (and action and reinforcement learning),\nas embodied for example in the work of Itamar Arel [ARC09] and Jeff Hawkins [HB06]. The\nintegrative diagram adopts this as the basic model of the perception and action subsystems\nof human intelligence. Language understanding and generation are also modeled according\nto this paradigm.\n5.3 An Architecture Diagram for Human-Like General Intelligence 97\nOne possible negative reaction to the integrative diagram might be to say that it’s a kind\nof Frankenstein monster diagram, piecing together aspects of different theories in a way that\nviolates the theoretical notions underlying all of them! For example, the integrative diagram\ntakes LIDA as a model of working memory and reactive processing, but from the papers on\nLIDA it’s unclear whether the creators of LIDA construe it more broadly than that. The deep\nlearning community tends to believe that the architecture of current deep learning networks,\nin itself, is close to sufficient for human-level general intelligence – whereas the integrative\ndiagram appropriates the ideas from this community mainly for handling perception, action\nand language, etc.\nOn the other hand, in a more positive perspective, one could view the integrative diagram\nas consistent with LIDA, but merely providing much more detail on some of the boxes in the\nLIDA diagram (e.g. dealing with perception and long-term memory). And one could view the\nintegrative diagram as consistent with the deep learning paradigm – via viewing it, not as\na description of components to be explicitly implemented in an AGI system, but rather as a\ndescription of the key structures and processes that must emerge in deep learning network, based\non its engagement with the world, in order for it to achieve human-like general intelligence.\nOur own view, underlying the creation of the integrative diagram, is that different communities\nof cognitive science researchers have focused on different aspects of intelligence, and have\nthus each created models that are more fully fleshed out in some aspects than others. But these\nvarious models all link together fairly cleanly, which is not surprising as they are all grounded\nin the same data regarding human intelligence. Many judgment calls must be made in fusing\nmultiple models in the way that the integrative diagram does, but we feel these can be made\nwithout violating the spirit of the component models. In assembling the integrative diagram, we\nhave made these judgment calls as best we can, but we’re well aware that different judgments\nwould also be feasible and defensible. Revisions are likely as time goes on, not only due to\nnew data about human intelligence but also to evolution of understanding regarding the best\napproach to model integration.\nAnother possible argument against the ideas presented here is that there’s nothing new – all\nthe ingredients presented have been given before elsewhere. To this our retort is to quote Pascal:\n\"Let no one say that I have said nothing new ... the arrangement of the subject is new.\" The\nvarious architecture diagrams incorporated into the integrative diagram are either extremely\nhigh level (Sloman’s diagram) or focus primarily on one aspect of intelligence, treating the\nothers very concisely by summarizing large networks of distinction structures and processes in\nsmall boxes. The integrative diagram seeks to cover all aspects of human-like intelligence at a\nroughly equal granularity – a different arrangement.\nThis kind of high-level diagramming exercise is not precise enough, nor dynamics-focused\nenough, to serve as a guide for creating human-level or more advanced AGI. But it can be a\nuseful tool for explaining and interpreting a concrete AGI design, such as CogPrime.\n5.3 An Architecture Diagram for Human-Like General Intelligence\nThe integrative diagram is presented here in a series of seven Figures.\nFigure 5.1 gives a high-level breakdown into components, based on Sloman’s high-level\ncognitive-architectural sketch [Slo01]. This diagram represents, roughly speaking, \"modern common\nsense\" about how a human-like mind is architected. The separation between structures\n98 5 A Generic Architecture of Human-Like Cognition\nFig. 5.1: High-Level Architecture of a Human-Like Mind\nand processes, embodied in having separate boxes for Working Memory vs. Reactive Processes,\nand for Long Term Memory vs. Deliberative Processes, could be viewed as somewhat artificial,\nsince in the human brain and most AGI architectures, memory and processing are closely integrated.\nHowever, the tradition in cognitive psychology is to separate out Working Memory and\nLong Term Memory from the cognitive processes acting thereupon, so we have adhered to that\nconvention. The other changes from Sloman’s diagram are the explicit inclusion of language,\nrepresenting the hypothesis that language processing is handled in a somewhat special way in\nthe human brain; and the inclusion of a reinforcement component parallel to the perception and\naction hierarchies, as inspired by intelligent control systems theory (e.g. Albus as mentioned\nabove) and deep learning theory. Of course Sloman’s high level diagram in its original form is\nintended as inclusive of language and reinforcement, but we felt it made sense to give them\nmore emphasis.\nFigure 5.2, modeling working memory and reactive processing, is essentially the LIDA diagram\nas given in prior papers by Stan Franklin, Bernard Baars and colleagues [BF09]. The\nboxes in the upper left corner of the LIDA diagram pertain to sensory and motor processing,\nwhich LIDA does not handle in detail, and which are modeled more carefully by deep learning\ntheory. The bottom left corner box refers to action selection, which in the integrative diagram\nis modeled in more detail by Psi. The top right corner box refers to Long-Term Memory, which\nthe integrative diagram models in more detail as a synergetic multi-memory system (Figure\n5.4).\nThe original LIDA diagram refers to various \"codelets\", a key concept in LIDA theory. We\nhave replaced \"attention codelets\" here with \"attention flow\", a more generic term. We suggest\none can think of an attention codelet as: a piece of information stating that, for a certain group\nof items, it’s currently pertinent to pay attention to this group as a collective.\n5.3 An Architecture Diagram for Human-Like General Intelligence 99\nFig. 5.2: Architecture of Working Memory and Reactive Processing, closely modeled on the\nLIDA architecture\nFigure 5.3, modeling motivation and action selection, is a lightly modified version of the\nPsi diagram from Joscha Bach’s book Principles of Synthetic Intelligence [Bac09]. The main\ndifference from Psi is that in the integrative diagram the Psi motivated action framework is\nembedded in a larger, more complex cognitive model. Psi comes with its own theory of working\nand long-term memory, which is related to but different from the one given in the integrative\ndiagram – it views the multiple memory types distinguished in the integrative diagram as\nemergent from a common memory substrate. Psi comes with its own theory of perception and\naction, which seems broadly consistent with the deep learning approach incorporated in the\nintegrative diagram. Psi’s handling of working memory lacks the detailed, explicit workflow of\nLIDA, though it seems broadly conceptually consistent with LIDA.\nIn Figure 5.3, the box labeled \"Other portions of working memory\" is labeled \"Protocol and\nsituation memory\" in the original Psi diagram. The Perception, Action Execution and Action\nSelection boxes have fairly similar semantics to the similarly labeled boxes in the LIDA-like\nFigure 5.2, so that these diagrams may be viewed as overlapping. The LIDA model doesn’t\nexplain action selection and planning in as much detail as Psi, so the Psi-like Figure 5.3 could\nbe viewed as an elaboration of the action-selection portion of the LIDA-like Figure 5.2. In\nPsi, reinforcement is considered as part of the learning process involved in action selection and\nplanning; in Figure 5.3 an explicit \"reinforcement box\" has been added to the original Psi\ndiagram, to emphasize this.\nFigure 5.4, modeling long-term memory and deliberative processing, is derived from our own\nprior work studying the \"cognitive synergy\" between different cognitive processes associated\nwith different types of memory. The division into types of memory is fairly standard. Declarative,\nprocedural, episodic and sensorimotor memory are routinely distinguished; we like to distinguish\nattentional memory and intentional (goal) memory as well, and view these as the interface\nbetween long-term memory and the mind’s global control systems. One focus of our AGI design\nwork has been on designing learning algorithms, corresponding to these various types of memory,\n100 5 A Generic Architecture of Human-Like Cognition\nFig. 5.3: Architecture of Motivated Action\nFig. 5.4: Architecture of Long-Term Memory and Deliberative and Metacognitive Thinking\nthat interact with each other in a synergetic way [Goe09c], helping each other to overcome\ntheir intrinsic combinatorial explosions. There is significant evidence that these various types\nof long-term memory are differently implemented in the brain, but the degree of structure and\ndynamical commonality underlying these different implementations remains unclear.\n5.3 An Architecture Diagram for Human-Like General Intelligence 101\nEach of these long-term memory types has its analogue in working memory as well. In some\ncognitive models, the working memory and long-term memory versions of a memory type and\ncorresponding cognitive processes, are basically the same thing. CogPrime is mostly like this\n– it implements working memory as a subset of long-term memory consisting of items with\nparticularly high importance values. The distinctive nature of working memory is enforced via\nusing slightly different dynamical equations to update the importance values of items with\nimportance above a certain threshold. On the other hand, many cognitive models treat working\nand long term memory as more distinct than this, and there is evidence for significant functional\nand anatomical distinctness in the brain in some cases. So for the purpose of the integrative\ndiagram, it seemed best to leave working and long-term memory subcomponents as parallel but\ndistinguished.\nFigure 5.4 also encompasses metacognition, under the hypothesis that in human beings and\nhuman-like minds, metacognitive thinking is carried out using basically the same processes as\nplain ordinary deliberative thinking, perhaps with various tweaks optimizing them for thinking\nabout thinking. If it turns out that humans have, say, a special kind of reasoning faculty\nexclusively for metacognition, then the diagram would need to be modified. Modeling of self\nand others is understood to occur via a combination of metacognition and deliberative thinking,\nas well as via implicit adaptation based on reactive processing.\nFig. 5.5: Architecture for Multimodal Perception\nFigure 5.5 models perception, according to the basic ideas of deep learning theory. Vision and\naudition are modeled as deep learning hierarchies, with bottom-up and top-down dynamics. The\nlower layers in each hierarchy refer to more localized patterns recognized in, and abstracted from,\nsensory data. Output from these hierarchies to the rest of the mind is not just through the top\nlayers, but via some sort of sampling from various layers, with a bias toward the top layers. The\ndifferent hierarchies cross-connect, and are hence to an extent dynamically coupled together. It\nis also recognized that there are some sensory modalities that aren’t strongly hierarchical, e.g\n102 5 A Generic Architecture of Human-Like Cognition\ntouch and smell (the latter being better modeled as something like an asymmetric Hopfield net,\nprone to frequent chaotic dynamics [LLW + 05]) – these may also cross-connect with each other\nand with the more hierarchical perceptual subnetworks. Of course the suggested architecture\ncould include any number of sensory modalities; the diagram is restricted to four just for\nsimplicity.\nThe self-organized patterns in the upper layers of perceptual hierarchies may become quite\ncomplex and may develop advanced cognitive capabilities like episodic memory, reasoning, language\nlearning, etc. A pure deep learning approach to intelligence argues that all the aspects\nof intelligence emerge from this kind of dynamics (among perceptual, action and reinforcement\nhierarchies). Our own view is that the heterogeneity of human brain architecture argues against\nthis perspective, and that deep learning systems are probably better as models of perception\nand action than of general cognition. However, the integrative diagram is not committed to\nour perspective on this – a deep-learning theorist could accept the integrative diagram, but\nargue that all the other portions besides the perceptual, action and reinforcement hierarchies\nshould be viewed as descriptions of phenomena that emerge in these hierarchies due to their\ninteraction.\nFig. 5.6: Architecture for Action and Reinforcement\nFigure 5.6 shows an action subsystem and a reinforcement subsystem, parallel to the perception\nsubsystem. Two action hierarchies, one for an arm and one for a leg, are shown for\n5.3 An Architecture Diagram for Human-Like General Intelligence 103\nconcreteness, but of course the architecture is intended to be extended more broadly. In the\nhierarchy corresponding to an arm, for example, the lowest level would contain control patterns\ncorresponding to individual joints, the next level up to groupings of joints (like fingers), the\nnext level up to larger parts of the arm (hand, elbow). The different hierarchies corresponding\nto different body parts cross-link, enabling coordination among body parts; and they also connect\nat multiple levels to perception hierarchies, enabling sensorimotor coordination. Finally\nthere is a module for motor planning, which links tightly with all the motor hierarchies, and\nalso overlaps with the more cognitive, inferential planning activities of the mind, in a manner\nthat is modeled different ways by different theorists. Albus [AM01] has elaborated this kind of\nhierarchy quite elaborately.\nThe reward hierarchy in Figure 5.6 provides reinforcement to actions at various levels on\nthe hierarchy, and includes dynamics for propagating information about reinforcement up and\ndown the hierarchy.\nFig. 5.7: Architecture for Language Processing\nFigure 5.7 deals with language, treating it as a special case of coupled perception and action.\nThe traditional architecture of a computational language comprehension system is a pipeline\n[JM09] [Goe10d], which is equivalent to a hierarchy with the lowest-level linguistic features (e.g.\nsounds, words) at the bottom, and the highest level features (semantic abstractions) at the top,\nand syntactic features in the middle. Feedback connections enable semantic and cognitive modulation\nof lower-level linguistic processing. Similarly, language generation is commonly modeled\nhierarchically, with the top levels being the ideas needing verbalization, and the bottom level\ncorresponding to the actual sentence produced. In generation the primary flow is top-down,\nwith bottom-up flow providing modulation of abstract concepts by linguistic surface forms.\nSo, that’s it – an integrative architecture diagram for human-like general intelligence, split\namong seven different pictures, formed by judiciously merging together architecture diagrams\nproduced via a number of cognitive theorists with different, overlapping foci and research\nparadigms.\nIs anything critical left out of the diagram? A quick perusal of the table of contents of\ncognitive psychology textbooks suggests to me that if anything major is left out, it’s also\nunknown to current cognitive psychology. However, one could certainly make an argument for\nexplicit inclusion of certain other aspects of intelligence, that in the integrative diagram are\n104 5 A Generic Architecture of Human-Like Cognition\nleft as implicit emergent phenomena. For instance, creativity is obviously very important to\nintelligence, but, there is no \"creativity\" box in any of these diagrams – because in our view,\nand the view of the cognitive theorists whose work we’ve directly drawn on here, creativity\nis best viewed as a process emergent from other processes that are explicitly included in the\ndiagrams.\n5.4 Interpretation and Application of the Integrative Diagram\nA tongue-partly-in-cheek definition of a biological pathway is \"a subnetwork of a biological\nnetwork, that fits on a single journal page.\" Cognitive architecture diagrams have a similar\nproperty – they are crude abstractions of complex structures and dynamics, sculpted in accordance\nwith the size of the printed page, and the tolerance of the human eye for absorbing\ndiagrams, and the tolerance of the human author for making diagrams.\nHowever, sometimes constraints – even arbitrary ones – are useful for guiding creative efforts,\ndue to the fact that they force choices. Creating an architecture for human-like general\nintelligence that fits in a few (okay, seven) fairly compact diagrams, requires one to make many\nchoices about what features and relationships are most essential. In constructing the integrative\ndiagram, we have sought to make these choices, not purely according to our own tastes in cognitive\ntheory or AGI system design, but according to a sort of blend of the taste and judgment\nof a number of scientists whose views we respect, and who seem to have fairly compatible,\ncomplementary perspectives.\nWhat is the use of a cognitive architecture diagram like this? It can help to give newcomers\nto the field a basic idea about what is known and suspected about the nature of human-like\ngeneral intelligence. Also, it could potentially be used as a tool for cross-correlating different\nAGI architectures. If everyone who authored an AGI architecture would explain how their architecture\naccounts for each of the structures and processes identified in the integrative diagram,\nthis would give a means of relating the various AGI designs to each other.\nThe integrative diagram could also be used to help connect AGI and cognitive psychology\nto neuroscience in a more systematic way. In the case of LIDA, a fairly careful correspondence\nhas been drawn up between the LIDA diagram nodes and links and various neural structures\nand processes [FB08]. Similar knowledge exists for the rest of the integrative diagram, though\nnot organized in such a systematic fashion. A systematic curation of links between the nodes\nand links in the integrative diagram and current neuroscience knowledge, would constitute an\ninteresting first approximation of the holistic cognitive behavior of the human brain.\nFinally (and harking forward to later chapters), the big omission in the integrative diagram\nis dynamics. Structure alone will only get you so far, and you could build an AGI system with\nreasonable-looking things in each of the integrative diagram’s boxes, interrelating according to\nthe given arrows, and yet still fail to make a viable AGI system. Given the limitations the\nreal world places on computing resources, it’s not enough to have adequate representations\nand algorithms in all the boxes, communicating together properly and capable doing the right\nthings given sufficient resources. Rather, one needs to have all the boxes filled in properly\nwith structures and processes that, when they act together using feasible computing resources,\nwill yield appropriately intelligent behaviors via their cooperative activity. And this has to do\nwith the complex interactive dynamics of all the processes in all the different boxes – which is\n5.4 Interpretation and Application of the Integrative Diagram 105\nsomething the integrative diagram doesn’t touch at all. This brings us again to the network of\nideas we’ve discussed under the name of \"cognitive synergy,\" to be discussed later on.\nIt might be possible to make something similar to the integrative diagram on the level of\ndynamics rather than structures, complementing the structural integrative diagram given here;\nbut this would seem significantly more challenging, because we lack a standard set of tools for\ndepicting system dynamics. Most cognitive theorists and AGI architects describe their structural\nideas using boxes-and-lines diagrams of some sort, but there is no standard method for depicting\ncomplex system dynamics. So to make a dynamical analogue to the integrative diagram, via\na similar integrative methodology, one would first need to create appropriate diagrammatic\nformalizations of the dynamics of the various cognitive theories being integrated – a fascinating\nbut onerous task.\nWhen we first set out to make an integrated cognitive architecture diagram, via combining\nthe complementary insights of various cognitive science and AGI theorists, we weren’t sure how\nwell it would work. But now we feel the experiment was generally a success – the resultant\nintegrated architecture seems sensible and coherent, and reasonably complete. It doesn’t come\nclose to telling you everything you need to know to understand or implement a human-like\nmind – but it tells you the various processes and structures you need to deal with, and which of\ntheir interrelations are most critical. And, perhaps just as importantly, it gives a concrete way\nof understanding the insights of a specific but fairly diverse set of cognitive science and AGI\ntheorists as complementary rather than contradictory. In a CogPrime context, it provides a\nway of tying in the specific structures and dynamics involved in CogPrime, with a more generic\nportrayal of the structures and dynamics of human-like intelligence.\n\nChapter 6\nA Brief Overview of CogPrime\n6.1 Introduction\nJust as there are many different approaches to human flight – airplanes, helicopters, balloons,\nspacecraft, and doubtless many methods no person has thought of yet – similarly, there are likely\nmany different approaches to advanced artificial general intelligence. All the different approaches\nto flight exploit the same core principles of aerodynamics in different ways; and similarly, the\nvarious different approaches to AGI will exploit the same core principles of general intelligence\nin different ways.\nIn the chapters leading up to this one, we have taken a fairly broad view of the project\nof engineering AGI. We have presented a conception and formal model of intelligence, and\ndescribed environments, teaching methodologies and cognitive and developmental pathways\nthat we believe are collectively appropriate for the creation of AGI at the human level and\nultimately beyond, and with a roughly human-like bias to its intelligence. These ideas stand\nalone and may be compatible with a variety of approaches to engineering AGI systems. However,\nthey also set the stage for the presentation of CogPrime, the particular AGI design on which\nwe are currently working.\nThe thorough presentation of the CogPrime design is the job of Part 2 of this book – where,\nnot only are the algorithms and structures involved in CogPrime reviewed in more detailed,\nbut their relationship to the theoretical ideas underlying CogPrime is pursued more deeply.\nThe job of this chapter is a smaller one: to give a high-level overview of some key aspects the\nCogPrime architecture at a mostly nontechnical level, so as to enable you to approach Part\n2 with a little more idea of what to expect. The remainder of Part 1, following this chapter,\nwill present various theoretical notions enabling the particulars, intent and consequences of the\nCogPrime design to be more thoroughly understood.\n6.2 High-Level Architecture of CogPrime\nFigures 6.1, 6.2 , 6.4 and 6.5 depict the high-level architecture of CogPrime, which involves\nthe use of multiple cognitive processes associated with multiple types of memory to enable\nan intelligent agent to execute the procedures that it believes have the best probability of\nworking toward its goals in its current context. In a robot preschool context, for example, the\n107\n108 6 A Brief Overview of CogPrime\ntop-level goals will be simple things such as pleasing the teacher, learning new information\nand skills, and protecting the robot’s body. Figure 6.3 shows part of the architecture via which\ncognitive processes interact with each other, via commonly acting on the AtomSpace knowledge\nrepository.\nComparing these diagrams to the integrative human cognitive architecture diagrams given\nin Chapter 5, one sees the main difference is that the CogPrime diagrams commit to specific\nstructures (e.g. knowledge representations) and processes, whereas the generic integrative architecture\ndiagram refers merely to types of structures and processes. For instance, the integrative\ndiagram refers generally to declarative knowledge and learning, whereas the CogPrime diagram\nrefers to PLN, as a specific system for reasoning and learning about declarative knowledge. Table\n6.1 articulates the key connections between the components of the CogPrime diagram and\nthose of the integrative diagram, thus indicating the general cognitive functions instantiated by\neach of the CogPrime components.\n6.3 Current and Prior Applications of OpenCog\nBefore digging deeper into the theory, and elaborating some of the dynamics underlying the\nabove diagrams, we pause to briefly discuss some of the practicalities of work done with the\nOpenCog system currently implementing parts of the CogPrime architecture.\nOpenCog, the open-source software framework underlying the “OpenCogPrime” (currently\npartial) implementation of the CogPrime architecture, has been used for commercial applications\nin the area of natural language processing and data mining; for instance, see [GPPG06]\nwhere OpenCogPrime’s PLN reasoning and RelEx language processing are combined to do\nautomated biological hypothesis generation based on information gathered from PubMed abstracts.\nMost relevantly to the present work, it has also been used to control virtual agents in\nvirtual worlds [GEA08].\nPrototype work done during 2007-2008 involved using an OpenCog variant called the Open-\nPetBrain to control virtual dogs in a virtual world (see Figure 6.6 for a screenshot of an\nOpenPetBrain-controlled virtual dog). While these OpenCog virtual dogs did not display intelligence\nclosely comparable to that of real dogs (or human children), they did demonstrate a\nvariety of interesting and relevant functionalities including:\n• learning new behaviors based on imitation and reinforcement\n• responding to natural language commands and questions, with appropriate actions and\nnatural language replies\n• spontaneous exploration of their world, remembering their experiences and using them to\nbias future learning and linguistic interaction\nOne current OpenCog initiative involves extending the virtual dog work via using OpenCog\nto control virtual agents in a game world inspired by the game Minecraft. These agents are\ninitially specifically concerned with achieving goals in a game world via constructing structures\nwith blocks and carrying out simple English communications. Representative example tasks\nwould be:\n• Learning to build steps or ladders to get desired objects that are high up\n• Learning to build a shelter to protect itself from aggressors\n6.3 Current and Prior Applications of OpenCog 109\nFig. 6.1: High-Level Architecture of CogPrime. This is a conceptual depiction, not a\ndetailed flowchart (which would be too complex for a single image). Figures 6.2 , 6.4 and 6.5\nhighlight specific aspects of this diagram.\n• Learning to build structures resembling structures that it’s shown (even if the available\nmaterials are a bit different)\n• Learning how to build bridges to cross chasms\nOf course, the AI significance of learning tasks like this all depends on what kind of feedback\nthe system is given, and how complex its environment is. It would be relatively simple to make\nan AI system do things like this in a trivial and highly specialized way, but that is not the intent\nof the project the goal is to have the system learn to carry out tasks like this using general\nlearning mechanisms and a general cognitive architecture, based on embodied experience and\n110 6 A Brief Overview of CogPrime\nonly scant feedback from human teachers. If successful, this will provide an outstanding platform\nfor ongoing AGI development, as well as a visually appealing and immediately meaningful demo\nfor OpenCog.\nSpecific, particularly simple tasks that are the focus of this project team’s current work at\ntime of writing include:\n• Watch another character build steps to reach a high-up object\n• Figure out via imitation of this that, in a different context, building steps to reach a high\nup object may be a good idea\n• Also figure out that, if it wants a certain high-up object but there are no materials for\nbuilding steps available, finding some other way to get elevated will be a good idea that\nmay help it get the object\n6.3.1 Transitioning from Virtual Agents to a Physical Robot\nPreliminary experiments have also been conducted using OpenCog to control a Nao robot as well\nas a virtual dog [GdG08]. This involves hybridizing OpenCog with a separate (but interlinked)\nsubsystem handling low-level perception and action. In the experiments done so far, this has\nbeen accomplished in an extremely simplistic way. How to do this right is a topic treated in\ndetail in Chapter 26 of Part 2.\nWe suspect that reasonable level of capability will be achievable by simply interposing DeS-\nTIN (or some other system in its place) as a perception/action “black box” between OpenCog\nand a robot. Some preliminary experiments in this direction have already been carried out, connecting\nthe OpenPetBrain to a Nao robot using simpler, less capable software than DeSTIN in\nthe intermediary role (off-the-shelf speech-to-text, text-to-speech and visual object recognition\nsoftware).\nHowever, we also suspect that to achieve robustly intelligent robotics we must go beyond this\napproach, and connect robot perception and actuation software with OpenCogPrime in a “white\nbox” manner that allows intimate dynamic feedback between perceptual, motoric, cognitive\nand linguistic functions. We will achieve this via the creation and real-time utilization of links\nbetween the nodes in CogPrime’s and DeSTIN’s internal networks (a topic to be explored in\nmore depth later in this chapter).\n6.4 Memory Types and Associated Cognitive Processes in CogPrime\nNow we return to the basic description of the CogPrime approach, turning to aspects of the\nrelationship between structure and dynamics. Architecture diagrams are all very well, but,\nultimately it is dynamics that makes an architecture come alive. Intelligence is all about learning,\nwhich is by definition about change, about dynamical response to the environment and internal\nself-organizing dynamics.\nCogPrime relies on multiple memory types and, as discussed above, is founded on the premise\nthat the right course in architecting a pragmatic, roughly human-like AGI system is to handle\ndifferent types of memory differently in terms of both structure and dynamics.\n6.4 Memory Types and Associated Cognitive Processes in CogPrime 111\nCogPrime’s memory types are the declarative, procedural, sensory, and episodic memory\ntypes that are widely discussed in cognitive neuroscience [TC05], plus attentional memory for\nallocating system resources generically, and intentional memory for allocating system resources\nin a goal-directed way. Table 6.2 overviews these memory types, giving key references and indicating\nthe corresponding cognitive processes, and also indicating which of the generic patternist\ncognitive dynamics each cognitive process corresponds to (pattern creation, association, etc.).\nFigure 6.7 illustrates the relationships between several of the key memory types in the context\nof a simple situation involving an OpenCogPrime-controlled agent in a virtual world.\nIn terms of patternist cognitive theory, the multiple types of memory in CogPrime should be\nconsidered as specialized ways of storing particular types of patterns, optimized for spacetime\nefficiency. The cognitive processes associated with a certain type of memory deal with creating\nand recognizing patterns of the type for which the memory is specialized. While in principle all\nthe different sorts of pattern could be handled in a unified memory and processing architecture,\nthe sort of specialization used in CogPrime is necessary in order to achieve acceptable efficient\ngeneral intelligence using currently available computational resources. And as we have argued\nin detail in Chapter 7, efficiency is not a side-issue but rather the essence of real-world AGI\n(since as Hutter has shown, if one casts efficiency aside, arbitrary levels of general intelligence\ncan be achieved via a trivially simple program).\nThe essence of the CogPrime design lies in the way the structures and processes associated\nwith each type of memory are designed to work together in a closely coupled way, yielding cooperative\nintelligence going beyond what could be achieved by an architecture merely containing\nthe same structures and processes in separate “black boxes.”\nThe inter-cognitive-process interactions in OpenCog are designed so that\n• conversion between different types of memory is possible, though sometimes computationally\ncostly (e.g. an item of declarative knowledge may with some effort be interpreted\nprocedurally or episodically, etc.)\n• when a learning process concerned centrally with one type of memory encounters a situation\nwhere it learns very slowly, it can often resolve the issue by converting some of the relevant\nknowledge into a different type of memory: i.e. cognitive synergy\n6.4.1 Cognitive Synergy in PLN\nTo put a little meat on the bones of the \"cognitive synergy\" idea, discussed repeatedly in prior\nchapters and more extensively in latter chapters, we now elaborate a little on the role it plays\nin the interaction between procedural and declarative learning.\nWhile MOSES handles much of CogPrime’s procedural learning, and CogPrime’s internal\nsimulation engine handles most episodic knowledge, CogPrime’s primary tool for handling\ndeclarative knowledge is an uncertain inference framework called Probabilistic Logic Networks\n(PLN). The complexities of PLN are the topic of a lengthy technical monograph [GMIH08], and\nare summarized in Chapter 34; here we will eschew most details and focus mainly on pointing\nout how PLN seeks to achieve efficient inference control via integration with other cognitive\nprocesses.\nAs a logic, PLN is broadly integrative: it combines certain term logic rules with more standard\npredicate logic rules, and utilizes both fuzzy truth values and a variant of imprecise probabilities\ncalled indefinite probabilities. PLN mathematics tells how these uncertain truth values propagate\n112 6 A Brief Overview of CogPrime\nthrough its logic rules, so that uncertain premises give rise to conclusions with reasonably\naccurately estimated uncertainty values. This careful management of uncertainty is critical for\nthe application of logical inference in the robotics context, where most knowledge is abstracted\nfrom experience and is hence highly uncertain.\nPLN can be used in either forward or backward chaining mode; and in the language introduced\nabove, it can be used for either analysis or synthesis. As an example, we will consider\nbackward chaining analysis, exemplified by the problem of a robot preschool-student trying to\ndetermine whether a new playmate “Bob” is likely to be a regular visitor to is preschool or not\n(evaluating the truth value of the implication Bob → regular_visitor). The basic backward\nchaining process for PLN analysis looks like:\n1. Given an implication L ≡ A → B whose truth value must be estimated (for instance\nL ≡ Concept ∧ Procedure → Goal as discussed above), create a list (A 1 , ..., A n ) of (inference\nrule, stored knowledge) pairs that might be used to produce L\n2. Using analogical reasoning to prior inferences, assign each A i a probability of success\n• If some of the A i are estimated to have reasonable probability of success at generating\nreasonably confident estimates of L’s truth value, then invoke Step 1 with A i in place\nof L (at this point the inference process becomes recursive)\n• If none of the A i looks sufficiently likely to succeed, then inference has “gotten stuck”\nand another cognitive process should be invoked, e.g.\n– Concept creation may be used to infer new concepts related to A and B, and then\nStep 1 may be revisited, in the hope of finding a new, more promising A i involving\none of the new concepts\n– MOSES may be invoked with one of several special goals, e.g. the goal of finding\na procedure P so that P (X) predicts whether X → B. If MOSES finds such a\nprocedure P then this can be converted to declarative knowledge understandable\nby PLN and Step 1 may be revisited....\n– Simulations may be run in CogPrime’s internal simulation engine, so as to observe\nthe truth value of A → B in the simulations; and then Step 1 may be revisited....\nThe combinatorial explosion of inference control is combatted by the capability to defer to\nother cognitive processes when the inference control procedure is unable to make a sufficiently\nconfident choice of which inference steps to take next. Note that just as MOSES may rely\non PLN to model its evolving populations of procedures, PLN may rely on MOSES to create\ncomplex knowledge about the terms in its logical implications. This is just one example of the\nmultiple ways in which the different cognitive processes in CogPrime interact synergetically; a\nmore thorough treatment of these interactions is given in [Goe09a].\nIn the “new playmate” example, the interesting case is where the robot initially seems not\nto know enough about Bob to make a solid inferential judgment (so that none of the A i seem\nparticularly promising). For instance, it might carry out a number of possible inferences and not\ncome to any reasonably confident conclusion, so that the reason none of the A i seem promising\nis that all the decent-looking ones have been tried already. So it might then recourse to MOSES,\nsimulation or concept creation.\nFor instance, the PLN controller could make a list of everyone who has been a regular\nvisitor, and everyone who has not been, and pose MOSES the task of figuring out a procedure\nfor distinguishing these two categories. This procedure could then be used directly to make the\nneeded assessment, or else be translated into logical rules to be used within PLN inference. For\n6.5 Goal-Oriented Dynamics in CogPrime 113\nexample, perhaps MOSES would discover that older males wearing ties tend not to become\nregular visitors. If the new playmate is an older male wearing a tie, this is directly applicable.\nBut if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that\neven though a tuxedo is not a tie, it’s a similar form of fancy dress – so PLN may extend the\nMOSES-learned rule to the present case and infer that the new playmate is not likely to be a\nregular visitor.\n6.5 Goal-Oriented Dynamics in CogPrime\nCogPrime’s dynamics has both goal-oriented and “spontaneous” aspects; here for simplicity’s\nsake we will focus on the goal-oriented ones. The basic goal-oriented dynamic of the CogPrime\nsystem, within which the various types of memory are utilized, is driven by implications known\nas “cognitive schematics”, which take the form\nContext ∧ P rocedure → Goal < p >\n(summarized C ∧ P → G). Semi-formally, this implication may be interpreted to mean: “If the\ncontext C appears to hold currently, then if I enact the procedure P , I can expect to achieve the\ngoal G with certainty p.” Cognitive synergy means that the learning processes corresponding to\nthe different types of memory actively cooperate in figuring out what procedures will achieve\nthe system’s goals in the relevant contexts within its environment.\nCogPrime’s cognitive schematic is significantly similar to production rules in classical architectures\nlike SOAR and ACT-R (as reviewed in Chapter 4; however, there are significant\ndifferences which are important to CogPrime’s functionality. Unlike with classical production\nrules systems, uncertainty is core to CogPrime’s knowledge representation, and each CogPrime\ncognitive schematic is labeled with an uncertain truth value, which is critical to its utilization by\nCogPrime’s cognitive processes. Also, in CogPrime, cognitive schematics may be incomplete,\nmissing one or two of the terms, which may then be filled in by various cognitive processes\n(generally in an uncertain way). A stronger similarity is to MicroPsi’s triplets; the differences\nin this case are more low-level and technical and have already been mentioned in Chapter 4.\nFinally, the biggest difference between CogPrime’s cognitive schematics and production rules\nor other similar constructs, is that in CogPrime this level of knowledge representation is not\nthe only important one. CLARION [SZ04], as reviewed above, is an example of a cognitive\narchitecture that uses production rules for explicit knowledge representation and then uses a\ntotally separate subsymbolic knowledge store for implicit knowledge. In CogPrime\nboth explicit and implicit knowledge are stored in the same graph of nodes and links, with\n• explicit knowledge stored in probabilistic logic based nodes and links such as cognitive\nschematics (see Figure 6.8 for a depiction of some explicit linguistic knowledge.)\n• implicit knowledge stored in patterns of activity among these same nodes and links, defined\nvia the activity of the “importance” values (see Figure 6.9 for an illustrative example thereof)\nassociated with nodes and links and propagated by the ECAN attention allocation process\nThe meaning of a cognitive schematic in CogPrime is hence not entirely encapsulated in its\nexplicit logical form, but resides largely in the activity patterns that ECAN causes its activation\nor exploration to give rise to. And this fact is important because the synergetic interactions\nof system components are in large part modulated by ECAN activity. Without the real-time\n114 6 A Brief Overview of CogPrime\ncombination of explicit and implicit knowledge in the system’s knowledge graph, the synergetic\ninteraction of different cognitive processes would not work so smoothly, and the emergence of\neffective high-level hierarchical, heterarchical and self structures would be less likely.\n6.6 Analysis and Synthesis Processes in CogPrime\nWe now return to CogPrime’s fundamental cognitive dynamics, using examples from the “virtual\ndog” application to motivate the discussion.\nThe cognitive schematic Context ∧ Procedure → Goal leads to a conceptualization of the\ninternal action of an intelligent system as involving two key categories of learning:\n• Analysis: Estimating the probability p of a posited C ∧ P → G relationship\n• Synthesis: Filling in one or two of the variables in the cognitive schematic, given assumptions\nregarding the remaining variables, and directed by the goal of maximizing the\nprobability of the cognitive schematic\nMore specifically, where synthesis is concerned,\n• The MOSES probabilistic evolutionary program learning algorithm is applied to find P ,\ngiven fixed C and G. Internal simulation is also used, for the purpose of creating a simulation\nembodying C and seeing which P lead to the simulated achievement of G.\n– Example: A virtual dog learns a procedure P to please its owner (the goal G) in the\ncontext C where there is a ball or stick present and the owner is saying “fetch”.\n• PLN inference, acting on declarative knowledge, is used for choosing C, given fixed P and\nG (also incorporating sensory and episodic knowledge as appropriate). Simulation may also\nbe used for this purpose.\n– Example: A virtual dog wants to achieve the goal G of getting food, and it knows that\nthe procedure P of begging has been successful at this before, so it seeks a context C\nwhere begging can be expected to get it food. Probably this will be a context involving a\nfriendly person.\n• PLN-based goal refinement is used to create new subgoals G to sit on the right hand side\nof instances of the cognitive schematic.\n– Example: Given that a virtual dog has a goal of finding food, it may learn a subgoal of\nfollowing other dogs, due to observing that other dogs are often heading toward their\nfood.\n• Concept formation heuristics are used for choosing G and for fueling goal refinement, but\nespecially for choosing C (via providing new candidates for C). They are also used for\nchoosing P , via a process called “predicate schematization” that turns logical predicates\n(declarative knowledge) into procedures.\n– Example: At first a virtual dog may have a hard time predicting which other dogs are\ngoing to be mean to it. But it may eventually observe common features among a number\nof mean dogs, and thus form its own concept of “pit bull,” without anyone ever teaching\nit this concept explicitly.\n6.6 Analysis and Synthesis Processes in CogPrime 115\nWhere analysis is concerned:\n• PLN inference, acting on declarative knowledge, is used for estimating the probability of\nthe implication in the cognitive schematic, given fixed C, P and G. Episodic knowledge\nis also used in this regard, via enabling estimation of the probability via simple similarity\nmatching against past experience. Simulation is also used: multiple simulations may be run,\nand statistics may be captured therefrom.\n– Example: To estimate the degree to which asking Bob for food (the procedure P is “asking\nfor food”, the context C is “being with Bob”) will achieve the goal G of getting food, the\nvirtual dog may study its memory to see what happened on previous occasions where it\nor other dogs asked Bob for food or other things, and then integrate the evidence from\nthese occasions.\n• Procedural knowledge, mapped into declarative knowledge and then acted on by PLN inference,\ncan be useful for estimating the probability of the implication C ∧ P → G, in cases\nwhere the probability of C ∧ P 1 → G is known for some P 1 related to P .\n– Example: knowledge of the internal similarity between the procedure of asking for food\nand the procedure of asking for toys, allows the virtual dog to reason that if asking Bob\nfor toys has been successful, maybe asking Bob for food will be successful too.\n• Inference, acting on declarative or sensory knowledge, can be useful for estimating the\nprobability of the implication C ∧ P → G, in cases where the probability of C 1 ∧ P → G is\nknown for some C 1 related to C.\n– Example: if Bob and Jim have a lot of features in common, and Bob often responds\npositively when asked for food, then maybe Jim will too.\n• Inference can be used similarly for estimating the probability of the implication C ∧P → G,\nin cases where the probability of C ∧ P → G 1 is known for some G 1 related to G. Concept\ncreation can be useful indirectly in calculating these probability estimates, via providing\nnew concepts that can be used to make useful inference trails more compact and hence\neasier to construct.\n– Example: The dog may reason that because Jack likes to play, and Jack and Jill are both\nchildren, maybe Jill likes to play too. It can carry out this reasoning only if its concept\ncreation process has invented the concept of “child” via analysis of observed data.\nIn these examples we have focused on cases where two terms in the cognitive schematic are\nfixed and the third must be filled in; but just as often, the situation is that only one of the\nterms is fixed. For instance, if we fix G, sometimes the best approach will be to collectively\nlearn C and P . This requires either a procedure learning method that works interactively with a\ndeclarative-knowledge-focused concept learning or reasoning method; or a declarative learning\nmethod that works interactively with a procedure learning method. That is, it requires the sort\nof cognitive synergy built into the CogPrime design.\n116 6 A Brief Overview of CogPrime\n6.7 Conclusion\nTo thoroughly describe a comprehensive, integrative AGI architecture in a brief chapter would\nbe an impossible task; all we have attempted here is a brief overview, to be elaborated on in\nthe 800-odd pages of Part 2 of this book. We do not expect this brief summary to be enough to\nconvince the skeptical reader that the approach described here has a reasonable odds of success\nat achieving its stated goals, or even of fulfilling the conceptual notions outlined in the preceding\nchapters. However, we hope to have given the reader at least a rough idea of what sort of AGI\ndesign we are advocating, and why and in what sense we believe it can lead to advanced artificial\ngeneral intelligence. For more details on the structure, dynamics and underlying concepts of\nCogPrime, the reader is encouraged to proceed to Part 2– after completing Part 1, of course.\nPlease be patient – building a thinking machine is a big topic, and we have a lot to say about\nit!\n6.7 Conclusion 117\nFig. 6.2: Key Explicitly Implemented Processes of CogPrime . The large box at the\ncenter is the Atomspace, the system’s central store of various forms of (long-term and working)\nmemory, which contains a weighted labeled hypergraph whose nodes and links are \"Atoms\" of\nvarious sorts. The hexagonal boxes at the bottom denote various hierarchies devoted to recognition\nand generation of patterns: perception, action and linguistic. Intervening between these\nrecognition/generation hierarchies and the Atomspace, we have a pattern mining/imprinting\ncomponent (that recognizes patterns in the hierarchies and passes them to the Atomspace; and\nimprints patterns from the Atomspace on the hierarchies); and also OpenPsi, a special dynamical\nframework for choosing actions based on motivations. Above the Atomspace we have a\nhost of cognitive processes, which act on the Atomspace, some continually and some only as\ncontext dictates, carrying out various sorts of learning and reasoning (pertinent to various sorts\nof memory) that help the system fulfill its goals and motivations.\n118 6 A Brief Overview of CogPrime\nFig. 6.3: MindAgents and AtomSpace in OpenCog. This is a conceptual depiction of\none way cognitive processes may interact in OpenCog – they may be wrapped in MindAgent\nobjects, which interact via cooperatively acting on the AtomSpace.\n6.7 Conclusion 119\nFig. 6.4: Links Between Cognitive Processes and the Atomspace. The cognitive processes\ndepicted all act on the Atomspace, in the sense that they operate by observing certain\nAtoms in the Atomspace and then modifying (or in rare cases deleting) them, and potentially\nadding new Atoms as well. Atoms represent all forms of knowledge, but some forms of knowledge\nare additionally represented by external data stores connected to the Atomspace, such as\nthe Procedure Repository; these are also shown as linked to the Atomspace.\n120 6 A Brief Overview of CogPrime\nFig. 6.5: Invocation of Atom Operations By Cognitive Processes. This diagram depicts\nsome of the Atom modification, creation and deletion operations carried out by the abstract\ncognitive processes in the CogPrime architecture.\n6.7 Conclusion 121\nCogPrime\nComponent\nInt. Diag.\nSub-Diagram\nInt. Diag. Component\nProcedure Repository Long-Term Memory Procedural\nProcedure Repository Working Memory Active Procedural\nAssociative Episodic\nMemory\nLong-Term Memory\nEpisodic\nAssociative Episodic\nMemory\nWorking Memory Transient Episodic\nBackup Store Long-Term Memory\nno correlate: a function not\nnecessarily possessed by the human\nmind\nSpacetime Server Long-Term Memory Declarative and Sensorimotor\nDimensional\nEmbedding Space\nno clear correlate: a\ntool for helping\nmultiple types of LTM\nDimensional\nEmbedding Agent\nno clear correlate\nBlending\nLong-Term and\nWorking Memory\nConcept Formation\nClustering\nLong-Term and\nWorking Memory\nConcept Formation\nPLN Probabilistic\nInference\nMOSES / Hillclimbing\nWorld Simulation\nEpisodic Encoding /\nRecall\nEpisodic Encoding /\nRecall\nForgetting / Freezing\n/ Defrosting\nMap Formation\nLong-Term and\nWorking Memory\nLong-Term and\nWorking Memory\nLong-Term and\nWorking Memory\nLong-Term g Memory\nWorking Memory\nLong-Term and\nWorking Memory\nLong-Term Memory\nReasoning and Plan\nLearning/Optimization\nProcedure Learning\nSimulation\nStory-telling\nConsolidation\nno correlate: a function not\nnecessarily possessed by the human\nmind\nConcept Formation and Pattern\nMining\nAttention Allocation\nLong-Term and\nWorking Memory\nHebbian/Attentional Learning\nAttention Allocation\nHigh-Level Mind\nArchitecture\nReinforcement\nAttention Allocation Working Memory\nPerceptual Associative Memory and\nLocal Association\nAtomSpace\nHigh-Level Mind\nArchitecture\nno clear correlate: a general tool for\nrepresenting memory including\nlong-term and working, plus some of\nperception and action\nAtomSpace Working Memory\nGlobal Workspace (the high-STI\nportion of AtomSpace) & other\nWorkspaces\nDeclarative Atoms\nLong-Term and\nWorking Memory\nDeclarative and Sensorimotor\nProcedure Atoms\nLong-Term and\nWorking Memory\nProcedural\nHebbian Atoms\nLong-Term and\nWorking Memory\nAttentional\nGoal Atoms\nLong-Term and\nWorking Memory\nIntentional\nFeeling Atoms\nLong-Term and\nWorking Memory\nspanning Declarative, Intentional and\nSensorimotor\nOpenPsi\nHigh-Level Mind\nArchitecture\nMotivation / Action Selection\nOpenPsi Working Memory Action Selection\nPattern Miner\nHigh-Level Mind\nArchitecture\narrows between perception and\nworking and long-term memory\nPattern Miner Working Memory\narrows between sensory memory and\nperceptual associative and transient\nepisodic memory\narrows between action selection and\n122 6 A Brief Overview of CogPrime\nFig. 6.6: Screenshot of OpenCog-controlled virtual dog\nFig. 6.7: Relationship Between Multiple Memory Types. The bottom left corner shows\na program tree, constituting procedural knowledge. The upper left shows declarative nodes and\nlinks in the Atomspace. The upper right corner shows a relevant system goal. The lower right\ncorner contains an image symbolizing relevant episodic and sensory knowledge. All the various\ntypes of knowledge link to each other and can be approximatively converted to each other.\n6.7 Conclusion 123\nMemory Type\nDeclarative\nProcedural\nEpisodic\nAttentional\nIntentional\nSensory\nSpecific Cognitive Processes\nProbabilistic Logic Networks (PLN)\n[GMIH08]; conceptual blending\n[FT02]\nMOSES (a novel probabilistic\nevolutionary program learning\nalgorithm) [Loo06]\ninternal simulation engine [GEA08]\nEconomic Attention Networks\n(ECAN) [GPI + 10]\nprobabilistic goal hierarchy refined by\nPLN and ECAN, structured\naccording to MicroPsi [Bac09]\nIn CogBot, this will be supplied by\nthe DeSTIN component\nGeneral Cognitive\nFunctions\npattern creation\npattern creation\nassociation, pattern\ncreation\nassociation, credit\nassignment\ncredit assignment,\npattern creation\nassociation, attention\nallocation, pattern\ncreation, credit\nassignment\nTable 6.2: Memory Types and Cognitive Processes in CogPrime. The third column indicates\nthe general cognitive function that each specific cognitive process carries out, according to the\npatternist theory of cognition.\n124 6 A Brief Overview of CogPrime\nFig. 6.8: Example of Explicit Knowledge in the Atomspace. One simple example of\nexplicitly represented knowledge in the Atomspace is linguistic knowledge, such as words and\nthe concepts directly linked to them. Not all of a CogPrime system’s concepts correlate to\nwords, but some do.\n6.7 Conclusion 125\nFig. 6.9: Example of Implicit Knowledge in the Atomspace. A simple example of implicit\nknowledge in the Atomspace. The \"chicken\" and \"food\" concepts are represented by \"maps\"\nof ConceptNodes interconnected by HebbianLinks, where the latter tend to form between ConceptNodes\nthat are often simultaneously important. The bundle of links between nodes in the\nchicken map and nodes in the food map, represents an \"implicit, emergent link\" between the\ntwo concept maps. This diagram also illustrates \"glocal\" knowledge representation, in that the\nchicken and food concepts are each represented by individual nodes, but also by distributed\nmaps. The \"chicken\" ConceptNode, when important, will tend to make the rest of the map\nimportant – and vice versa. Part of the overall chicken concept possessed by the system is expressed\nby the explicit links coming out of the chicken ConceptNode, and part is represented\nonly by the distributed chicken map as a whole.\n\nSection II\nToward a General Theory of General Intelligence\n\nChapter 7\nA Formal Model of Intelligent Agents\n7.1 Introduction\nThe artificial intelligence field is full of sophisticated mathematical models and equations, but\nmost of these are highly specialized in nature – e.g. formalizations of particular logic systems,\nanalyzes of the dynamics of specific sorts of neural nets, etc. On the other hand, a number of\nhighly general models of intelligent systems also exist, including Hutter’s recent formalization\nof universal intelligence [Hut05] and a large body of work in the disciplines of systems science\nand cybernetics – but these have tended not to yield many specific lessons useful for engineering\nAGI systems, serving more as conceptual models in mathematical form.\nIt would be fantastic to have a mathematical theory bridging these extremes – a real \"general\ntheory of general intelligence,\" allowing the derivation and analysis of specific structures and\nprocesses playing a role in practical AGI systems, from broad mathematical models of general\nintelligence in various situations and under various constraints. However, the path to such a\ntheory is not entirely clear at present; and, as valuable as such a theory would be, we don’t\nbelieve such a thing to be necessary for creating advanced AGI. One possibility is that the\ndevelopment of such a theory will occur contemporaneously and synergetically with the advent\nof practical AGI technology.\nLacking a mature, pragmatically useful \"general theory of general intelligence,\" however, we\nhave still found it valuable to articulate certain theoretical ideas about the nature of general\nintelligence, with a level of rigor a bit greater than the wholly informal discussions of the previous\nchapters. The chapters in this section of the book articulate some ideas we have developed in\npursuit of a general theory of general intelligence; ideas that, even in their current relatively\nundeveloped form, have been very helpful in guiding our concrete work on the CogPrime design.\nThis chapter presents a more formal version of the notion of intelligence as “achieving complex\ngoals in complex environments,” based on a formal model of intelligent agents. These formalizations\nof agents and intelligence will be used in later chapters as a foundation for formalizing\nother concepts like inference and cognitive synergy. Chapters 8 and 9 pursue the notion of cognitive\nsynergy a little more thoroughly than was done in previous chapters. Chapter 10 sketches\na general theory of general intelligence using tools from category theory – not bringing it to the\nlevel where one can use it to derive specific AGI algorithms and structures; but still, presenting\nideas that will be helpful in interpreting and explaining specific aspects of the CogPrime design\nin Part 2. Finally, Appendix ?? explores an additional theoretical direction, in which the mind\nof an intelligent system is viewed in terms of certain curved spaces – a novel way of thinking\n129\n130 7 A Formal Model of Intelligent Agents\nabout the dynamics of general intelligence, which has been useful in guiding development of the\nECAN component of CogPrime, and we expect will have more general value in future.\nDespite the intermittent use of mathematical formalism, the ideas presented in this section\nare fairly speculative, and we do not propose them as constituting a well-demonstrated theory\nof general intelligence. Rather, we propose them as an interesting way of thinking about general\nintelligence, which appears to be consistent with available data, and which has proved inspirational\nto us in conceiving concrete structures and dynamics for AGI, as manifested for example\nin the CogPrime design. Understanding the way of thinking described in these chapters is valuable\nfor understanding why the CogPrime design is the way it is, and for relating CogPrime to\nother practical and intellectual systems, and extending and improving CogPrime.\n7.2 A Simple Formal Agents Model (SRAM)\nWe now present a formalization of the concept of “intelligent agents” – beginning with a formalization\nof “agents” in general.\nDrawing on [Hut05, LH07a], we consider a class of active agents which observe and explore\ntheir environment and also take actions in it, which may affect the environment. Formally,\nthe agent sends information to the environment by sending symbols from some finite alphabet\ncalled the action space Σ; and the environment sends signals to the agent with symbols from\nan alphabet called the perception space, denoted P. Agents can also experience rewards, which\nlie in the reward space, denoted R, which for each agent is a subset of the rational unit interval.\nThe agent and environment are understood to take turns sending signals back and forth,\nyielding a history of actions, observations and rewards, which may be denoted\nor else\na 1 o 1 r 1 a 2 o 2 r 2 ...\na 1 x 1 a 2 x 2 ...\nif x is introduced as a single symbol to denote both an observation and a reward. The\ncomplete interaction history up to and including cycle t is denoted ax 1:t ; and the history before\ncycle t is denoted ax <t = ax 1:t−1 .\nThe agent is represented as a function π which takes the current history as input, and produces\nan action as output. Agents need not be deterministic, an agent may for instance induce a\nprobability distribution over the space of possible actions, conditioned on the current history. In\nthis case we may characterize the agent by a probability distribution π(a t |ax <t ). Similarly, the\nenvironment may be characterized by a probability distribution µ(x k |ax <k a k ). Taken together,\nthe distributions π and µ define a probability measure over the space of interaction sequences.\nNext, we extend this model in a few ways, intended to make it better reflect the realities of\nintelligent computational agents. The first modification is to allow agents to maintain memories\n(of finite size), via adding memory actions drawn from a set M into the history of actions,\nobservations and rewards. The second modification is to introduce the notion of goals.\n7.2 A Simple Formal Agents Model (SRAM) 131\n7.2.1 Goals\nWe define goals as mathematical functions (to be specified below) associated with symbols\ndrawn from the alphabet G; and we consider the environment as sending goal-symbols to the\nagent along with regular observation-symbols. (Note however that the presentation of a goalsymbol\nto an agent does not necessarily entail the explicit communication to the agent of the\ncontents of the goal function. This must be provided by other, correlated observations.) We also\nintroduce a conditional distribution γ(g, µ) that gives the weight of a goal g in the context of\na particular environment µ.\nIn this extended framework, an interaction sequence looks like\nor else\na 1 o 1 g 1 r 1 a 2 o 2 g 2 r 2 ...\na 1 y 1 a 2 y 2 ...\nwhere g i are symbols corresponding to goals, and y is introduced as a single symbol to denote\nthe combination of an observation, a reward and a goal.\nEach goal function maps each finite interaction sequence I g,s,t = ay s:t with g s to g t corresponding\nto g, into a value r g (I g,s,t ) ∈ [0, 1] indicating the value or “raw reward” of achieving\nthe goal during that interaction sequence. The total reward r t obtained by the agent is the sum\nof the raw rewards obtained at time t from all goals whose symbols occur in the agent’s history\nbefore t.\nThis formalism of goal-seeking agents allows us to formalize the notion of intelligence as\n“achieving complex goals in complex environments” – a direction that is pursued in Section 7.3\nbelow.\nNote that this is an external perspective of system goals, which is natural from the perspective\nof formally defining system intelligence in terms of system behavior, but is not necessarily very\nnatural in terms of system design. From the point of view of AGI design, one is generally more\nconcerned with the (implicit or explicit) representation of goals inside an AGI system, as in\nCogPrime’s Goal Atoms to be reviewed in Chapter 22 below.\nFurther, it is important to also consider the case where an AGI system has no explicit goals,\nand the system’s environment has no immediately identifiable goals either. But in this case, we\ndon’t see any clear way to define a system’s intelligence, except via approximating the system in\nterms of other theoretical systems which do have explicit goals. This approximation approach\nis developed in Section 7.3.5 below.\nThe awkwardness of linking the general formalism of intelligence theory presented here, with\nthe practical business of creating and designing AGI systems, may indicate a shortcoming on\nthe part of contemporary intelligence theory or AGI designs. On the other hand, this sort of\nsituation often occurs in other domains as well – e.g. the leap from quantum theory to the\nanalysis of real-world systems like organic molecules involves a lot of awkwardness and large\nleaps a well.\n132 7 A Formal Model of Intelligent Agents\n7.2.2 Memory Stores\nAs well as goals, we introduce into the model a long-term memory and a workspace. Regarding\nlong-term memory we assume the agent’s memory consists of multiple memory stores corresponding\nto various types of memory, e.g.: procedural (K P roc ), declarative (K Dec ), episodic\n(K Ep ), attentional (K Att ) and Intentional (K Int ). In Appendix ?? a category-theoretic model\nof these memory stores is introduced; but for the moment, we need only assume the existence\nof\n• an injective mapping Θ Ep : K Ep → H where H is the space of fuzzy sets of subhistories\n(subhistories being “episodes” in this formalism)\n• an injective mapping Θ P roc : K P roc × M × W → A, where M is the set of memory states,\nW is the set of (observation, goal, reward) triples, and A is the set of actions (this maps\neach procedure object into a function that enacts actions in the environment or memory,\nbased on the memory state and current world-state)\n• an injective mapping Θ Dec : K Dec → L, where L is the set of expressions in some formal language\n(which may for example be a logical language), which possesses words corresponding\nto the observations, goals, reward values and actions in our agent formalism\n• an injective mapping Θ Int : K Int → G, where G is the space of goals mentioned above\n• an injective mapping Θ Att : K Int ∪ K Ep ∪ K P roc ∪ K Ec → V, where V is the space of\n“attention values” (structures that gauge the importance of paying attention to an item of\nknowledge over various time-scales or in various contexts)\nWe also assume that the vocabulary of actions contains memory-actions corresponding to the\noperations of inserting the current observation, goal, reward or action into the episodic and/or\ndeclarative memory store. And, we assume that the activity of the agent, at each time-step,\nincludes the enaction of one or more of the procedures in the procedural memory store. If several\nprocedures are enacted at once, then the end result is still formally modeled as a single action\na = a [1] ∗ ... ∗ a [k] where ∗ is an operator on action-space that composes multiple actions into a\nsingle one.\nFinally, we assume that, at each time-step, the agent may carry out an external action a i\non the environment, a memory action m i on the (long-term) memory, and an action b i on its\ninternal workspace. Among the actions that can be carried out on the workspace, are the\nability to insert or delete observations, goals, actions or reward-values from the workspace.\nThe workspace can be thought of as a sort of short-term memory or else in terms of Baars’\n“global workspace” concept mentioned above. The workspace provides a medium for interaction\nbetween the different memory types.\nThe workspace provides a mechanism by which declarative, episodic and procedural memory\nmay interact with each other. For this mechanism to work, we must assume that there are\nactions corresponding to query operations that allow procedures to look into declarative and\nepisodic memory. The nature of these query operations will vary among different agents, but\nwe can assume that in general an agent has\n• one or more procedures Q Dec (x) serving as declarative queries, meaning that when Q Dec is\nenacted on some x that is an ordered set of items in the workspace, the result is that one\nor more items from declarative memory is entered into the workspace\n• one or more procedures Q Ep (x) serving as episodic queries, meaning that when Q Ep is\nenacted on some x that is an ordered set of items in the workspace, the result is that one\nor more items from episodic memory is entered into the workspace\n7.2 A Simple Formal Agents Model (SRAM) 133\nOne additional aspect of CogPrime’s knowledge representation that is important to PLN is\nthe attachment of nonnegative weights n i corresponding to elementary observations o i . These\nweights denote the amount of evidence contained in the observation. For instance, in the context\nof a robotic agent, one could use these values to encode the assumption that an elementary visual\nobservation has more evidential value than an elementary olfactory observation.\nWe now have a model of an agent with long-term memory comprising procedural, declarative\nand episodic aspects, an internal cognitive workspace, and the capability to use procedures to\ndrive actions based on items in memory and the workspace, and to move items between longterm\nmemory and the workspace.\n7.2.2.1 Modeling CogPrime\nOf course, this formal model may be realized differently in various real-world AGI systems. In\nCogPrime we have\n• a weighted, labeled hypergraph structure called the AtomSpace used to store declarative\nknowledge (this is the representation used by PLN)\n• a collection of programs in a LISP-like language called Combo, stored in a ProcedureRepository\ndata structure, used to store procedural knowledge\n• a collection of partial “movies” of the system’s experience, played back using an internal\nsimulation engine, used to store episodic knowledge\n• AttentionValue objects, minimally containing ShortTermImportance (STI) and LongTermImportance\n(LTI) values used to store attentional knowledge\n• Goal Atoms for intentional knowledge, stored in the same format as declarative knowledge\nbut whose dynamics involve a special form of artificial currency that is used to govern action\nselection\nThe AtomSpace is the central repository and procedures and episodes are linked to Atoms\nin the AtomSpace which serve as their symbolic representatives. The “workspace” in CogPrime\nexists only virtually: each item in the AtomSpace has a “short term importance” (STI) level, and\nthe workspace consists of those items in the AtomSpace with highest STI, and those procedures\nand episodes whose symbolic representatives in the AtomSpace have highest STI.\nOn the other hand, as we saw above, the LIDA architecture uses separate representations for\nprocedural, declarative and episodic memory, but also has an explicit workspace component,\nwhere the most currently contextually relevant items from all different types of memory are\ngathered and used together in the course of actions. However, compared to CogPrime, it lacks\ncomparably fine-grained methods for integrating the different types of memory.\nSystematically mapping various existing cognitive architectures, or human brain structure,\ninto this formal agents model would be a substantial though quite plausible exercise; but we\nwill not undertake this here.\n7.2.3 The Cognitive Schematic\nNext we introduce an additional specialization into SRAM: the cognitive schematic, written\ninformally as\n134 7 A Formal Model of Intelligent Agents\nContext & P rocedure → Goal\nand considered more formally as holds(C) & ex(P ) → h i where h may be an externally specified\ngoal g i or an internally specified goal h derived as a (possibly uncertain) subgoal of one of more\ng i ; C is a piece of declarative or episodic knowledge and P is a procedure that the agent can\ninternally execute to generate a series of actions. ex(P ) is the proposition that P is successfully\nexecuted. If C is episodic then holds(C) may be interpreted as the current context (i.e. some\nfinite slice of the agent’s history) being similar to C; if C is declarative then holds(C) may be\ninterpreted as the truth value of C evaluated at the current context. Note that C may refer to\nsome part of the world quite distant from the agent’s current sensory observations; but it may\nstill be formally evaluated based on the agent’s history.\nIn the standard CogPrime notation as introduced formally in Chapter 20 (where indentation\nhas function-argument syntax similar to that in Python, and relationship types are prepended\nto their relata without parentheses), for the case C is declarative this would be written as\nPredictiveExtensionalImplication\nAND\nC\nExecution P\nG\nand in the case C is episodic one replaces C in this formula with a predicate expressing C’s\nsimilarity to the current context. The semantics of the PredictiveExtensionalInheritance relation\nwill be discussed below. The Execution relation simply denotes the proposition that procedure\nP has been executed.\nFor the class of SRAM agents who (like CogPrime) use the cognitive schematic to govern\nmany or all of their actions, a significant fragment of agent intelligence boils down to estimating\nthe truth values of PredictiveExtensionalImplication relationships. Action selection procedures\ncan be used, which choose procedures to enact based on which ones are judged most likely\nto achieve the current external goals g i in the current context. Rather than enter into the\nparticularities of action selection or other cognitive architecture issues, we will restrict ourselves\nto PLN inference, which in the context of the present agent model is a method for handling\nPredictiveImplication in the cognitive schematic.\nConsider an agent in a virtual world, such as a virtual dog, one of whose external goals is to\nplease its owner. Suppose its owner has asked it to find a cat, and it can translate this into a\nsubgoal “find cat.” If the agent operates according to the cognitive schematic, it will search for\nP so that\nPredictiveExtensionalImplication\nAND\nC\nExecution P\nEvaluation\nfound\ncat\nholds.\n7.3 Toward a Formal Characterization of Real-World General Intelligence 135\n7.3 Toward a Formal Characterization of Real-World General\nIntelligence\nHaving defined what we mean by an agent acting in an environment, we now turn to the\nquestion of what it means for such an agent to be “intelligent.”\nAs we have reviewed extensively in Chapter 2 above, “intelligence” is a commonsense, “folk\npsychology” concept, with all the imprecision and contextuality that this generally entails.\nOne cannot expect any compact, elegant formalism to capture all of its meanings. Even in\nthe psychology and AI research communities, divergent definitions abound; Legg and Hutter\n[LH07a] lists and organizes 70+ definitions from the literature.\nPractical study of natural intelligence in humans and other organisms, and practical design,\ncreation and instruction of artificial intelligences, can proceed perfectly well without an\nagreed-upon formalization of the “intelligence” concept. Some researchers may conceive their\nown formalisms to guide their own work, others may feel no need for any such thing.\nBut nevertheless, it is of interest to seek formalizations of the concept of intelligence, which\ncapture useful fragments of the commonsense notion of intelligence, and provide guidance for\npractical research in cognitive science and AI. A number of such formalizations have been given\nin recent decades, with varying degrees of mathematical rigor. Perhaps the most carefullywrought\nformalization of intelligence so far is the theory of “universal intelligence” presented by\nShane Legg and Marcus Hutter in [LH07b], which draws on ideas from algorithmic information\ntheory.\nUniversal intelligence captures a certain aspect of the “intelligence” concept very well, and\nhas the advantage of connecting closely with ideas in learning theory, decision theory and\ncomputation theory. However, the kind of general intelligence it captures best, is a kind which\nis in a sense more general in scope than human-style general intelligence. Universal intelligence\ndoes capture the sense in which humans are more intelligent than worms, which are more\nintelligent than rocks; and the sense in which theoretical AGI systems like Hutter’s AIXI or\nAIXI tl [Hut05] would be much more intelligent than humans. But it misses essential aspects\nof the intelligence concept as it is used in the context of intelligent natural systems like humans\nor real-world AI systems.\nOur main goal in this section is to present variants of universal intelligence that better\ncapture the notion of intelligence as it is typically understood in the context of real-world\nnatural and artificial systems. The first variant we describe is pragmatic general intelligence,\nwhich is inspired by the intuitive notion of intelligence as “the ability to achieve complex goals\nin complex environments,” given in [Goe93a]. After assuming a prior distribution over the\nspace of possible environments, and one over the space of possible goals, one then defines the\npragmatic general intelligence as the expected level of goal-achievement of a system relative\nto these distributions. Rather than measuring truly broad mathematical general intelligence,\npragmatic general intelligence measures intelligence in a way that’s specifically biased toward\ncertain environments and goals.\nAnother variant definition is then presented, the efficient pragmatic general intelligence,\nwhich takes into account the amount of computational resources utilized by the system in\nachieving its intelligence. Some argue that making efficient use of available resources is a defining\ncharacteristic of intelligence, see e.g. [Wan06].\nA critical question left open is the characterization of the prior distributions corresponding\nto everyday human reality; we give a semi-formal sketch of some ideas on this in Chapter 9\nbelow, where we present the notion of a “communication prior,” which assigns a probability\n136 7 A Formal Model of Intelligent Agents\nweight to a situation S based on the ease with which one agent in a society can communicate\nS to another agent in that society, using multimodal communication (including verbalization,\ndemonstration, dramatic and pictorial depiction, etc.).\nFinally, we present a formal measure of the “generality” of an intelligence, which precisiates\nthe informal distinction between “general AI” and “narrow AI.”\n7.3.1 Biased Universal Intelligence\nTo define universal intelligence, Legg and Hutter consider the class of environments that are\nreward-summable, meaning that the total amount of reward they return to any agent is bounded\nby 1. Where r i denotes the reward experienced by the agent from the environment at time i,\nthe expected total reward for the agent π from the environment µ is defined as\n∞∑\nVµ π ≡ E( r i ) ≤ 1\nTo extend their definition in the direction of greater realism, we first introduce a second-order\nprobability distribution ν, which is a probability distribution over the space of environments\nµ. The distribution ν assigns each environment a probability. One such distribution ν is the\nSolomonoff-Levin universal distribution in which one sets ν = 2 −K(µ) ; but this is not the only\ndistribution ν of interest. In fact a great deal of real-world general intelligence consists of the\nadaptation of intelligent systems to particular distributions ν over environment-space, differing\nfrom the universal distribution.\nWe then define\nDefinition 4 The biased universal intelligence of an agent π is its expected performance\nwith respect to the distribution ν over the space of all computable reward-summable environments,\nE, that is,\n1\nΥ (π) ≡ ∑ µ∈E\nν(µ)V π µ\nLegg and Hutter’s universal intelligence is obtained by setting ν equal to the universal\ndistribution.\nThis framework is more flexible than it might seem. E.g. suppose one wants to incorporate\nagents that die. Then one may create a special action, say a 666 , corresponding to the state of\ndeath, to create agents that\n• in certain circumstances output action a 666\n• have the property that if their previous action was a 666 , then all of their subsequent actions\nmust be a 666\nand to define a reward structure so that actions a 666 always bring zero reward. It then follows\nthat death is generally a bad thing if one wants to maximize intelligence. Agents that die will\nnot get rewarded after they’re dead; and agents that live only 70 years, say, will be restricted\nfrom getting rewards involving long-term patterns and will hence have specific limits on their\nintelligence.\n7.3 Toward a Formal Characterization of Real-World General Intelligence 137\n7.3.2 Connecting Legg and Hutter’s Model of Intelligent Agents to\nthe Real World\nA notable aspect of the Legg and Hutter formalism is the separation of the reward mechanism\nfrom the cognitive mechanisms of the agent. While commonplace in the reinforcement learning\nliterature, this seems psychologically unrealistic in the context of biological intelligences and\nmany types of machine intelligences. Not all human intelligent activity is specifically rewardseeking\nin nature; and even when it is, humans often pursue complexly constructed rewards,\nthat are defined in terms of their own cognitions rather than separately given. Suppose a certain\nhuman’s goals are true love, or world peace, and the proving of interesting theorems – then these\ngoals are defined by the human herself, and only she knows if she’s achieved them. An externallyprovided\nreward signal doesn’t capture the nature of this kind of goal-seeking behavior, which\ncharacterizes much human goal-seeking activity (and will presumably characterize much of the\ngoal-seeking activity of advanced engineered intelligences also) ... let alone human behavior that\nis spontaneous and unrelated to explicit goals, yet may still appear commonsensically intelligent.\nOne could seek to bypass this complaint about the reward mechanisms via a sort of “neo-\nFreudian” argument, via\n• associating the reward signal, not with the “external environment” as typically conceived,\nbut rather with a portion of the intelligent agent’s brain that is separate from the cognitive\ncomponent\n• viewing complex goals like true love, world peace and proving interesting theorems as indirect\nways of achieving the agent’s “basic goals”, created within the agent’s memory via\nsubgoaling mechanisms\nbut it seems to us that a general formalization of intelligence should not rely on such strong\nassumptions about agents’ cognitive architectures. So below, after introducing the pragmatic\nand efficient pragmatic general intelligence measures, we will propose an alternate interpretation\nwherein the mechanism of external rewards is viewed as a theoretical test framework for\nassessing agent intelligence, rather than a hypothesis about intelligent agent architecture.\nIn this alternate interpretation, formal measures like the universal, pragmatic and efficient\npragmatic general intelligence are viewed as not directly applicable to real-world intelligences,\nbecause they involve the behaviors of agents over a wide variety of goals and environments,\nwhereas in real life the opportunities to observe agents are more limited. However, they are\nviewed as being indirectly applicable to real-world agents, in the sense that an external intelligence\ncan observe an agent’s real-world behavior and then infer its likely intelligence according\nto these measures.\nIn a sense, this interpretation makes our formalized measures of intelligence the opposite of\nreal-world IQ tests. An IQ test is a quantified, formalized test which is designed to approximately\npredict the informal, qualitative achievement of humans in real life. On the other hand,\nthe formal definitions of intelligence we present here are quantified, formalized tests that are\ndesigned to capture abstract notions of intelligence, but which can be approximately evaluated\non a real-world intelligent system by observing what it does in real life.\n138 7 A Formal Model of Intelligent Agents\n7.3.3 Pragmatic General Intelligence\nThe above concept of biased universal intelligence is perfectly adequate for many purposes, but\nit is also interesting to explicitly introduce the notion of a goal into the calculation. This allows\nus to formally capture the notion presented in [Goe93a] of intelligence as “the ability to achieve\ncomplex goals in complex environments.”\nIf the agent is acting in environment µ, and is provided with g s corresponding to g at the\nstart and the end of the time-interval T = {i ∈ (s, ..., t)}, then the expected goal-achievement\nof the agent, relative to g, during the interval is the expectation\nV π µ,g,T ≡ E(\nt∑\nr g (I g,s,i ))\ni=s\nwhere the expectation is taken over all interaction sequences I g,s,i drawn according to µ. We\nthen propose\nDefinition 5 The pragmatic general intelligence of an agent π, relative to the distribution\nν over environments and the distribution γ over goals, is its expected performance with respect\nto goals drawn from γ in environments drawn from ν, over the time-scales natural to the goals;\nthat is,\n∑\nΠ(π) ≡ ν(µ)γ(g, µ)Vµ,g,T\nπ\nµ∈E,g∈G,T\n(in those cases where this sum is convergent).\nThis definition formally captures the notion that “intelligence is achieving complex goals in\ncomplex environments,” where “complexity” is gauged by the assumed measures ν and γ.\nIf ν is taken to be the universal distribution, and γ is defined to weight goals according to\nthe universal distribution, then pragmatic general intelligence reduces to universal intelligence.\nFurthermore, it is clear that a universal algorithmic agent like AIXI [Hut05] would also\nhave a high pragmatic general intelligence, under fairly broad conditions. As the interaction\nhistory grows longer, the pragmatic general intelligence of AIXI would approach the theoretical\nmaximum; as AIXI would implicitly infer the relevant distributions via experience. However,\nif significant reward discounting is involved, so that near-term rewards are weighted much\nhigher than long-term rewards, then AIXI might compare very unfavorably in pragmatic general\nintelligence, to other agents designed with prior knowledge of ν, γ and τ in mind.\nThe most interesting case to consider is where ν and γ are taken to embody some particular\nbias in a real-world space of environments and goals, and this bias is appropriately reflected\nin the internal structure of an intelligent agent. Note that an agent needs not lack universal\nintelligence in order to possess pragmatic general intelligence with respect to some non-universal\ndistribution over goals and environments. However, in general, given limited resources, there\nmay be a tradeoff between universal intelligence and pragmatic intelligence. Which leads to the\nnext point: how to encompass resource limitations into the definition.\nOne might argue that the definition of Pragmatic General Intelligence is already encompassed\nby Legg and Hutter’s definition because one may bias the distribution of environments within\nthe latter by considering different Turing machines underlying the Kolmogorov complexity.\nHowever this is not a general equivalence because the Solomonoff-Levin measure intrinsically\n7.3 Toward a Formal Characterization of Real-World General Intelligence 139\ndecays exponentially, whereas an assumptive distribution over environments might decay at\nsome other rate. This issue seems to merit further mathematical investigation.\n7.3.4 Incorporating Computational Cost\nLet η π,µ,g,T be a probability distribution describing the amount of computational resources consumed\nby an agent π while achieving goal g over time-scale T . This is a probability distribution\nbecause we want to account for the possibility of nondeterministic agents. So, η π,µ,g,T (Q) tells\nthe probability that Q units of resources are consumed. For simplicity we amalgamate space\nand time resources, energetic resources, etc. into a single number Q, which is assumed to live\nin some subset of the positive reals. Space resources of course have to do with the size of the\nsystem’s memory. Then we may define\nDefinition 6 The efficient pragmatic general intelligence of an agent π with resource\nconsumption η π,µ,g,T , relative to the distribution ν over environments and the distribution γ\nover goals, is its expected performance with respect to goals drawn from γ in environments drawn\nfrom ν, over the time-scales natural to the goals, normalized by the amount of computational\neffort expended to achieve each goal; that is,\nΠ Eff (π) ≡\n∑\nµ∈E,g∈G,Q,T\n(in those cases where this sum is convergent).\nν(µ)γ(g, µ)η π,µ,g,T (Q)\nVµ,g,T\nπ Q\nThis is a measure that rates an agent’s intelligence higher if it uses fewer computational\nresources to do its business. Roughly, it measures reward achieved per spacetime computation\nunit.\nNote that, by abandoning the universal prior, we have also abandoned the proof of convergence\nthat comes with it. In general the sums in the above definitions need not converge; and\nexploration of the conditions under which they do converge is a complex matter.\n7.3.5 Assessing the Intelligence of Real-World Agents\nThe pragmatic and efficient pragmatic general intelligence measures are more “realistic” than\nthe Legg and Hutter universal intelligence measure, in that they take into account the innate\nbiasing and computational resource restrictions that characterize real-world intelligence. But as\ndiscussed earlier, they still live in “fantasy-land” to an extent – they gauge the intelligence of an\nagent via a weighted average over a wide variety of goals and environments; and they presume\na simplistic relationship between agents and rewards that does not reflect the complexities\nof real-world cognitive architectures. It is not obvious from the foregoing how to apply these\nmeasures to real-world intelligent systems, which lack the ability to exist in such a wide variety\nof environments within their often brief lifespans, and mostly go about their lives doing things\nother than pursuing quantified external rewards. In this brief section we describe an approach\nto bridging this gap. The treatment is left semi-formal in places.\n140 7 A Formal Model of Intelligent Agents\nWe suggest to view the definitions of pragmatic and efficient pragmatic general intelligence\nin terms of a “possible worlds” semantics – i.e. to view them as asking, counterfactually, how\nan agent would perform, hypothetically, on a series of tests (the tests being goals, defined in\nrelation to environments and reward signals).\nReal-world intelligent agents don’t normally operate in terms of explicit goals and rewards;\nthese are abstractions that we use to think about intelligent agents. However, this is no objection\nto characterizing various sorts of intelligence in terms of counterfactuals like: how would system\nS operate if it were trying to achieve this or that goal, in this or that environment, in order to\nseek reward? We can characterize various sorts of intelligence in terms of how it can be inferred\nan agent would perform on certain tests, even though the agent’s real life does not consist of\ntaking these tests.\nThis conceptual approach may seem a bit artificial but we don’t currently see a better\nalternative, if one wishes to quantitatively gauge intelligence (which is, in a sense, an “artificial”\nthing to do in the first place). Given a real-world agent X and a mandate to assess its intelligence,\nthe obvious alternative to looking at possible worlds in the manner of the above definitions,\nis just looking directly at the properties of the things X has achieved in the real world during\nits lifespan. But this isn’t an easy solution, because it doesn’t disambiguate which aspects of\nX’s achievements were due to its own actions versus due to the rest of the world that X was\ninteracting with when it made its achievements. To distinguish the amount of achievement that\nX “caused” via its own actions requires a model of causality, which is a complex can of worms in\nitself; and, critically, the standard models of causality also involve counterfactuals (asking “what\nwould have been achieved in this situation if the agent X hadn’t been there”, etc.) [MW07].\nRegardless of the particulars, it seems impossible to avoid counterfactual realities in assessing\nintelligence.\nThe approach we suggest – given a real-world agent X with a history of actions in a particular\nworld, and a mandate to assess its intelligence – is to introduce an additional player, an inference\nagent δ, into the picture. The agent π modeled above is then viewed as π X : the model of X that\nδ constructs, in order to explore X’s inferred behaviors in various counterfactual environments.\nIn the test situations embodied in the definitions of pragmatic and efficient pragmatic general\nintelligence, the environment gives π X rewards, based on specifically configured goals. In X’s\nreal life, the relation between goals, rewards and actions will generally be significantly subtler\nand perhaps quite different.\nWe model the real world similarly to the “fantasy world” of the previous section, but with\nthe omission of goals and rewards. We define a naturalistic context as one in which all goals and\nrewards are constant, i.e. g i = g 0 and r i = r 0 for all i. This is just a mathematical convention\nfor stating that there are no precisely-defined external goals and rewards for the agent. In a\nnaturalistic context, we then have a situation where agents create actions based on the past\nhistory of actions and perceptions, and if there is any relevant notion of reward or goal, it\nis within the cognitive mechanism of some agent. A naturalistic agent X is then an agent π\nwhich is restricted to one particular naturalistic context, involving one particular environment\nµ (formally, we may achieve this within the framework of agents described above via dictating\nthat X issues constant “null actions” a 0 in all environments except µ).\nNext, we posit a metric space (Σ µ , d) of naturalistic agents defined on a naturalistic context\ninvolving environment µ, and a subspace ∆ ∈ Σ µ of inference agents, which are naturalistic\nagents that output predictions of other agents’ behaviors (a notion we will not fully formalize\nhere). If agents are represented as program trees, then d may be taken as edit distance on tree\nspace [Bil05]. Then, for each agent δ ∈ ∆, we may assess\n7.4 Intellectual Breadth: Quantifying the Generality of an Agent’s Intelligence 141\n• the prior probability θ(δ) according to some assumed distribution θ\n• the effectiveness p(δ, X) of δ at predicting the actions of an agent X ∈ Σ µ\nWe may then define\nDefinition 7 The inference ability of the agent δ, relative to µ and X, is\n∑\nY ∈Σ\nq µ,X (δ) = θ(δ)\nµ\nsim(X, Y )p(δ, Y )\n∑\nY ∈Σ µ\nsim(X, Y )\nwhere sim is a specified decreasing function of d(X, Y ), such as sim(X, Y ) =\n1\n1+d(X,Y ) .\nTo construct π X , we may then use the model of X created by the agent δ ∈ ∆ with the\nhighest inference ability relative to µ and X (using some specified ordering, in case of a tie).\nHaving constructed π X , we can then say that\nDefinition 8 The inferred pragmatic general intelligence (relative to ν and γ) of a naturalistic\nagent X defined relative to an environment µ, is defined as the pragmatic general intelligence\nof the model π X of X produced by the agent δ ∈ ∆ with maximal inference ability relative to µ\n(and in the case of a tie, the first of these in the ordering defined over ∆). The inferred efficient\npragmatic general intelligence of X relative to µ is defined similarly.\nThis provides a precise characterization of the pragmatic and efficient pragmatic intelligence\nof real-world systems, based on their observed behaviors. It’s a bit messy; but the real world\ntends to be like that.\n7.4 Intellectual Breadth: Quantifying the Generality of an Agent’s\nIntelligence\nWe turn now to a related question: How can one quantify the degree of generality that an\nintelligent agent possesses? Above we have discussed the qualitative distinction between AGI\nand “Narrow AI”, and intelligence as we have formalized it above is specifically intended as\na measure of general intelligence. But quantifying intelligence is different than quantifying\ngenerality versus narrowness.\nTo make the discussion simpler, we introduce the term “context” as a shorthand for “environment/interval\ntriple (µ, g, T ).” Given a context (µ, g, T ), and a set Σ of agents, one may\nconstruct a fuzzy set Ag µ,g,T gathering those agents that are intelligent relative to the context;\nand given a set of contexts, one may also define a fuzzy set Con π gathering those contexts with\nrespect to which a given agent π is intelligent. The relevant formulas are:\nχ Agµ,g,T (π) = χ Conπ (µ, g, T ) = 1 N\n∑\nQ\nη µ,g,T (Q)V π µ,g,T\nQ\nwhere N = N(µ, g, T ) is a normalization factor defined appropriately, e.g. via N(µ, g, T ) =\nmax\nπ V π µ,g,T .\nOne could make similar definitions leaving out the computational cost factor Q, but we\nsuspect that incorporating Q is a more promising direction. We then propose\n142 7 A Formal Model of Intelligent Agents\nDefinition 9 The intellectual breadth of an agent π, relative to the distribution ν over\nenvironments and the distribution γ over goals, is\nwhere H is the entropy and\nH(χ P Con π\n(µ, g, T ))\nχ P Con π\n(µ, g, T ) =\n∑\n(µ α,g β .T ω)\nν(µ)γ(g, µ)χ Conπ (µ, g, T )\nν(µ α )γ(g β , µ α )χ Conπ (µ α , g β , T ω )\nis the probability distribution formed by normalizing the fuzzy set χ Conπ (µ, g, T ).\nA similar definition of the intellectual breadth of a context (µ, g, T ), relative to the distribution\nσ over agents, may be posited. A weakness of these definitions is that they don’t try to\naccount for dependencies between agents or contexts; perhaps more refined formulations may\nbe developed that account explicitly for these dependencies.\nNote that the intellectual breadth of an agent as defined here is largely independent of\nthe (efficient or not) pragmatic general intelligence of that agent. One could have a rather\n(efficiently or not) pragmatically generally intelligent system with little breadth: this would be\na system very good at solving a fair number of hard problems, yet wholly incompetent on a\nlarger number of hard problems. On the other hand, one could also have a terribly (efficiently or\nnot) pragmatically generally stupid system with great intellectual breadth: i.e a system roughly\nequally dumb in all contexts!\nThus, one can characterize an intelligent agent as “narrow” with respect to distribution ν over\nenvironments and the distribution γ over goals, based on evaluating it as having low intellectual\nbreadth. A “narrow AI” relative to ν and γ would then be an AI agent with a relatively high\nefficient pragmatic general intelligence but a relatively low intellectual breadth.\n7.5 Conclusion\nOur main goal in this chapter has been to push the formal understanding of intelligence in a more\npragmatic direction. Much more work remains to be done, e.g. in specifying the environment,\ngoal and efficiency distributions relevant to real-world systems, but we believe that the ideas\npresented here constitute nontrivial progress.\nIf the line of research suggested in this chapter succeeds, then eventually, one will be able to\ndo AGI research as follows: Specify an AGI architecture formally, and then use the mathematics\nof general intelligence to derive interesting results about the environments, goals and hardware\nplatforms relative to which the AGI architecture will display significant pragmatic or efficient\npragmatic general intelligence, and intellectual breadth. The remaining chapters in this section\npresent further ideas regarding how to work toward this goal. For the time being, such a mode\nof AGI research remains mainly for the future, but we have still found the formalism given in\nthese chapters useful for formulating and clarifying various aspects of the CogPrime design as\nwill be presented in later chapters.\nChapter 8\nCognitive Synergy\n8.1 Cognitive Synergy\nAs we have seen, the formal theory of general intelligence, in its current form, doesn’t really\ntell us much that’s of use for creating real-world AGI systems. It tells us that creating extraordinarily\npowerful general intelligence is almost trivial if one has unrealistically huge amounts\nof computational resources; and that creating moderately powerful general intelligence using\nfeasible computational resources is all about creating AI algorithms and data structures that\n(explicitly or implicitly) match the restrictions implied by a certain class of situations, to which\nthe general intelligence is biased.\nWe’ve also described, in various previous chapters, some non-rigorous, conceptual principles\nthat seem to explain key aspects of feasible general intelligence: the complementary reliance on\nevolution and autopoiesis, the superposition of hierarchical and heterarchical structures, and so\nforth. These principles can be considered as broad strategies for achieving general intelligence\nin certain broad classes of situations. Although, a lot of research needs to be done to figure out\nnice ways to describe, for instance, in what class of situations evolution is an effective learning\nstrategy, in what class of situations dual hierarchical/heterarchical structure is an effective way\nto organize memory, etc.\nIn this chapter we’ll dig deeper into one of the “general principle of feasible general intelligences”\nbriefly alluded to earlier: the cognitive synergy principle, which is both a conceptual\nhypothesis about the structure of generally intelligent systems in certain classes of environments,\nand a design principle used to guide the architecting of CogPrime.\nWe will focus here on cognitive synergy specifically in the case of “multi-memory systems,”\nwhich we define as intelligent systems (like CogPrime) whose combination of environment,\nembodiment and motivational systems make it important for them to possess memories that\ndivide into partially but not wholly distinct components corresponding to the categories of:\n• Declarative memory\n• Procedural memory (memory about how to do certain things)\n• Sensory and episodic memory\n• Attentional memory (knowledge about what to pay attention to in what contexts\n• Intentional memory (knowledge about the system’s own goals and subgoals)\nIn Chapter 9 below we present a detailed argument as to how the requirement for a multimemory\nunderpinning for general intelligence emerges from certain underlying assumptions\n143\n144 8 Cognitive Synergy\nregarding the measurement of the simplicity of goals and environments; but the points made\nhere do not rely on that argument. What they do rely on is the assumption that, in the\nintelligence in question, the different components of memory are significantly but not wholly\ndistinct. That is, there are significant “family resemblances” between the memories of a single\ntype, yet there are also thoroughgoing connections between memories of different types.\nThe cognitive synergy principle, if correct, applies to any AI system demonstrating intelligence\nin the context of embodied, social communication. However, one may also take the theory\nas an explicit guide for constructing AGI systems; and of course, the bulk of this book describes\none AGI architecture, CogPrime, designed in such a way.\nIt is possible to cast these notions in mathematical form, and we make some efforts in this\ndirection in Appendix ??, using the languages of category theory and information geometry.\nHowever, this formalization has not yet led to any rigorous proof of the generality of cognitive\nsynergy nor any other exciting theorems; with luck this will come as the mathematics is further\ndeveloped. In this chapter the presentation is kept on the heuristic level, which is all that is\ncritically needed for motivating the CogPrime design.\n8.2 Cognitive Synergy\nThe essential idea of cognitive synergy, in the context of multi-memory systems, may be expressed\nin terms of the following points:\n1. Intelligence, relative to a certain set of environments, may be understood as the capability\nto achieve complex goals in these environments.\n2. With respect to certain classes of goals and environments (see Chapter 9 for a hypothesis\nin this regard), an intelligent system requires a “multi-memory” architecture, meaning\nthe possession of a number of specialized yet interconnected knowledge types, including:\ndeclarative, procedural, attentional, sensory, episodic and intentional (goal-related). These\nknowledge types may be viewed as different sorts of patterns that a system recognizes in\nitself and its environment. Knowledge of these various different types must be interlinked,\nand in some cases may represent differing views of the same content (see Figure ??)\n3. Such a system must possess knowledge creation (i.e. pattern recognition / formation) mechanisms\ncorresponding to each of these memory types. These mechanisms are also called\n“cognitive processes.”\n4. Each of these cognitive processes, to be effective, must have the capability to recognize when\nit lacks the information to perform effectively on its own; and in this case, to dynamically\nand interactively draw information from knowledge creation mechanisms dealing with other\ntypes of knowledge\n5. This cross-mechanism interaction must have the result of enabling the knowledge creation\nmechanisms to perform much more effectively in combination than they would if operated\nnon-interactively. This is “cognitive synergy.”\nWhile these points are implicit in the theory of mind given in [Goe06a], they are not articulated\nin this specific form there.\nInteractions as mentioned in Points 4 and 5 in the above list are the real conceptual meat\nof the cognitive synergy idea. One way to express the key idea here is that most AI algorithms\nsuffer from combinatorial explosions: the number of possible elements to be combined in a\n8.2 Cognitive Synergy 145\nFig. 8.1: Illustrative example of the interactions between multiple types of knowledge, in representing\na simple piece of knowledge. Generally speaking, one type of knowledge can be converted\nto another, at the cost of some loss of information. The synergy between cognitive processes\nassociated with corresponding pieces of knowledge, possessing different type, is a critical aspect\nof general intelligence.\nsynthesis or analysis is just too great, and the algorithms are unable to filter through all the\npossibilities, given the lack of intrinsic constraint that comes along with a “general intelligence”\ncontext (as opposed to a narrow-AI problem like chess-playing, where the context is constrained\nand hence restricts the scope of possible combinations that needs to be considered). In an AGI\narchitecture based on cognitive synergy, the different learning mechanisms must be designed\nspecifically to interact in such a way as to palliate each others’ combinatorial explosions - so\nthat, for instance, each learning mechanism dealing with a certain sort of knowledge, must\nsynergize with learning mechanisms dealing with the other sorts of knowledge, in a way that\ndecreases the severity of combinatorial explosion.\nOne prerequisite for cognitive synergy to work is that each learning mechanism must recognize\nwhen it is “stuck,” meaning it’s in a situation where it has inadequate information to\nmake a confident judgment about what steps to take next. Then, when it does recognize that\nit’s stuck, it may request help from other, complementary cognitive mechanisms.\nA theoretical notion closely related to cognitive synergy is the cognitive schematic, formalized\nin Chapter 7 above, which states that the activity of the different cognitive processes involved\nin an intelligent system may be modeled in terms of the schematic implication\nContext ∧ P rocedure → Goal\n146 8 Cognitive Synergy\nwhere the Context involves sensory, episodic and/or declarative knowledge; and attentional\nknowledge is used to regulate how much resource is given to each such schematic implication in\nmemory. Synergy among the learning processes dealing with the context, the procedure and the\ngoal is critical to the adequate execution of the cognitive schematic using feasible computational\nresources.\nFinally, drilling a little deeper into Point 3 above, one arrives at a number of possible knowledge\ncreation mechanisms (cognitive processes) corresponding to each of the key types of knowledge.\nFigure ?? below gives a high-level overview of the main types of cognitive process considered\nin the current version of Cognitive Synergy Theory, categorized according to the type\nof knowledge with which each process deals.\n8.3 Cognitive Synergy in CogPrime\nDifferent cognitive systems will use different processes to fulfill the various roles identified in\nFigure ?? above. Here we briefly preview the basic cognitive processes that the CogPrime AGI\ndesign uses for these roles, and the synergies that exist between these.\n8.3.1 Cognitive Processes in CogPrime\n: a Cognitive Synergy Based Architecture...\" from ICCI 2009\nTable 8.1: default\nTable will go here\nTable 8.2: The OpenCogPrime data structures used to represent the key knowledge types involved\nTable 8.3: default\nTable will go here\nTable 8.4: Key cognitive processes, and the algorithms that play their roles in CogPrime\nTables 8.1 and 8.3 present the key structures and processes involved in CogPrime, identifying\neach one with a certain memory/process type as considered in cognitive synergy theory. That\nis: each of these cognitive structures or processes deals with one or more types of memory –\ndeclarative, procedural, sensory, episodic or attentional. Table 8.5 describes the key CogPrime\n8.3 Cognitive Synergy in CogPrime 147\nFig. 8.2: High-level overview of the key cognitive dynamics considered here in the context of\ncognitive synergy. The cognitive synergy principle describes the behavior of a system as it\npursues a set of goals (which in most cases may be assumed to be supplied to the system\n“a priori”, but then refined by inference and other processes). The assumed intelligent agent\nmodel is roughly as follows: At each time the system chooses a set of procedures to execute,\nbased on its judgments regarding which procedures will best help it achieve its goals in the\ncurrent context. These procedures may involve external actions (e.g. involving conversation,\nor controlling an agent in a simulated world) and/or internal cognitive actions. In order to\nmake these judgments it must effectively manage declarative, procedural, episodic, sensory\nand attentional memory, each of which is associated with specific algorithms and structures\nas depicted in the diagram. There are also global processes spanning all the forms of memory,\nincluding the allocation of attention to different memory items and cognitive processes, and the\nidentification and reification of system-wide activity patterns (the latter referred to as “map\nformation”)\nTable 8.5: default\nTable will go here\nTable 8.6: Key OpenCogPrime cognitive processes categorized according to knowledge type and\nprocess type\n148 8 Cognitive Synergy\nprocesses in terms of the “analysis vs. synthesis” distinction. Finally, Tables ?? and ?? exemplify\nthese structures and processes in the context of embodied virtual agent control.\nIn the CogPrime context, a procedure in this cognitive schematic is a program tree stored\nin the system’s procedural knowledge base; and a context is a (fuzzy, probabilistic) logical\npredicate stored in the AtomSpace, that holds, to a certain extent, during each interval of time.\nA goal is a fuzzy logical predicate that has a certain value at each interval of time, as well.\nAttentional knowledge is handled in CogPrime by the ECAN artificial economics mechanism,\nthat continually updates ShortTermImportance and LongTerm Importance values associated\nwith each item in the CogPrime system’s memory, which control the amount of attention other\ncognitive mechanisms pay to the item, and how much motive the system has to keep the\nitem in memory. HebbianLinks are then created between knowledge items that often possess\nShortTermImportance at the same time; this is CogPrime’s version of traditional Hebbian\nlearning.\nECAN has deep interactions with other cognitive mechanisms as well, which are essential\nto its efficient operation; for instance, PLN inference may be used to help ECAN extrapolate\nconclusions about what is worth paying attention to, and MOSES may be used to recognize\nsubtle attentional patterns. ECAN also handles “assignment of credit”, the figuring-out of the\ncauses of an instance of successful goal-achievement, drawing on PLN and MOSES as needed\nwhen the causal inference involved here becomes difficult.\nThe synergies between CogPrime’s cognitive processes are well summarized below, which is\na 16x16 matrix summarizing a host of interprocess interactions generic to CST.\nOne key aspect of how CogPrime implements cognitive synergy is PLN’s sophisticated management\nof the confidence of judgments. This ties in with the way OpenCogPrime’s PLN inference\nframework represents truth values in terms of multiple components (as opposed to the\nsingle probability values used in many probabilistic inference systems and formalisms): each\nitem in OpenCogPrime’s declarative memory has a confidence value associated with it, which\ntells how much weight the system places on its knowledge about that memory item. This assists\nwith cognitive synergy as follows: A learning mechanism may consider itself “stuck”, generally\nspeaking, when it has no high-confidence estimates about the next step it should take.\nWithout reasonably accurate confidence assessment to guide it, inter-component interaction\ncould easily lead to increased rather than decreased combinatorial explosion. And of course\nthere is an added recursion here, in that confidence assessment is carried out partly via PLN\ninference, which in itself relies upon these same synergies for its effective operation.\nTo illustrate this point further, consider one of the synergetic aspects described in ?? below:\nthe role cognitive synergy plays in deductive inference. Deductive inference is a hard problem\nin general - but what is hard about it is not carrying out inference steps, but rather “inference\ncontrol” (i.e., choosing which inference steps to carry out). Specifically, what must happen for\ndeduction to succeed in CogPrime is:\n1. the system must recognize when its deductive inference process is “stuck”, i.e. when the\nPLN inference control mechanism carrying out deduction has no clear idea regarding which\ninference step(s) to take next, even after considering all the domain knowledge at is disposal\n2. in this case, the system must defer to another learning mechanism to gather more information\nabout the different choices available - and the other learning mechanism chosen must,\na reasonable percentage of the time, actually provide useful information that helps PLN to\nget “unstuck” and continue the deductive process\n8.4 Some Critical Synergies 149\nFor instance, deduction might defer to the “attentional knowledge” subsystem, and make\na judgment as to which of the many possible next deductive steps are most associated with\nthe goal of inference and the inference steps taken so far, according to the HebbianLinks constructed\nby the attention allocation subsystem, based on observed associations. Or, if this fails,\ndeduction might ask MOSES (running in supervised categorization mode) to learn predicates\ncharacterizing some of the terms involving the possible next inference steps. Once MOSES provides\nthese new predicates, deduction can then attempt to incorporate these into its inference\nprocess, hopefully (though not necessarily) arriving at a higher-confidence next step.\n8.4 Some Critical Synergies\nReferring back to Figure ??, and summarizing many of the ideas in the previous section, Table\n?? enumerates a number of specific ways in which the cognitive processes mentioned in the\nFigure may synergize with one another, potentially achieving dramatically greater efficiency\nthan would be possible on their own.\nOf course, realizing these synergies on the practical algorithmic level requires significant\ninventiveness and may be approached in many different ways. The specifics of how CogPrime\nmanifests these synergies are discussed in many following chapters.\nFig. 8.3: This table, and the following ones, show some of the synergies between the primary\ncognitive processes explicitly used in CogPrime.\n150 8 Cognitive Synergy\n8.5 The Cognitive Schematic 151\n8.5 The Cognitive Schematic\nNow we return to the “cognitive schematic” notion, according to which various cognitive processes\ninvolved in intelligence may be understood to work together via the implication\nContext ∧ P rocedure → Goal < p >\n(summarized C ∧ P → G). Semi-formally, this implication may be interpreted to mean: “If the\ncontext C appears to hold currently, then if I enact the procedure P , I can expect to achieve\nthe goal G with certainty p.”\nThe cognitive schematic leads to a conceptualization of the internal action of an intelligent\nsystem as involving two key categories of learning:\n• Analysis: Estimating the probability p of a posited C ∧ P → G relationship\n• Synthesis: Filling in one or two of the variables in the cognitive schematic, given assumptions\nregarding the remaining variables, and directed by the goal of maximizing the\nprobability of the cognitive schematic\nMore specifically, where synthesis is concerned, some key examples are:\n• The MOSES probabilistic evolutionary program learning algorithm is applied to find P ,\ngiven fixed C and G. Internal simulation is also used, for the purpose of creating a simulation\nembodying C and seeing which P lead to the simulated achievement of G.\n– Example: A virtual dog learns a procedure P to please its owner (the goal G) in the\ncontext C where there is a ball or stick present and the owner is saying “fetch”.\n• PLN inference, acting on declarative knowledge, is used for choosing C, given fixed P and\nG (also incorporating sensory and episodic knowledge as appropriate). Simulation may also\nbe used for this purpose.\n152 8 Cognitive Synergy\n– Example: A virtual dog wants to achieve the goal G of getting food, and it knows that\nthe procedure P of begging has been successful at this before, so it seeks a context C\nwhere begging can be expected to get it food. Probably this will be a context involving a\nfriendly person.\n• PLN-based goal refinement is used to create new subgoals G to sit on the right hand side\nof instances of the cognitive schematic.\n– Example: Given that a virtual dog has a goal of finding food, it may learn a subgoal of\nfollowing other dogs, due to observing that other dogs are often heading toward their\nfood.\n• Concept formation heuristics are used for choosing G and for fueling goal refinement, but\nespecially for choosing C (via providing new candidates for C). They are also used for\nchoosing P , via a process called “predicate schematization” that turns logical predicates\n(declarative knowledge) into procedures.\n– Example: At first a virtual dog may have a hard time predicting which other dogs are\ngoing to be mean to it. But it may eventually observe common features among a number\nof mean dogs, and thus form its own concept of “pit bull,” without anyone ever teaching\nit this concept explicitly.\nWhere analysis is concerned:\n• PLN inference, acting on declarative knowledge, is used for estimating the probability of\nthe implication in the cognitive schematic, given fixed C, P and G. Episodic knowledge\nis also used this regard, via enabling estimation of the probability via simple similarity\nmatching against past experience. Simulation is also used: multiple simulations may be\nrun, and statistics may be captured therefrom.\n– Example: To estimate the degree to which asking Bob for food (the procedure P is “asking\nfor food”, the context C is “being with Bob”) will achieve the goal G of getting food, the\nvirtual dog may study its memory to see what happened on previous occasions where it\nor other dogs asked Bob for food or other things, and then integrate the evidence from\nthese occasions.\n• Procedural knowledge, mapped into declarative knowledge and then acted on by PLN inference,\ncan be useful for estimating the probability of the implication C ∧ P → G, in cases\nwhere the probability of C ∧ P 1 → G is known for some P 1 related to P .\n– Example: knowledge of the internal similarity between the procedure of asking for food\nand the procedure of asking for toys, allows the virtual dog to reason that if asking Bob\nfor toys has been successful, maybe asking Bob for food will be successful too.\n• Inference, acting on declarative or sensory knowledge, can be useful for estimating the\nprobability of the implication C ∧ P → G, in cases where the probability of C 1 ∧ P → G is\nknown for some C 1 related to C.\n– Example: if Bob and Jim have a lot of features in common, and Bob often responds\npositively when asked for food, then maybe Jim will too.\n• Inference can be used similarly for estimating the probability of the implication C ∧P → G,\nin cases where the probability of C ∧ P → G 1 is known for some G 1 related to G. Concept\n8.6 Cognitive Synergy for Procedural and Declarative Learning 153\ncreation can be useful indirectly in calculating these probability estimates, via providing\nnew concepts that can be used to make useful inference trails more compact and hence\neasier to construct.\n– Example: The dog may reason that because Jack likes to play, and Jack and Jill are both\nchildren, maybe Jill likes to play too. It can carry out this reasoning only if its concept\ncreation process has invented the concept of “child” via analysis of observed data.\nIn these examples we have focused on cases where two terms in the cognitive schematic are\nfixed and the third must be filled in; but just as often, the situation is that only one of the\nterms is fixed. For instance, if we fix G, sometimes the best approach will be to collectively\nlearn C and P . This requires either a procedure learning method that works interactively with a\ndeclarative-knowledge-focused concept learning or reasoning method; or a declarative learning\nmethod that works interactively with a procedure learning method. That is, it requires the sort\nof cognitive synergy built into the CogPrime design.\n8.6 Cognitive Synergy for Procedural and Declarative Learning\nWe now present a little more algorithmic detail regarding the operation and synergetic interaction\nof CogPrime’s two most sophisticated components: the MOSES procedure learning\nalgorithm (see Chapter 33), and the PLN uncertain inference framework (see Chapter 34). The\ntreatment is necessarily quite compact, since we have not yet reviewed the details of either\nMOSES or PLN; but as well as illustrating the notion of cognitive synergy more concretely,\nperhaps the high-level discussion here will make clearer how MOSES and PLN fit into the big\npicture of CogPrime.\n8.6.1 Cognitive Synergy in MOSES\nMOSES, CogPrime’s primary algorithm for learning procedural knowledge, has been tested on\na variety of application problems including standard GP test problems, virtual agent control,\nbiological data analysis and text classification [Loo06]. It represents procedures internally as\nprogram trees. Each node in a MOSES program tree is supplied with a “knob,” comprising a\nset of values that may potentially be chosen to replace the data item or operator at that node.\nSo for instance a node containing the number 7 may be supplied with a knob that can take\non any integer value. A node containing a while loop may be supplied with a knob that can\ntake on various possible control flow operators including conditionals or the identity. A node\ncontaining a procedure representing a particular robot movement, may be supplied with a knob\nthat can take on values corresponding to multiple possible movements. Following a metaphor\nsuggested by Douglas Hofstadter [Hof96], MOSES learning covers both “knob twiddling” (setting\nthe values of knobs) and “knob creation.”\nMOSES is invoked within CogPrime in a number of ways, but most commonly for finding a\nprocedure P satisfying a probabilistic implication C&P → G as described above, where C is an\nobserved context and G is a system goal. In this case the probability value of the implication\nprovides the “scoring function” that MOSES uses to assess the quality of candidate procedures.\n154 8 Cognitive Synergy\nFig. 8.4: High-Level Control Flow of MOSES Algorithm\nFor example, suppose an CogPrime -controlled robot is trying to learn to play the game\nof “tag.\" (I.e. a multi-agent game in which one agent is specially labeled \"it\", and runs after\nthe other player agents, trying to touch them. Once another agent is touched, it becomes the\nnew \"it\" and the previous \"it\" becomes just another player agent.) Then its context C is that\nothers are trying to play a game they call “tag” with it; and we may assume its goals are to\nplease them and itself, and that it has figured out that in order to achieve this goal it should\nlearn some procedure to follow when interacting with others who have said they are playing\n“tag.” In this case a potential tag-playing procedure might contain nodes for physical actions\nlike step_f orward(speed s), as well as control flow nodes containing operators like if else\n(for instance, there would probably be a conditional telling the robot to do something different\ndepending on whether someone seems to be chasing it). Each of these program tree nodes would\nhave an appropriate knob assigned to it. And the scoring function would evaluate a procedure\nP in terms of how successfully the robot played tag when controlling its behaviors according to\nP (noting that it may also be using other control procedures concurrently with P ). It’s worth\nnoting here that evaluating the scoring function in this case involves some inference already,\nbecause in order to tell if it is playing tag successfully, in a real-world context, it must watch\nand understand the behavior of the other players.\nMOSES follows the high-level control flow depicted in Figure 8.4, which corresponds to the\nfollowing process for evolving a metapopulation of “demes“ of programs (each deme being a set\nof relatively similar programs, forming a sort of island in program space):\n1. Construct an initial set of knobs based on some prior (e.g., based on an empty program;\nor more interestingly, using prior knowledge supplied by PLN inference based on the\nsystem’s memory) and use it to generate an initial random sampling of programs. Add this\ndeme to the metapopulation.\n2. Select a deme from the metapopulation and update its sample, as follows:\n8.6 Cognitive Synergy for Procedural and Declarative Learning 155\na. Select some promising programs from the deme’s existing sample to use for modeling,\naccording to the scoring function.\nb. Considering the promising programs as collections of knob settings, generate new collections\nof knob settings by applying some (competent) optimization algorithm. For best\nperformance on difficult problems, it is important to use an optimization algorithm that\nmakes use of the system’s memory in its choices, consulting PLN inference to help\nestimate which collections of knob settings will work best.\nc. Convert the new collections of knob settings into their corresponding programs, reduce\nthe programs to normal form, evaluate their scores, and integrate them into the\ndeme’s sample, replacing less promising programs. In the case that scoring is expensive,\nscore evaluation may be preceded by score estimation, which may use PLN inference,\nenaction of procedures in an internal simulation environment, and/or similarity\nmatching against episodic memory.\n3. For each new program that meet the criterion for creating a new deme, if any:\na. Construct a new set of knobs (a process called “representation-building”) to define a\nregion centered around the program (the deme’s exemplar), and use it to generate a\nnew random sampling of programs, producing a new deme.\nb. Integrate the new deme into the metapopulation, possibly displacing less promising\ndemes.\n4. Repeat from step 2.\nMOSES is a complex algorithm and each part plays its role; if any one part is removed the\nperformance suffers significantly [Loo06]. However, the main point we want to highlight here is\nthe role played by synergetic interactions between MOSES and other cognitive components such\nas PLN, simulation and episodic memory, as indicated in boldface in the above pseudocode.\nMOSES is a powerful procedure learning algorithm, but used on its own it runs into scalability\nproblems like any other such algorithm; the reason we feel it has potential to play a major role\nin a human-level AI system is its capacity for productive interoperation with other cognitive\ncomponents.\nContinuing the “tag” example, the power of MOSES’s integration with other cognitive processes\nwould come into play if, before learning to play tag, the robot has already played simpler\ngames involving chasing. If the robot already has experience chasing and being chased by other\nagents, then its episodic and declarative memory will contain knowledge about how to pursue\nand avoid other agents in the context of running around an environment full of objects, and this\nknowledge will be deployable within the appropriate parts of MOSES’s Steps 1 and 2. Crossprocess\nand cross-memory-type integration make it tractable for MOSES to act as a “transfer\nlearning” algorithm, not just a task-specific machine-learning algorithm.\n8.6.2 Cognitive Synergy in PLN\nWhile MOSES handles much of CogPrime’s procedural learning, and OpenCogPrimes internal\nsimulation engine handles most episodic knowledge, CogPrime’s primary tool for handling\ndeclarative knowledge is an uncertain inference framework called Probabilistic Logic Networks\n(PLN). The complexities of PLN are the topic of a lengthy technical monograph [GMIH08], and\n156 8 Cognitive Synergy\nhere we will eschew most details and focus mainly on pointing out how PLN seeks to achieve\nefficient inference control via integration with other cognitive processes.\nAs a logic, PLN is broadly integrative: it combines certain term logic rules with more standard\npredicate logic rules, and utilizes both fuzzy truth values and a variant of imprecise probabilities\ncalled indefinite probabilities. PLN mathematics tells how these uncertain truth values propagate\nthrough its logic rules, so that uncertain premises give rise to conclusions with reasonably\naccurately estimated uncertainty values. This careful management of uncertainty is critical for\nthe application of logical inference in the robotics context, where most knowledge is abstracted\nfrom experience and is hence highly uncertain.\nPLN can be used in either forward or backward chaining mode; and in the language introduced\nabove, it can be used for either analysis or synthesis. As an example, we will consider\nbackward chaining analysis, exemplified by the problem of a robot preschool-student trying to\ndetermine whether a new playmate “Bob” is likely to be a regular visitor to its preschool or not\n(evaluating the truth value of the implication Bob → regular_visitor). The basic backward\nchaining process for PLN analysis looks like:\n1. Given an implication L ≡ A → B whose truth value must be estimated (for instance\nL ≡ C&P → G as discussed above), create a list (A 1 , ..., A n ) of (inference rule, stored\nknowledge) pairs that might be used to produce L\n2. Using analogical reasoning to prior inferences, assign each A i a probability of success\n• If some of the A i are estimated to have reasonable probability of success at generating\nreasonably confident estimates of L’s truth value, then invoke Step 1 with A i in place\nof L (at this point the inference process becomes recursive)\n• If none of the A i looks sufficiently likely to succeed, then inference has “gotten stuck”\nand another cognitive process should be invoked, e.g.\n– Concept creation may be used to infer new concepts related to A and B, and then\nStep 1 may be revisited, in the hope of finding a new, more promising A i involving\none of the new concepts\n– MOSES may be invoked with one of several special goals, e.g. the goal of finding\na procedure P so that P (X) predicts whether X → B. If MOSES finds such a\nprocedure P then this can be converted to declarative knowledge understandable\nby PLN and Step 1 may be revisited....\n– Simulations may be run in CogPrime’s internal simulation engine, so as to observe\nthe truth value of A → B in the simulations; and then Step 1 may be revisited....\nThe combinatorial explosion of inference control is combatted by the capability to defer to\nother cognitive processes when the inference control procedure is unable to make a sufficiently\nconfident choice of which inference steps to take next. Note that just as MOSES may rely\non PLN to model its evolving populations of procedures, PLN may rely on MOSES to create\ncomplex knowledge about the terms in its logical implications. This is just one example of the\nmultiple ways in which the different cognitive processes in CogPrime interact synergetically; a\nmore thorough treatment of these interactions is given in Chapter 49.\nIn the “new playmate” example, the interesting case is where the robot initially seems not\nto know enough about Bob to make a solid inferential judgment (so that none of the A i seem\nparticularly promising). For instance, it might carry out a number of possible inferences and not\ncome to any reasonably confident conclusion, so that the reason none of the A i seem promising\nis that all the decent-looking ones have been tried already. So it might then recourse to MOSES,\nsimulation or concept creation.\n8.7 Is Cognitive Synergy Tricky? 157\nFor instance, the PLN controller could make a list of everyone who has been a regular\nvisitor, and everyone who has not been, and pose MOSES the task of figuring out a procedure\nfor distinguishing these two categories. This procedure could then used directly to make the\nneeded assessment, or else be translated into logical rules to be used within PLN inference. For\nexample, perhaps MOSES would discover that older males wearing ties tend not to become\nregular visitors. If the new playmate is an older male wearing a tie, this is directly applicable.\nBut if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that\neven though a tuxedo is not a tie, it’s a similar form of fancy dress – so PLN may extend the\nMOSES-learned rule to the present case and infer that the new playmate is not likely to be a\nregular visitor.\n8.7 Is Cognitive Synergy Tricky?\n1\nIn this section we use the notion of cognitive synergy to explore a question that arises\nfrequently in the AGI community: the well-known difficulty of measuring intermediate progress\ntoward human-level AGI. We explore some potential reasons underlying this, via extending the\nnotion of cognitive synergy to a more refined notion of \"tricky cognitive synergy.\" These ideas\nare particularly relevant to the problem of creating a roadmap toward AGI, as we’ll explore in\nChapter 17 below.\n8.7.1 The Puzzle: Why Is It So Hard to Measure Partial Progress\nToward Human-Level AGI?\nIt’s not entirely straightforward to create tests to measure the final achievement of human-level\nAGI, but there are some fairly obvious candidates here. There’s the Turing Test (fooling judges\ninto believing you’re human, in a text chat), the video Turing Test, the Robot College Student\ntest (passing university, via being judged exactly the same way a human student would), etc.\nThere’s certainly no agreement on which is the most meaningful such goal to strive for, but\nthere’s broad agreement that a number of goals of this nature basically make sense.\nOn the other hand, how does one measure whether one is, say, 50 percent of the way to\nhuman-level AGI? Or, say, 75 or 25 percent?\nIt’s possible to pose many \"practical tests\" of incremental progress toward human-level AGI,\nwith the property that if a proto-AGI system passes the test using a certain sort of architecture\nand/or dynamics, then this implies a certain amount of progress toward human-level AGI based\non particular theoretical assumptions about AGI. However, in each case of such a practical test,\nit seems intuitively likely to a significant percentage of AGI researchers that there is some way\nto \"game\" the test via designing a system specifically oriented toward passing that test, and\nwhich doesn’t constitute dramatic progress toward AGI.\nSome examples of practical tests of this nature would be\n1 This section co-authored with Jared Wigmore\n158 8 Cognitive Synergy\n• The Wozniak \"coffee test\": go into an average American house and figure out how to make\ncoffee, including identifying the coffee machine, figuring out what the buttons do, finding\nthe coffee in the cabinet, etc.\n• Story understanding – reading a story, or watching it on video, and then answering questions\nabout what happened (including questions at various levels of abstraction)\n• Graduating (virtual-world or robotic) preschool\n• Passing the elementary school reading curriculum (which involves reading and answering\nquestions about some picture books as well as purely textual ones)\n• Learning to play an arbitrary video game based on experience only, or based on experience\nplus reading instructions\nOne interesting point about tests like this is that each of them seems to some AGI researchers\nto encapsulate the crux of the AGI problem, and be unsolvable by any system not far along\nthe path to human-level AGI – yet seems to other AGI researchers, with different conceptual\nperspectives, to be something probably game-able by narrow-AI methods. And of course, given\nthe current state of science, there’s no way to tell which of these practical tests really can be\nsolved via a narrow-AI approach, except by having a lot of people try really hard over a long\nperiod of time.\nA question raised by these observations is whether there is some fundamental reason why\nit’s hard to make an objective, theory-independent measure of intermediate progress toward\nadvanced AGI. Is it just that we haven’t been smart enough to figure out the right test – or is\nthere some conceptual reason why the very notion of such a test is problematic?\nWe don’t claim to know for sure – but in the rest of this section we’ll outline one possible\nreason why the latter might be the case.\n8.7.2 A Possible Answer: Cognitive Synergy is Tricky!\nWhy might a solid, objective empirical test for intermediate progress toward AGI be an infeasible\nnotion? One possible reason, we suggest, is precisely cognitive synergy, as discussed\nabove.\nThe cognitive synergy hypothesis, in its simplest form, states that human-level AGI intrinsically\ndepends on the synergetic interaction of multiple components (for instance, as in\nCogPrime, multiple memory systems each supplied with its own learning process). In this hypothesis,\nfor instance, it might be that there are 10 critical components required for a humanlevel\nAGI system. Having all 10 of them in place results in human-level AGI, but having only\n8 of them in place results in having a dramatically impaired system – and maybe having only\n6 or 7 of them in place results in a system that can hardly do anything at all.\nOf course, the reality is almost surely not as strict as the simplified example in the above\nparagraph suggests. No AGI theorist has really posited a list of 10 crisply-defined subsystems\nand claimed them necessary and sufficient for AGI. We suspect there are many different routes\nto AGI, involving integration of different sorts of subsystems. However, if the cognitive synergy\nhypothesis is correct, then human-level AGI behaves roughly like the simplistic example in the\nprior paragraph suggests. Perhaps instead of using the 10 components, you could achieve humanlevel\nAGI with 7 components, but having only 5 of these 7 would yield drastically impaired\nfunctionality – etc. Or the point could be made without any decomposition into a finite set\nof components, using continuous probability distributions. To mathematically formalize the\n8.7 Is Cognitive Synergy Tricky? 159\ncognitive synergy hypothesis becomes complex, but here we’re only aiming for a qualitative\nargument. So for illustrative purposes, we’ll stick with the \"10 components\" example, just for\ncommunicative simplicity.\nNext, let’s suppose that for any given task, there are ways to achieve this task using a system\nthat is much simpler than any subset of size 6 drawn from the set of 10 components needed\nfor human-level AGI, but works much better for the task than this subset of 6 components\n(assuming the latter are used as a set of only 6 components, without the other 4 components).\nNote that this supposition is a good bit stronger than mere cognitive synergy. For lack of\na better name, we’ll call it tricky cognitive synergy. The tricky cognitive synergy hypothesis\nwould be true if, for example, the following possibilities were true:\n• creating components to serve as parts of a synergetic AGI is harder than creating components\nintended to serve as parts of simpler AI systems without synergetic dynamics\n• components capable of serving as parts of a synergetic AGI are necessarily more complicated\nthan components intended to serve as parts of simpler AGI systems.\nThese certainly seem reasonable possibilities, since to serve as a component of a synergetic AGI\nsystem, a component must have the internal flexibility to usefully handle interactions with a lot\nof other components as well as to solve the problems that come its way. In a CogPrime context,\nthese possibilities ring true, in the sense that tailoring an AI process for tight integration with\nother AI processes within CogPrime, tends to require more work than preparing a conceptually\nsimilar AI process for use on its own or in a more task-specific narrow AI system.\nIt seems fairly obvious that, if tricky cognitive synergy really holds up as a property of\nhuman-level general intelligence, the difficulty of formulating tests for intermediate progress\ntoward human-level AGI follows as a consequence. Because, according to the tricky cognitive\nsynergy hypothesis, any test is going to be more easily solved by some simpler narrow AI process\nthan by a partially complete human-level AGI system.\n8.7.3 Conclusion\nWe haven’t proved anything here, only made some qualitative arguments. However, these arguments\ndo seem to give a plausible explanation for the empirical observation that positing tests\nfor intermediate progress toward human-level AGI is a very difficult prospect. If the theoretical\nnotions sketched here are correct, then this difficulty is not due to incompetence or lack\nof imagination on the part of the AGI community, nor due to the primitive state of the AGI\nfield, but is rather intrinsic to the subject matter. And if these notions are correct, then quite\nlikely the future rigorous science of AGI will contain formal theorems echoing and improving\nthe qualitative observations and conjectures we’ve made here.\nIf the ideas sketched here are true, then the practical consequence for AGI development\nis, very simply, that one shouldn’t worry a lot about producing intermediary results that are\ncompelling to skeptical observers. Just at 2/3 of a human brain may not be of much use,\nsimilarly, 2/3 of an AGI system may not be much use. Lack of impressive intermediary results\nmay not imply one is on a wrong development path; and comparison with narrow AI systems on\nspecific tasks may be badly misleading as a gauge of incremental progress toward human-level\nAGI.\n160 8 Cognitive Synergy\nHopefully it’s clear that the motivation behind the line of thinking presented here is a desire\nto understand the nature of general intelligence and its pursuit – not a desire to avoid testing our\nAGI software! Really, as AGI engineers, we would love to have a sensible rigorous way to test our\nintermediary progress toward AGI, so as to be able to pose convincing arguments to skeptics,\nfunding sources, potential collaborators and so forth. Our motivation here is not a desire to\navoid having the intermediate progress of our efforts measured, but rather a desire to explain\nthe frustrating (but by now rather well-established) difficulty of creating such intermediate\ngoals for human-level AGI in a meaningful way.\nIf we or someone else figures out a compelling way to measure partial progress toward AGI,\nwe will celebrate the occasion. But it seems worth seriously considering the possibility that the\ndifficulty in finding such a measure reflects fundamental properties of general intelligence.\nFrom a practical CogPrime perspective, we are interested in a variety of evaluation and\ntesting methods, including the \"virtual preschool\" approach mentioned briefly above and more\nextensively in later chapters. However, our focus will be on evaluation methods that give us\nmeaningful information about CogPrime’s progress, given our knowledge of how CogPrime\nworks and our understanding of the underlying theory. We are unlikely to focus on the achievement\nof intermediate test results capable of convincing skeptics of the reality of our partial\nprogress, because we have not yet seen any credible tests of this nature, and because we suspect\nthe reasons for this lack may be rooted in deep properties of feasible general intelligence, such\nas tricky cognitive synergy.\nChapter 9\nGeneral Intelligence in the Everyday Human\nWorld\n9.1 Introduction\nIntelligence is not just about what happens inside a system, but also about what happens outside\nthat system, and how the system interacts with its environment. Real-world general intelligence\nis about intelligence relative to some particular class of environments, and human-like general\nintelligence is about intelligence relative to the particular class of environments that humans\nevolved in (which in recent millennia has included environments humans have created using\ntheir intelligence). In Chapter 2, we reviewed some specific capabilities characterizing humanlike\ngeneral intelligence; to connect these with the general theory of general intelligence from the\nlast few chapters, we need to explain what aspects of human-relevant environments correspond\nto these human-like intelligent capabilities. We begin with aspects of the environment related\nto communication, which turn out to tie in closely with cognitive synergy. Then we turn to\nphysical aspects of the environment, which we suspect also connect closely with various human\ncognitive capabilities. Finally we turn to physical aspects of the human body and their relevance\nto the human mind. In the following chapter we present a deeper, more abstract theoretical\nframework encompassing these ideas.\nThese ideas are of theoretical importance, and they’re also of practical importance when one\nturns to the critical area of AGI environment design. If one is going to do anything besides\nrelease one’s young AGI into the “wilds” of everyday human life, then one has to put some\nthought into what kind of environment it will be raised in. This may be a virtual world or it\nmay be a robot preschool or some other kind of physical environment, but in any case some\nspecific choices must be made about what to include. Specific choices must also be made about\nwhat kind of body to give one’s AGI system – what sensors and actuators, and so forth. In\nChapter 16 we will present some specific suggestions regarding choices of embodiment and\nenvironment that we find to be ideal for AGI development – virtual and robot preschools – but\nthe material in this chapter is of more general import, beyond any such particularities. If one\nhas an intuitive idea of what properties of body and world human intelligence is biased for,\nthen one can make practical choices about embodiment and environment in a principled rather\nthan purely ad hoc or opportunistic way.\n161\n162 9 General Intelligence in the Everyday Human World\n9.2 Some Broad Properties of the Everyday World That Help\nStructure Intelligence\nThe properties of the everyday world that help structure intelligence are diverse and span\nmultiple levels of abstraction. Most of this chapter will focus on fairly concrete patterns of this\nnature, such as are involved in inter-agent communication and naive physics; however, it’s also\nworth noting the potential importance of more abstract patterns distinguishing the everyday\nworld from arbitrary mathematical environments.\nThe propensity to search for hierarchical patterns is one huge potential example of an abstract\neveryday-world property. We strongly suspect the reason that searching for hierarchical\npatterns works so well, in so many everyday-world contexts, lies in the particular structure of\nthe everyday world – it’s not something that would be true across all possible environments\n(even if one weights the space of possible environments in some clever way, say using programlength\naccording to some standard computational model). However, this sort of assertion is of\ncourse highly “philosophical,” and becomes complex to formulate and defend convincingly given\nthe current state of science and mathematics.\nGoing one step further, we recall from Chapter 3 a structure called the “dual network”, which\nconsists of superposed hierarchical and heterarchical networks: basically a hierarchy in which\nthe distance between two nodes in the hierarchy is correlated with the distance between the\nnodes in some metric space. Another high level property of the everyday world may be that dual\nnetwork structures are prevalent. This would imply that minds biased to represent the world in\nterms of dual network structure are likely to be intelligent with respect to the everyday world.\nIn a different direction, the extreme commonality of symmetry groups in the (everyday and\notherwise) physical world is another example: they occur so often that minds oriented toward\nrecognizing patterns involving symmetry groups are likely to be intelligent with respect to the\nreal world.\nWe suspect that the number of cognitively-relevant properties of the everyday world is huge\n... and that the essence of everyday-world intelligence lies in the list of varyingly abstract and\nconcrete properties, which must be embedded implicitly or explicitly in the structure of a natural\nor artificial intelligence for that system to have everyday-world intelligence.\nApart from these particular yet abstract properties of the everyday world, intelligence is just\nabout “finding patterns in which actions tend to achieve which goals in which situations” ... but,\nthe simple meta-algorithm needed to accomplish this universally is, we suggest, only a small\npercentage what it takes to make a mind.\nYou might say that a sufficiently generally intelligent system should be able to infer the\nvarious cognitively-relevant properties of the environment from looking at data about the everyday\nworld. We agree in principle, and in fact Ben Kuipers and his colleagues have done\nsome interesting work in this direction, showing that learning algorithms can infer some basics\nabout the structure of space and time from experience [MK07]. But we suggest that doing this\nreally thoroughly would require a massively greater amount of processing power than an AGI\nthat embodies and hence automatically utilizes these principles. It may be that the problem of\ninferring these properties is so hard as to require a wildly infeasible AIXI tl / Godel Machine\ntype system.\n9.3 Embodied Communication 163\n9.3 Embodied Communication\nNext we turn to the potential cognitive implications of seeking to achieve goals in an environment\nin which multimodal communication with other agents plays a prominent role.\nConsider a community of embodied agents living in a shared world, and suppose that the\nagents can communicate with each other via a set of mechanisms including:\n• Linguistic communication, in a language whose semantics is largely (not necessarily\nwholly) interpretable based on the mutually experienced world\n• Indicative communication, in which e.g. one agent points to some part of the world or\ndelimits some interval of time, and another agent is able to interpret the meaning\n• Demonstrative communication, in which an agent carries out a set of actions in the\nworld, and the other agent is able to imitate these actions, or instruct another agent as to\nhow to imitate these actions\n• Depictive communication, in which an agent creates some sort of (visual, auditory, etc.)\nconstruction to show another agent, with a goal of causing the other agent to experience\nphenomena similar to what they would experience upon experiencing some particular entity\nin the shared environment\n• Intentional communication, in which an agent explicitly communicates to another agent\nwhat its goal is in a certain situation 1\nIt is clear that ordinary everyday communication between humans possesses all these aspects.\nWe define the Embodied Communication Prior (ECP) as the probability distribution in\nwhich the probability of an entity (e.g. a goal or environment) is proportional to the difficulty of\ndescribing that entity, for a typical member of the community in question, using a particular set\nof communication mechanisms including the above five modes. We will sometimes refer to the\nprior probability of an entity under this distribution, as its “simplicity” under the distribution.\nNext, to further specialize the Embodied Communication Prior, we will assume that for\neach of these modes of communication, there are some aspects of the world that are much\nmore easily communicable using that mode than the other modes. For instance, in the human\neveryday world:\n• Abstract (declarative) statements spanning large classes of situations are generally much\neasier to communicate linguistically\n• Complex, multi-part procedures are much easier to communicate either demonstratively, or\nusing a combination of demonstration with other modes\n• Sensory or episodic data is often much easier to communicate demonstratively\n• The current value of attending to some portion of the shared environment is often much\neasier to communicate indicatively\n• Information about what goals to follow in a certain situation is often much easier to communicate\nintentionally, i.e. via explicitly indicating what one’s own goal is\nThese simple observations have significant implications for the nature of the Embodied Communication\nPrior. For one thing they let us define multiple forms of knowledge:\n• Isolatedly declarative knowledge is that which is much more easily communicable linguistically\n1 in Appendix ?? we recount some interesting recent results showing that mirror neurons fire in response to\nsome cases of intentional communication as thus defined\n164 9 General Intelligence in the Everyday Human World\n• Isolatedly procedural knowledge is that which is much more easily communicable\ndemonstratively\n• Isolatedly sensory knowledge is that which is much more easily communicable depictively\n• Isolatedly attentive knowledge is that which is much more easily communicable indicatively\n• Isolatedly intentional knowledge is that which is much more easily communicable intentionally\nThis categorization of knowledge types resembles many ideas from the cognitive theory of\nmemory [TC05], although the distinctions drawn here are a little crisper than any classification\ncurrently derivable from available neurological or psychological data.\nOf course there may be much knowledge, of relevance to systems seeking intelligence according\nto the ECP, that does not fall into any of these categories and constitutes “mixed knowledge.”\nThere are some very important specific subclasses of mixed knowledge. For instance, episodic\nknowledge (knowledge about specific real or hypothetical sets of events) will most easily be\ncommunicated via a combination of declarative, sensory and (in some cases) procedural communication.\nScientific and mathematical knowledge are generally mixed knowledge, as is most\neveryday commonsense knowledge.\nSome cases of mixed knowledge are reasonably well decomposable, in the sense that they\ndecompose into knowledge items that individually fall into some specific knowledge type. For\ninstance, an experimental chemistry procedure may be much more easily communicable procedurally,\nwhereas an allied piece of knowledge from theoretical chemistry may be much more\neasily communicable declaratively; but in order to fully communicate either the experimental\nprocedure or the abstract piece of knowledge, one may ultimately need to communicate both\naspects.\nAlso, even when the best way to communicate something is mixed-mode, it may be possible\nto identify one mode that poses the most important part of the communication. An example\nwould be a chemistry experiment that is best communicated via a practical demonstration\ntogether with a running narrative. It may be that the demonstration without the narrative\nwould be vastly more valuable than the narrative without the demonstration. To cover such\ncases we may make less restrictive definitions such as\n• Interactively declarative knowledge is that which is much more easily communicable\nin a manner dominated by linguistic communication\nand so forth. We call these “interactive knowledge categories,” by contrast to the “isolated\nknowledge categories” introduced earlier.\n9.3.0.1 Naturalness of Knowledge Categories\nNext we introduce an assumption we call NKC, for Naturalness of Knowledge Categories.\nThe NKC assumption states that the knowledge in each of the above isolated and interactive\ncommunication-modality-focused categories forms a “natural category,” in the sense that\nfor each of these categories, there are many different properties shared by a large percentage of\nthe knowledge in the category, but not by a large percentage of the knowledge in the other categories.\nThis means that, for instance, procedural knowledge systematically (and statistically)\nhas different characteristics than the other kinds of knowledge.\n9.3 Embodied Communication 165\nThe NKC assumption seems commonsensically to hold true for human everyday knowledge,\nand it has fairly dramatic implications for general intelligence. Suppose we conceive general\nintelligence as the ability to achieve goals in the environment shared by the communicating\nagents underlying the Embodied Communication Prior. Then, NKC suggests that the best way\nto achieve general intelligence according to the Embodied Communication Prior is going to\ninvolve\n• specialized methods for handling declarative, procedural, sensory and attentional knowledge\n(due to the naturalness of the isolated knowledge categories)\n• specialized methods for handling interactions between different types of knowledge, including\nmethods focused on the case where one type of knowledge is primary and the others are\nsupporting (the latter due to the naturalness of the interactive knowledge categories)\n9.3.0.2 Cognitive Completeness\nSuppose we conceive an AI system as consisting of a set of learning capabilities, each one\ncharacterized by three features:\n• One or more knowledge types that it is competent to deal with, in the sense of the two\nkey learning problems mentioned above\n• At least one learning type: either analysis, or synthesis, or both\n• At least one interaction type, for each (knowledge type, learning type) pair it handles:\n“isolated” (meaning it deals mainly with that knowledge type in isolation), or “interactive”\n(meaning it focuses on that knowledge type but in a way that explicitly incorporates other\nknowledge types into its process), or “fully mixed” (meaning that when it deals with the\nknowledge type in question, no particular knowledge type tends to dominate the learning\nprocess).\nThen, intuitively, it seems to follow from the ECP with NKC that systems with high efficient\ngeneral intelligence should have the following properties, which collectively we’ll call cognitive\ncompleteness:\n• For each (knowledge type, learning type, interaction type) triple, there should be a learning\ncapability corresponding to that triple.\n• Furthermore the capabilities corresponding to different (knowledge type, interaction type)\npairs should have distinct characteristics (since according to the NKC the isolated knowledge\ncorresponding to a knowledge type is a natural category, as is the dominant knowledge\ncorresponding to a knowledge type)\n• For each (knowledge type, learning type) pair (K,L), and each other knowledge type K1\ndistinct from K, there should be a distinctive capability with interaction type “interactive”\nand dealing with knowledge that is interactively K but also includes aspects of K1\nFurthermore, it seems intuitively sensible that according to the ECP with NKC, if the capabilities\nmentioned in the above points are reasonably able, then the system possessing the\ncapabilities will display general intelligence relative to the ECP. Thus we arrive at the hypothesis\nthat\n166 9 General Intelligence in the Everyday Human World\nUnder the assumption of the Embodied Communication Prior (with the Natural\nKnowledge Categories assumption), the property above called “cognitive completeness”\nis necessary and sufficient for efficient general intelligence at the level of an\ninteligent adult human (e.g. at the Piagetan formal level [Pia53]).\nOf course, the above considerations are very far from a rigorous mathematical proof (or\neven precise formulation) of this hypothesis. But we are presenting this here as a conceptual\nhypothesis, in order to qualitatively guide our practical AGI R&D and also to motivate further,\nmore rigorous theoretical work.\n9.3.1 Generalizing the Embodied Communication Prior\nOne interesting direction for further research would be to broaden the scope of the inquiry, in\na manner suggested above: instead of just looking at the ECP, look at simplicity measures in\ngeneral, and attack the question of how a mind must be structured in order to display efficient\ngeneral intelligence relative to a specified simplicity measure. This problem seems unapproachable\nin general, but some special cases may be more tractable.\nFor instance, suppose one has\n• a simplicity measure that (like the ECP) is approximately decomposable into a set of fairly\ndistinct components, plus their interactions\n• an assumption similar to NKC, which states that the entities displaying simplicity according\nto each of the distinct components, are roughly clustered together in entity-space\nThen one should be able to say that, to achieve efficient general intelligence relative to\nthis decomposable simplicity measure, a system should have distinct capabilities corresponding\nto each of the components of the simplicity measure interactions between these capabilities,\ncorresponding to the interaction terms in the simplicity measure.\nWith copious additional work, these simple observations could potentially serve as the seed for\na novel sort of theory of general intelligence – a theory of how the structure of a system depends\non the structure of the simplicity measure with which it achieves efficient general intelligence.\nCognitive Synergy Theory would then emerge as a special case of this more abstract theory.\n9.4 Naive Physics\nMultimodal communication is an important aspect of the environment for which human intelligence\nevolved – but not the only one. It seems likely that our human intelligence is also\nclosely adapted to various aspects of our physical environment – a matter that is worth carefully\nattending as we design environments for our robotically or virtually embodied AGI systems to\noperate in.\nOne interesting guide to the most cognitively relevant aspects of human environments is the\nsubfield of AI known as “naive physics” [Hay85] – a term that refers to the theories about the\nphysical world that human beings implicitly develop and utilize during their lives. For instance,\n9.4 Naive Physics 167\nwhen you figure out that you need to pressure the knife slightly harder when spreading peanut\nbutter rather than jelly, you’re not making this judgment using Newtonian physics or the\nNavier-Stokes equations of fluid dynamics; you’re using heuristic patterns that you figured out\nthrough experience. Maybe you figured out these patterns through experience spreading peanut\nbutter and jelly in particular. Or maybe you figured these heuristic patterns out before you ever\ntried to spread peanut butter or jelly specifically, via just touching peanut butter and jelly to\nsee what they feel like, and then carrying out inference based on your experience manipulating\nsimilar tools in the context of similar substances.\nOther examples of similar “naive physics” patterns are easy to come by, e.g.\n1. What goes up must come down.\n2. A dropped object falls straight down.\n3. A vacuum sucks things towards it.\n4. Centrifugal force throws rotating things outwards.\n5. An object is either at rest or moving, in an absolute sense.\n6. Two events are simultaneous or they are not.\n7. When running downhill, one must lift one’s knees up high.\n8. When looking at something that you just barely can’t discern accurately, squint.\nAttempts to axiomatically formulate naive physics have historically come up short, and we\ndoubt this is a promising direction for AGI. However, we do think the naive physics literature\ndoes a good job of identifying the various phenomena that the human mind’s naive physics deals\nwith. So, from the point of view of AGI environment design, naive physics is a useful source\nof requirements. Ideally, we would like an AGI’s environment to support all the fundamental\nphenomena that naive physics deals with.\nWe now describe some key aspects of naive physics in a more systematic manner. Naive\nphysics has many different formulations; in this section we draw heavily on [SC94], who divide\nnaive physics phenomena into 5 categories. Here we review these categories and identify a\nnumber of important things that humanlike intelligent agents must be able to do relative to\neach of them.\n9.4.1 Objects, Natural Units and Natural Kinds\nOne key aspect of naive physics involves recognition of various aspects of objects, such as:\n1. Recognition of objects amidst noisy perceptual data\n2. Recognition of surfaces and interiors of objects\n3. Recognition of objects as manipulable units\n4. Recognition of objects as potential subjects of fragmentation (splitting, cutting) and of\nunification (gluing, bonding)\n5. Recognition of the agent’s body as an object, and as parts of the agent’s body as objects\n6. Division of universe of perceived objects into “natural kinds”, each containing typical and\natypical instances\n168 9 General Intelligence in the Everyday Human World\n9.4.2 Events, Processes and Causality\nSpecific aspects of naive physics related to temporality and causality are:\n1. Distinguishing roughly-subjectively-instantaneous events from extended processes\n2. Identifying beginnings, endings and crossings of processes\n3. Identifying and distinguishing internal and external changes\n4. Identifying and distinguishing internal and external changes relative to one’s own body\n5. Interrelating body-changes with changes in external entities\nNotably, these aspects of naive physics involve a different processes occurring on a variety of\ndifferent time scales, intersecting in complex patterns, and involving processes inside the agent’s\nbody, outside the agent’s body, and crossing the boundary of the agent’s body.\n9.4.3 Stuffs, States of Matter, Qualities\nRegarding the various states of matter, some important aspects of naive physics are:\n1. Perceiving gaps between objects: holes, media, illusions like rainbows, mirages and holograms\n2. Distinguishing the manners in which different sorts of entities (e.g. smells, sounds, light) fill\nspace\n3. Distinguishing properties such as smoothness, roughness, graininess, stickiness, runniness,\netc.\n4. Distinguishing degrees of elasticity and fragility\n5. Assessing separability of aggregates\n9.4.4 Surfaces, Limits, Boundaries, Media\nGibson [Gib77, Gib79] has argued that naive physics is not mainly about objects but rather\nmainly about surfaces. Surfaces have a variety of aspects and relationships that are important\nfor naive physics, such as:\n1. Perceiving and reasoning about surfaces as two-sided or one-sided interfaces\n2. Inference of the various ecological laws of surfaces\n3. Perception of various media in the world as separated by surfaces\n4. Recognition of the textures of surfaces\n5. Recognition of medium/surface layout relationships such as: ground, open environment,\nenclosure, detached object, attached object, hollow object, place, sheet, fissure, stick, fibre,\ndihedral, etc.\nAs a concrete, evocative “toy” example of naive everyday knowledge about surfaces and\nboundaries, consider Sloman’s [Slo08a] example scenario, depicted in Figure 9.1 and drawn\nlargely from [SS74] (see also related discussion in [Slo08b], in which “A child can be given one\n9.4 Naive Physics 169\nFig. 9.1: One of Sloman’s example test domains for real-world inference. Left: a number of pins\nand a rubber band to be stretched around them. Right: use of the pins and rubber band to\nmake a letter T.\nor more rubber bands and a pile of pins, and asked to use the pins to hold the band in place to\nform a particular shape)... For example, things to be learnt could include”:\n1. There is an area inside the band and an area outside the band.\n2. The possible effects of moving a pin that is inside the band towards or further away\nfrom other pins inside the band. (The effects can depend on whether the band is already\nstretched.)\n3. The possible effects of moving a pin that is outside the band towards or further away from\nother pins inside the band.\n4. The possible effects of adding a new pin, inside or outside the band, with or without pushing\nthe band sideways with the pin first.\n5. The possible effects of removing a pin, from a position inside or outside the band.\n6. Patterns of motion/change that can occur and how they affect local and global shape\n(e.g. introducing a concavity or convexity, introducing or removing symmetry, increasing or\ndecreasing the area enclosed).\n7. The possibility of causing the band to cross over itself. (NB: Is an odd number of crosses\npossible?)\n8. How adding a second, or third band can enrich the space of structures, processes and effects\nof processes.\n9.4.5 What Kind of Physics Is Needed to Foster Human-like\nIntelligence?\nWe stated above that we would like an AGI’s environment to support all the fundamental phenomena\nthat naive physics deals with; and we have now reviewed a number of these specific\nphenomena. But it’s not entirely clear what the “fundamental” aspects underlying these phenomena\nare. One important question in the environment-design context is how close an AGI\nenvironment needs to stick to the particulars of real-world naive physics. Is it important that a\nyoung AGI can play with the specific differences between spreading peanut butter versus jelly?\nOr is it enough that it can play with spreading and smearing various substances of different\nconsistencies? How close does the analogy between an AGI environment’s naive physics and\n170 9 General Intelligence in the Everyday Human World\nreal-world naive physics need to be? This is a question to which we have no scientific answer at\npresent. Our own working hypothesis is that the analogy does not need to be extremely close,\nand with this in mind in Chapter 16 we propose a virtual environment BlocksNBeadsWorld\nthat encompasses all the basic conceptual phenomena of real-world naive physics, but does not\nattempt to emulate their details.\nFramed in terms of human psychology rather than environment design, the question becomes:\nAt what level of detail must one model the physical world to understand the ways in\nwhich human intelligence has adapted to the physical world?. Our suspicion, which underlies\nour BlocksNBeadsWorld design, is that it’s approximately enough to have\n• Newtonian physics, or some close approximation\n• Matter in multiple phases and forms vaguely similar to the ones we see in the real world:\nsolid, liquid, gas, paste, goo, etc.\n• Ability to transform some instances of matter from one form to another\n• Ability to flexibly manipulate matter in various forms with various solid tools\n• Ability to combine instances of matter into new ones in a fairly rich way: e.g. glue or tie\nsolids togethermix liquids together, etc.\n• Ability to position instances of matter with respect to each other in a rich way: e.g. put\nliquid in a solid cavity, cover something with a lid or a piece of fabric, etc.\nIt seems to us that if the above are present in an environment, then an AGI seeking to\nachieve appropriate goals in that environment will be likely to form an appropriate “humanlike\nphysical-world intuition.\" We doubt that the specifics of the naive physics of different\nforms of matter are critical to human-like intelligence. But, we suspect that a great amount\nof unconscious human metaphorical thinking is conditioned on the fact that humans evolved\naround matter that takes a variety of forms, can be changed from one form to another, and can\nbe fairly easily arranged and composited to form new instances from prior ones. Without many\ndiverse instances of matter transformation, arrangement and composition in its experience, an\nAGI is unlikely to form an internal “metaphor-base” even vaguely similar to the human one –\nso that, even if it’s highly intelligent, its thinking will be radically non-human-like in character.\nNaturally this is all somewhat speculative and must be explored via experimentation. Maybe\nan elaborate blocks-world with only solid objects will be sufficient to create human-level, roughly\nhuman-like AGI with rich spatiotemporal and manipulative intuition. Or maybe human intelligence\nis more closely adapted to the specifics of our physical world – with water and dirt and\nplants and hair and so forth – than we currently realize. One thing that is very clear is that, as\nwe proceed with embodying, situating and educating our AGI systems, we need to pay careful\nattention to the way their intelligence is conditioned by their environment.\n9.5 Folk Psychology\nRelated to naive physics is the notion of “naive psychology” or “folk psychology” [Rav04], which\nincludes for instance the following aspects:\n1. Mental simulation of other agents\n2. Mental theory regarding other agents\n3. Attribution of beliefs, desires and intentions (BDI) to other agents via theory or simulation\n9.6 Body and Mind 171\n4. Recognition of emotions in other agents via their physical embodiment\n5. Recognition of desires and intentions in other agents via their physical embodiment\n6. Analogical and contextual inferences between self and other, regarding BDI and other aspects\n7. Attribute causes and meanings to other agents behaviors\n8. Anthropomorphize non-human, including inanimate objects\nThe main special requirement placed on an AGI’s embodiment by the above aspects pertains\nto the ability of agents to express their emotions and intentions to each other. Humans do this\nvia facial expressions, gestures and language.\n9.5.1 Motivation, Requiredness, Value\nRelatedly to folk psychology, Gestalt [Koh38] and ecological [Gib77, Gib79] psychology suggest\nthat humans perceive the world substantially in terms of the affordances it provides them for\ngoal-directed action. This suggests that, to support human-like intelligence, an AGI must be\ncapable of:\n1. Perception of entities in the world as differentially associated with goal-relevant value\n2. Perception of entities in the world in terms of the potential actions they afford the agent,\nor other agents\nThe key point is that entities in the world need to provide a wide variety of ways for agents\nto interact with them, enabling richly complex perception of affordances.\n9.6 Body and Mind\nThe above discussion has focused on the world external to the body of the AGI agent embodied\nand embedded in the world, but the issue of the AGI’s body also merits consideration. There\nseems little doubt that a human’s intelligence is highly conditioned by the particularities of the\nhuman body.\n9.6.1 The Human Sensorium\nHere the requirements seem fairly simple: while surely not strictly necessary, it would certainly\nbe preferable to provide an AGI with fairly rich analogues of the human senses of touch, sight,\nsound, kinesthesia, taste and smell. Each of these senses provides different sorts of cognitive\nstimulation to the human mind; and while similar cognitive stimulation could doubtless be\nachieved without analogous senses, the provision of such seems the most straightforward approach.\nIt’s hard to know how much of human intelligence is specifically biased to the sorts of\noutputs provided by human senses.\nAs vision already is accorded such a prominent role in the AI and cognitive science literature\n– and is discussed in moderate depth in Chapter 26 of Part 2, we won’t take time elaborating\n172 9 General Intelligence in the Everyday Human World\non the importance of vision processing for humanlike cognition. The key thing an AGI requires\nto support humanlike “visual intelligence” is an environment containing a sufficiently robust\ncollection of materials that object and event recognition and identification become interesting\nproblems.\nAudition is cognitively valuable for many reasons, one of which is that it gives a very rich\nand precise method of sensing the world that is different from vision. The fact that humans can\ndisplay normal intelligence while totally blind or totally deaf is an indication that, in a sense,\nvision and audition are redundant for understanding the everyday world. However, it may be\nimportant that the brain has evolved to account for both of these senses, because this forced it\nto account for the presence of two very rich and precise methods of sensing the world – which\nmay have forced it to develop more abstract representation mechanisms than would have been\nnecessary with only one such method.\nTouch is a sense that is, in our view, generally badly underappreciated within the AI community.\nIn particular the cognitive robotics community seems to worry too little about the terribly\nimpoverished sense of touch possessed by most current robots (though fortunately there are\nrecent technologies that may help improve robots in this regard; see e.g. [Nan08]). Touch is how\nthe human infant learns to distinguish self from other, and in this way it is the most essential\nsense for the establishment of an internal self-model. Touching others’ bodies is a key method\nfor developing a sense of the emotional reality and responsiveness of others, and is hence key to\nthe development of theory of mind and social understanding in humans. For this reason, among\nothers, human children lacking sufficient tactile stimulation will generally wind up badly impaired\nin multiple ways. A good-quality embodiment should supply an AI agent with a body\nthat possesses skin, which has varying levels of sensitivity on different parts of the skin (so that\nit can effectively distinguish between reality and its perception thereof in a tactile context);\nand also varying types of touch sensors (e.g. temperature versus friction), so that it experiences\ntextures as multidimensional entities.\nRelated to touch, kinesthesia refers to direct sensation of phenomena happening inside the\nbody. Rarely mentioned in AI, this sense seems quite critical to cognition, as it underpins many\nof the analogies between self and other that guide cognition. Again, it’s not important that an\nAGI’s virtual body have the same internal body parts as a human body. But it seems valuable\nto have the AGI’s virtual body display some vaguely human-body-like properties, such as feeling\ninternal strain of various sorts after getting exercise, feeling discomfort in certain places when\nrunning out of energy, feeling internally different when satisfied versus unsatisfied, etc.\nNext, taste is a cognitively interesting sense in that it involves the interplay between the\ninternal and external world; it involves the evaluation of which entities from the external world\nare worthy of placing inside the body. And smell is cognitively interesting in large part because\nof its relationship with taste. A smell is, among other things, a long-distance indicator of what\na certain entity might taste like. So, the combination of taste and smell provides means for\nconceptualizing relationships between self, world and distance.\n9.6.2 The Human Body’s Multiple Intelligences\nWhile most unique aspect of human intelligence is rooted in what one might call the \"cognitive\ncortex\" – the portions of the brain dealing with self-reflection and abstract thought. But the\ncognitive cortex does its work in close coordination with the body’s various more specialized\n9.6 Body and Mind 173\nintelligent subsystems, including those associated with the gut, the heart, the liver, the immune\nand endocrine systems, and the perceptual and motor cortices.\nIn the perspective underlying this book, the human cognitive cortex – or the core cognitive\nnetwork of any roughly human-like AGI system – should be viewed as a highly flexible, selforganizing\nnetwork. These cognitive networks are modelable e.g. as a recurrent neural net with\ngeneral topology, or a weighted labeled hypergraph, and are centrally concerned with recognizing\npatterns in its environment and itself, especially patterns regarding the achievement of the\nsystem’s goals in various appropriate contexts. Here we augment this perspective, noting that\nthe human brain’s cognitive network is closely coupled with a variety of simpler and more\nspecialized intelligent \"body-system networks\" which provide it with structural and dynamical\ninductive biasing. We then discuss the implications of this observation for practical AGI design.\nOne recalls Pascal’s famous quote \"The heart has its reasons, of which reason knows not.\"\nAs we now know, the intuitive sense that Pascal and so many others have expressed, that the\nheart and other body systems have their own reasons, is grounded in the fact that they actually\ndo carry out simple forms of reasoning (i.e. intelligent, adaptive dynamics), in close, sometimes\ncognitively valuable, coordination with the central cognitive network.\n9.6.2.1 Some of the Human Body’s Specialized Intelligent Subsystems\nThe human body contains multiple specialized intelligences apart from the cognitive cortex.\nHere we review some of the most critical.\nHierarchies of Visual and Auditory Perception\n. The hierarchical structure of visual and auditory cortex has been taken by some researchers\n[Kur12], [HB06] as the generic structure of cognition. While we suspect this is overstated, we\nagree it is important that these cortices nudge large portions of the cognitive cortex to assume\nan approximately hierarchical structure.\nOlfactory Attractors\n. The process of recognizing a familiar smell is grounded in a neural process similar to convergence\nto an attractor in a nonlinear dynamical system [Fre95]. There is evidence that the\nmammalian cognitive cortex evolved in close coordination with the olfactory cortex [Row11],\nand much of abstract cognition reflects a similar dynamic of gradually coming to a conclusion\nbased on what initially \"smells right.\"\nPhysical and Cognitive Action\n. The cerebellum, a specially structured brain subsystem which controls motor movements,\nhas for some time been understood to also have involvement in attention, executive control,\nlanguage, working memory, learning, pain, emotion, and addiction [PSF09].\n174 9 General Intelligence in the Everyday Human World\nThe Second Brain\n. The gastrointestinal neural net contains millions of neurons and is capable of operating independently\nof the brain. It modulates stress response and other aspects of emotion and motivation\nbased on experience – resulting in so-called \"gut feelings\" [Ger99].\nThe Heart’s Neural Network\n. The heart has its own neural network, which modulates stress response, energy level and\nrelaxation/excitement (factors key to motivation and emotion) based on experience [Arm04].\nPattern Recognition and Memory in the Liver\n. The liver is a complex pattern recognition system, adapting via experience to better identify\ntoxins [CB06]. Like the heart, it seems to store some episodic memories as well, resulting in liver\ntransplant recipients sometimes acquiring the tastes in music or sports of the donor [EMC12].\nImmune Intelligence\n. The immune network is a highly complex, adaptive self-organizing system, which ongoingly\nsolves the learning problem of identifying antigens and distinguishing them from the body\nsystem [FP86]. As immune function is highly energetically costly, stress response involves subtle\nmodulation of the energy allocation to immune function, which involves communication between\nneural and immune networks.\nThe Endocrine System: A Key Bridge Between Mind and Body\n. The endocrine (hormonal) system regulates (and is related by) emotion, thus guiding all\naspects of intelligence (due to the close connection of emotion and motivation) [PH12].\nBreathing Guides Thinking\n. As oxygenation of the brain plays a key role in the spread of neural activity, the flow of breath\nis a key driver of cognition. Forced alternate nostril breathing has been shown to significantly\naffect cognition via balancing activity of the two brain hemispheres [SKBB91].\nMuch remains unknown, and the totality of feedback loops between the human cognitive\ncortex and the various specialized intelligences operative throughout the human body, has not\nyet been thoroughly charted.\n9.6 Body and Mind 175\n9.6.2.2 Implications for AGI\nWhat lesson should the AGI developer draw from all this? The particularities of the human\nmind/body should not be taken as general requirements for general intelligence. However, it\nis worth remembering just how difficult is the computational problem of learning, based on\nexperiential feedback alone, the right way to achieve the complex goal of controlling a system\nwith general intelligence at the human level or beyond. To solve this problem without some sort\nof strong inductive biasing may require massively more experience than young humans obtain.\nAppropriate inductive bias may be embedded in an AGI system in many different ways.\nSome AGI designers have sought to embed it very explicitly, e.g. with hand-coded declarative\nknowledge as in Cyc, SOAR and other \"GOFAI\" type systems. On the other hand, the human\nbrain receives its inductive bias much more subtly and implicitly, both via the specifics of the\ninitial structure of the cognitive cortex, and via ongoing coupling of the cognitive cortex with\nother systems possessing more focused types of intelligence and more specific structures and/or\ndynamics.\nIn building an AGI system, one has four choices, very broadly speaking:\n1. Create a flexible mind-network, as unbiased as feasible, and attempt to have it learn how\nto achieve its goals via experience\n2. Closely emulate key aspects of the human body along with the human mind\n3. Imitate the human mind-body, conceptually if not in detail, and create a number of structurally\nand dynamically simpler intelligent systems closely and appropriately coupled to\nthe abstract cognitive mind-network, provide useful inductive bias.\n4. Find some other, creative way to guide and probabilistically constrain one’s AGI system’s\nmind-network, providing inductive bias appropriate to the tasks at hand, without emulating\neven conceptually the way the human mind-brain receives its inductive bias via coupling\nwith simpler intelligent systems.\nOur suspicion is that the first option will not be viable. On the other hand, to do the second\noption would require more knowledge of the human body than biology currently possesses. This\nleaves the third and fourth options, both of which seem viable to us.\nCogPrime incorporates a combination of the third and fourth options. CogPrime’s generic\ndynamic knowledge store, the Atomspace, is coupled with specialized hierarchical networks\n(DeSTIN) for vision and audition, somewhat mirroring the human cortex. An artificial endocrine\nsystem for OpenCog is also under development, speculatively, as part of a project using\nOpenCog to control humanoid robots. On the other hand, OpenCog has no gastrointestinal nor\ncardiological nervous system, and the stress-response-based guidance provided to the human\nbrain by a combination of the heart, gut, immune system and other body systems, is achieved\nin CogPrime in a more explicit way using the OpenPsi model of motivated cognition, and its\nintegration with the system’s attention allocation dynamics.\nLikely there is no single correct way to incorporate the lessons of intelligent human bodysystem\nnetworks into AGI designs. But these are aspects of human cognition that all AGI\nresearchers should be aware of.\n176 9 General Intelligence in the Everyday Human World\n9.7 The Extended Mind and Body\nFinally, Hutchins [Hut95], Logan [Log07] and others have promoted a view of human intelligence\nthat views the human mind as extended beyond the individual body, incorporating social\ninteractions and also interactions with inanimate objects, such as tools, plants and animals.\nThis leads to a number of requirements for a humanlike AGI’s environment:\n1. The ability to create a variety of different tools for interacting with various aspects of the\nworld in various different ways, including tools for making tools and ultimately machinery\n2. The existence of other mobile, virtual life-forms in the world, including simpler and less\nintelligent ones, and ones that interact with each other and with the AGI\n3. The existence of organic growing structures in the world, with which the AGI can interact\nin various ways, including halting their growth or modifying their growth pattern\nHow necessary these requirements are is hard to say – but it is clear that these things have\nplayed a major role in the evolution of human intelligence.\n9.8 Conclusion\nHappily, this diverse chapter supports a simple, albeit tentative conclusion. Our suggestion is\nthat, if an AGI is\n• placed in an environment capable of roughly supporting multimodal communication and\nvaguely (but not necessarily precisely) real-world-ish naive physics\n• surrounded with other intelligent agents of varying levels of complexity, and other complex,\ndynamic structures to interface with\n• given a body that can perceive this environment through some forms of sight, sound and\ntouch; and perceive itself via some form of kinesthesia\n• given a motivational system that encourages it to make rich use of these aspects of its\nenvironment\nthen the AGI is likely to have an experience-base reinforcing the key inductive biases provided\nby the everyday world for the guidance of humanlike intelligence.\nChapter 10\nA Mind-World Correspondence Principle\n10.1 Introduction\nReal-world minds are always adapted to certain classes of environments and goals. The ideas\nof the previous chapter, regarding the connection between a human-like intelligence’s internals\nand its environment, result from exploring the implications of this adaptation in the context\nof the cognitive synergy concept. In this chapter we explore the mind-world connection in a\nbroader and more abstract way – making a more ambitious attempt to move toward a \"general\ntheory of general intelligence.\"\nOne basic premise here, as in the preceding chapters is: Even a system of vast general\nintelligence, subject to real-world space and time constraints, will necessarily be more efficient\nat some kinds of learning than others. Thus, one approach to formulating a general theory of\ngeneral intelligence is to look at the relationship between minds and worlds – where a \"world\"\nis conceived as an environment and a set of goals defined in terms of that environment.\nIn this spirit, we here formulate a broad principle binding together worlds and the minds that\nare intelligent in these worlds. The ideas of the previous chapter constitute specific, concrete\ninstantiations of this general principle. A careful statement of the principle requires introduction\nof a number of technical concepts, and will be given later on in the chapter. A crude, informal\nversion of the principle would be:\nMIND-WORLD CORRESPONDENCE-PRINCIPLE\nFor a mind to work intelligently toward certain goals in a certain world, there should be a\nnice mapping from goal-directed sequences of world-states into sequences of mind-states, where\n\"nice\" means that a world-state-sequence W composed of two parts W 1 and W 2 , gets mapped\ninto a mind-state-sequence M composed of two corresponding parts M 1 and M 2 .\nWhat’s nice about this principle is that it relates the decomposition of the world into parts,\nto the decomposition of the mind into parts.\n177\n178 10 A Mind-World Correspondence Principle\n10.2 What Might a General Theory of General Intelligence Look\nLike?\nIt’s not clear, at this point, what a real \"general theory of general intelligence\" would look like\n– but one tantalizing possibility is that it might confront the two questions:\n• How does one design a world to foster the development of a certain sort of mind?\n• How does one design a mind to match the particular challenges posed by a certain sort of\nworld?\nOne way to achieve this would be to create a theory that, given a description of an environment\nand some associated goals, would output a description of the structure and dynamics that a\nsystem should possess to be intelligent in that environment relative to those goals, using limited\ncomputational resources.\nSuch a theory would serve a different purpose from the mathematical theory of \"universal\nintelligence\" developed by Marcus Hutter [Hut05] and others. For all its beauty and theoretical\npower, that approach currently gives it useful conclusions only about general intelligences\nwith infinite or infeasibly massive computational resources. On the other hand, the approach\nsuggested here is aimed toward creation of a theory of real-world general intelligences utilizing\nrealistic amounts of computational power, but still possessing general intelligence comparable\nto human beings or greater.\nThis reflects a vision of intelligence as largely concerned with adaptation to particular classes\nof environments and goals. This may seem contradictory to the notion of \"general\" intelligence,\nbut I think it actually embodies a realistic understanding of general intelligence. Maximally\ngeneral intelligence is not pragmatically feasible; it could only be achieved using infinite computational\nresources [Hut05]. Real-world systems are inevitably limited in the intelligence they\ncan display in any real situation, because real situations involve finite resources, including finite\namounts of time. One may say that, in principle, a certain system could solve any problem\ngiven enough resources and time but, even when this is true, it’s not necessarily the most interesting\nway to look at the system’s intelligence. It may be more important to look at what a\nsystem can do given the resources at its disposal in reality. And this perspective leads one to\nask questions like the ones posed above: which bounded-resources systems are well-disposed to\ndisplay intelligence in which classes of situations?\nAs noted in Chapter 7 above, one can assess the generality of a system’s intelligence via\nlooking at the entropy of the class of situations across which it displays a high level of intelligence\n(where “high” is measured relative to its total level of intelligence across all situations). A system\nwith a high generality of intelligence will tend to be roughly equally intelligent across a wide\nvariety of situations; whereas a system with lower generality of intelligence will tend to be much\nmore intelligent in a small subclass of situations, than in any other. The definitions given above\nembody this notion in a formal and quantitative way.\nIf one wishes to create a general theory of general intelligence according to this sort of\nperspective, the main question then becomes how to represent goals/environments and systems\nin such a way as to render transparent the natural correspondence between the specifics of the\nformer and the latter, in the context of resource-bounded intelligence. This is the business of\nthe next section.\n10.3 Steps Toward A (Formal) General Theory of General Intelligence 179\n10.3 Steps Toward A (Formal) General Theory of General Intelligence\nNow begins the formalism. At this stage of development of the theory proposed in this chapter,\nmathematics is used mainly as a device to ensure clarity of expression. However, once the theory\nis further developed, it may possibly become useful for purposes of calculation as well.\nSuppose one has any system S (which could be an AI system, or a human, or an environment\nthat a human or AI is interacting with, or the combination of an environment and a human or\nAI’s body, etc.). One may then construct an uncertain transition graph associated with that\nsystem S, in the following way:\n• The nodes of the graph represent fuzzy sets of states of system S (I’ll call these state-sets\nfrom here on, leaving the fuzziness implicit)\n• The (directed) links of the graph represent probabilistically weighted transitions between\nstate-sets\nSpecifically, the weight of the link from B to A should be defined as\nwhere\nP (o(S, A, t(T ))|o(S, B, T ))\no(S, A, T )\ndenotes the presence of the system S in the state-set A during time-distribution T , and t() is\na temporal succession function defined so that t(T ) refers to a time-distribution conceived as\n\"after\" T . A time-distribution is a probability distribution over time-points. The interaction of\nfuzziness and probability here is fairly straightforward and may be handled in the manner of\nPLN, as outlined in subsequent chapters. Note that the definition of link weights is dependent\non the specific implementation of the temporal succession function, which includes an implicit\ntime-scale.\nSuppose one has a transition graph corresponding to an environment; then a goal relative to\nthat environment may be defined as a particular node in the transition graph. The goals of a\nparticular system acting in that environment may then be conceived as one or more nodes in\nthe transition graph. The system’s situation in the environment at any point in time may also\nbe associated with one or more nodes in the transition graph; then, the system’s movement\ntoward goal-achievement may be associated with paths through the environment’s transition\ngraph leading from its current state to goal states.\nIt may be useful for some purposes to filter the uncertain transition graph into a crisp\ntransition graph by placing a threshold on the link weights, and removing links with weights\nbelow the threshold.\nThe next concept to introduce is the world-mind transfer function, which maps world (environment)\nstate-sets into organism (e.g. AI system) state-sets in a specific way. Given a world\nstate-set W , the world-mind transfer function M maps W into various organism state-sets with\nvarious probabilities, so that we may say: M(W ) is the probability distribution of state-sets the\norganism tends to be in, when its environment is in state-set W . (Recall also that state-sets are\nfuzzy.)\nNow one may look at the spaces of world-paths and mind-paths. A world-path is a path\nthrough the world’s transition graph, and a mind-path is a path through the organism’s transi-\n180 10 A Mind-World Correspondence Principle\ntion graph. Given two world-paths P and Q, it’s obvious how to define the composition P ∗Q one\nfollows P and then, after that, follows Q, thus obtaining a longer path. Similarly for mind-paths.\nIn category theory terms, we are constructing the free category associated with the graph:\nthe objects of the category are the nodes, and the morphisms of the category are the paths.\nAnd category theory is the right way to be thinking here we want to be thinking about the\nrelationship between the world category and the mind category.\nThe world-mind transfer function can be interpreted as a mapping from paths to subgraphs:\nGiven a world-path, it produces a set of mind state-sets, which have a number of links between\nthem. One can then define a world-mind path transfer function M(P ) via taking the mind-graph\nM(nodes(P )), and looking at the highest-weight path spanning M(nodes(P )). (Here nodes?\nobviously means the set of nodes of the path P .)\nA functor F between the world category and the mind category is a mapping that preserves\nobject identities and so that\nF (P ∗ Q) = F (P ) ∗ F (Q)\nWe may also introduce the notion of an approximate functor, meaning a mapping F so that\nthe average of\nd(F (P ∗ Q), F (P ) ∗ F (Q))\nis small.\nOne can introduce a prior distribution into the average here. This could be the Levin universal\ndistribution or some variant (the Levin distribution assigns higher probability to computationally\nsimpler entities). Or it could be something more purpose specific: for example, one can give\na higher weight to paths leading toward a certain set of nodes (e.g. goal nodes). Or one can\nuse a distribution that weights based on a combination of simplicity and directedness toward\na certain set of nodes. The latter seems most interesting, and I will define a goal-weighted approximate\nfunctor as an approximate functor, defined with averaging relative to a distribution\nthat balances simplicity with directedness toward a certain set of goal nodes.\nThe move to approximate functors is simple conceptually, but mathematically it’s a fairly\nbig step, because it requires us to introduce a geometric structure on our categories. But there\nare plenty of natural metrics defined on paths in graphs (weighted or not), so there’s no real\nproblem here.\n10.4 The Mind-World Correspondence Principle\nNow we finally have the formalism set up to make a non-trivial statement about the relationship\nbetween minds and worlds. Namely, the hypothesis that:\nMIND-WORLD CORRESPONDENCE PRINCIPLE\nFor an organism with a reasonably high level of intelligence in a certain world, relative to\na certain set of goals, the mind-world path transfer function is a goal-weighted approximate\nfunctor.\n10.5 How Might the Mind-World Correspondence Principle Be Useful? 181\nThat is, a little more loosely: the hypothesis is that, for intelligence to occur, there has to be a\nnatural correspondence between the transition-sequences of world-states and the corresponding\ntransition-sequences of mind-states, at least in the cases of transition-sequences leading to\nrelevant goals.\nWe suspect that a variant of the above proposition can be formally proved, using the definition\nof general intelligence presented in Chapter 7. The proof of a theorem corresponding to the\nabove would certainly constitute an interesting start toward a general formal theory of general\nintelligence. Note that proving anything of this nature would require some attention to the\ntime-scale-dependence of the link weights in the transition graphs involved.\nA formally proved variant of the above proposition would be in short, a \"MIND-WORLD\nCORRESPONDENCE THEOREM.\"\nRecall that at the start of the chapter, we expressed the same idea as:\nMIND-WORLD CORRESPONDENCE-PRINCIPLE\nFor a mind to work intelligently toward certain goals in a certain world, there should be a\nnice mapping from goal-directed sequences of world-states into sequences of mind-states, where\n\"nice\" means that a world-state-sequence W composed of two parts W 1 and W 2 , gets mapped\ninto a mind-state-sequence M composed of two corresponding parts M 1 and M 2 .\nThat is a reasonable gloss of the principle, but it’s clunkier and less accurate, than the\nstatement in terms of functors and path transfer functions, because it tries to use only commonlanguage\nvocabulary, which doesn’t really contain all the needed concepts.\n10.5 How Might the Mind-World Correspondence Principle Be\nUseful?\nSuppose one believes the Mind-World Correspondence Principle as laid out above so what?\nOur hope, obviously, is that the principle could be useful in actually figuring out how to\narchitect intelligent systems biased toward particular sorts of environment. And of course, this\nis said with the understanding that any finite intelligence must be biased toward some sorts of\nenvironment.\nRelatedly, given a specific AGI design (such as CogPrime), one could use the principle to\nfigure out which environments it would be best suited for. Or one could figure out how to\nadjust the particulars of the design, to maximize the system’s intelligence in the environments\nof interest.\nOne next step in developing this network of ideas, aside from (and potentially building on)\nfull formalization of the principle, would be an exploration of real-world environments in terms\nof transition graphs. What properties do the transition graphs induced from the real world\nhave?\nOne such property, we suggest, is successive refinement. Often the path toward a goal involves\nfirst gaining an approximate understanding of a situation, then a slightly more accurate\nunderstanding, and so forth – until finally one has achieved a detailed enough understanding to\nactually achieve the goal. This would be represented by a world-path whose nodes are state-sets\ninvolving the gathering of progressively more detailed information.\n182 10 A Mind-World Correspondence Principle\nVia pursuing to the mind-world correspondence property in this context, I believe we will\nfind that world-paths reflecting successive refinement correspond to mind-paths embodying successive\nrefinement. This will be found to relate to the hierarchical structures found so frequently\nin both the physical world and the human mind-brain. Hierarchical structures allow many relevant\ngoals to be approached via successive refinement, which I believe is the ultimate reason\nwhy hierarchical structures are so common in the human mind-brain.\nAnother next step would be exploring what mind-world correspondence means for the structure\nand dynamics of a limited-resources intelligence. If an organism O has limited resources\nand, to be intelligent, needs to make\nP (o(O, M(A), t(T ))|o(O, M(B), T ))\nhigh for particular world state-sets A and B, then what’s the organism’s best approach?\nArguably, it should represent M(A) and M(B) internally in such a way that very little computational\neffort is required for it to transition between M(A) and M(B). For instance, this could\nbe done by coding its knowledge in such a way that M(A) and M(B) share many common bits;\nor it could be done in other more complicated ways.\nIf, for instance, A is a subset of B, then it may prove beneficial for the organism to represent\nM(A) physically as a subset of its representation of M(B).\nPursuing this line of thinking, one could likely derive specific properties of an intelligent\norganism’s internal information-flow, from properties of the environment and goals with respect\nto which it’s supposed to be intelligent.\nThis would allow us to achieve the holy grail of intelligence theory as I understand it: given\na description of an environment and goals, to be able to derive an architectural description for\nan organism that will display a high level of intelligence relative to those goals, given limited\ncomputational resources.\nWhile this “holy grail” is obviously a far way off, what we’ve tried to do here is to outline a\nclear mathematical and conceptual direction for moving toward it.\n10.6 Conclusion\nThe Mind-World Correspondence Principle presented here – if in the vicinity of correctness –\nconstitutes a non-trivial step toward fleshing out the concept of a general theory of general\nintelligence. But obviously the theory is still rather abstract, and also not completely rigorous.\nThere’s a lot more work to be done.\nThe Mind-World Correspondence Principle as articulated above is not quite a formal mathematical\nstatement. It would take a little work to put in all the needed quantifiers to formulate\nit as one, and it’s not clear the best way to do so the details would perhaps become clear in the\ncourse of trying to prove a version of it rigorously. One could interpret the ideas presented in\nthis chapter as a philosophical theory that hopes to be turned into a mathematical theory and\nto play a key role in a scientific theory.\nFor the time being, the main role to be served by these ideas is qualitative: to help us think\nabout concrete AGI designs like CogPrime in a sensible way. It’s important to understand what\nthe goal of a real-world AGI system needs to be: to achieve the ability to broadly learn and\ngeneralize, yes, but not with infinite capability rather with biases and patterns that are implicitly\nand/or explicitly tuned to certain broad classes of goals and environments. The Mind-World\n10.6 Conclusion 183\nCorrespondence Principle tells us something about what this \"tuning\" should involve – namely,\nmaking a system possessing mind-state sequences that correspond meaningfully to world-state\nsequences. CogPrime’s overall design and particular cognitive processes are reasonably well\ninterpreted as an attempt to achieve this for everyday human goals and environments.\nOne way of extending these theoretical ideas into a more rigorous theory is explored in Appendix\n??. The key ideas involved there are: modeling multiple memory types as mathematical\ncategories (with functors mapping between them), modeling memory items as probability distributions,\nand measuring distance between memory items using two metrics, one based on\nalgorithmic information theory and one on classical information geometry. Building on these\nideas, core hypotheses are then presented:\n• a syntax-semantics correlation principle, stating that in a successful AGI system, these\ntwo metrics should be roughly correlated\n• a cognitive geometrodynamics principle, stating that on the whole intelligent minds\ntend to follow geodesics (shortest paths) in mindspace, according to various appropriately\ndefined metrics (e.g. the metric measuring the distance between two entities in terms of the\nlength and/or runtime of the shortest programs computing one from the other).\n• a cognitive synergy principle, stating that shorter paths may be found through the composite\nmindspace formed by considering multiple memory types together, than by following\nthe geodesics in the mindspaces corresponding to individual memory types.\nThe material is relegated to an appendix because it is so speculative, and it’s not yet clear\nwhether it will really be useful in advancing or interpreting CogPrime or other AGI systems\n(unlike the material from the present chapter, which has at least been useful in interpreting\nand tweaking the CogPrime design, even though it can’t be claimed that CogPrime was derived\ndirectly from these theoretical ideas). However, this sort of speculative exploration is, in our\nview, exactly the sort of thing that’s needed as a first phase in transitioning the ideas of the\npresent chapter into a more powerful and directly actionable theory.\n\nSection III\nCognitive and Ethical Development\n\nChapter 11\nStages of Cognitive Development\nCo-authored with Stephan Vladimir Bugaj\n11.1 Introduction\nCreating AGI, we have said, is not only about having the right structural and dynamical\npossibilities implemented in the initial version of one’s system – but also about the environment\nand embodiment that one’s system is associated with, and the match between the system’s\ninternals and these externals. Another key aspect is the long-term time-course of the system’s\nevolution over time, both in its internals and its external interaction – i.e., what is known as\ndevelopment.\nDevelopment is a critical topic in our approach to AGI because we believe that much of\nwhat constitutes human-level, human-like intelligence emerges in an intelligent system due to\nits engagement with its environment and its environment-coupled self-organization. So, it’s not\nto be expected that the initial version of an AGI system is going to display impressive feats\nof intelligence, even if the engineering is totally done right. A good analogy is the apparent\nunintelligence of a human baby. Yes, scientists have discovered that human babies are capable\nof interesting and significant intelligence – but one has to hunt to find it ... at first observation,\nbabies are rather idiotic and simple-minded creatures: much less intelligent-appearing than\nlizards or fish, maybe even less than cockroaches....\nIf the goal of an AGI project is to create an AGI system that can progressively develop\nadvanced intelligence through learning in an environment richly populated with other agents\nand various inanimate stimuli and interactive entities – then an understanding of the nature of\ncognitive development becomes extremely important to that project.\nUnfortunately, contemporary cognitive science contains essentially no theory of “abstract\ndevelopmental psychology” which can conveniently be applied to understand developing AIs.\nThere is of course an extensive science of human developmental psychology, and so it is a\nnatural research program to take the chief ideas from the former and inasmuch as possible port\nthem to the AGI domain. This is not an entirely simple matter both because of the differences\nbetween humans and AIs and because of the unsettled nature of contemporary developmental\npsychology theory. But it’s a job that must (and will) be done, and the ideas in this chapter\nmay contribute toward this effort.\nWe will begin here with Piaget’s well-known theory of human cognitive development, presenting\nit in a general systems theory context, then introducing some modifications and extensions\nand discussing some other relevant work.\n187\n188 11 Stages of Cognitive Development\n11.2 Piagetan Stages in the Context of a General Systems Theory of\nDevelopment\nOur review of AGI architectures in Chapter 4 focused heavily on the concept of symbolism,\nand the different ways in which different classes of cognitive architecture handle symbol representation\nand manipulation. We also feel that symbolism is critical to the notion of AGI\ndevelopment – and even more broadly, to the systems theory of development in general.\nAs a broad conceptual perspective on development, we suggest that one may view the development\nof a complex information processing system, embedded in an environment, in terms\nof the stages:\n• automatic: the system interacts with the environment by “instinct”, according to its innate\nprogramming\n• adaptive: the system internally adapts to the environment, then interacting with the environment\nin a more appropriate way\n• symbolic: the system creates internal symbolic representations of itself and the environment,\nwhich in the case of a complex, appropriately structured environment, allows it to\ninteract with the environment more intelligently\n• reflexive: the system creates internal symbolic representations of its own internal symbolic\nrepresentations, thus achieving an even higher degree of intelligence\nSketched so broadly, these are not precisely defined categories but rather heuristic, intuitive\ncategories. Formalizing them would be possible but would lead us too far astray here.\nOne can interpret these stages in a variety of different contexts. Here our focus is the cognitive\ndevelopment of humans and human-like AGI systems, but in Table 11.1 we present them in\na slightly more general context, using two examples: the Piagetan example of the human (or\nhumanlike) mind as it develops from infancy to maturity; and also the example of the “origin\nof life” and the development of life from proto-life up into its modern form. In any event, we\nallude to this more general perspective on development here mainly to indicate our view that\nthe Piagetan perspective is not something ad hoc and arbitrary, but rather can plausibly be seen\nas a specific manifestation of more fundamental principles of complex systems development.\n11.3 Piaget’s Theory of Cognitive Development\nThe ghost of Jean Piaget hangs over modern developmental psychology in a yet unresolved\nway. Piaget’s theories provide a cogent overarching perspective on human cognitive development,\ncoordinating broad theoretical ideas and diverse experimental results into a unified whole\n[Pia55]. Modern experimental work has shown Piaget’s ideas to be often oversimplified and incorrect.\nHowever, what has replaced the Piagetan understanding is not an alternative unified\nand coherent theory, but a variety of microtheories addressing particular aspects of cognitive\ndevelopment. For this reason a number of contemporary theorists taking a computer science\n[Shu03] or dynamical systems [Wit07] approach to developmental psychology have chosen to\nadopt the Piagetan framework in spite of its demonstrated shortcomings, both because of its\nconceptual strengths and for lack of a coherent, more rigorously grounded alternative.\nOur own position is that the Piagetan view of development has some fundamental truth to it,\nwhich is reflected via how nicely it fits with a broader view of development in complex systems.\n11.3 Piaget’s Theory of Cognitive Development 189\nStage General Description Cognitive Development\nOrigin of Life\nAutomatic System-environment Piagetan infantile Self-organizing protolife\ninformation exchange stage\nsystem, e.g. Oparin\ncontrolled mainly by\n[Opa52] water droplet,\ninnate system structures\nor Cairns-Smith [CS90]\nor environment\nclay-based\nprotolife\nAdaptive\nSymbolic\nSystem-environment\ninfo exchange heavily\nguided by adaptively\ninternally-created\nsystem structures\nInternal symbolic representation\nof information\nexchange process\nReflexive Thoroughgoing selfmodification\nPiagetan\nbased\non this symbolic\nrepresentation\nPiagetan “concrete operational”\nstage: systematic\ninternal worldmodel\nguides worldexploration\nPiagetan formal stage:\nexplicit logical/experimental\nlearning about\nhow to cognize in various\ncontexts\nstage: purposive selfmodification\nof basic\nmental processes\nSimple autopoietic system,\ne.g. Oparin water\ndroplet w/ basic\nmetabolism\nGenetic code: internal\nentities that “stand\nfor” aspects of organism\nand environment,\nthus enabling complex\nepigenesis\npost-formal Genes+memes: genetic\ncode-patterns guide\ntheir own modification\nvia influencing culture\nTable 11.1: General Systems Theory of Development: Parallels Between Development of Mind\nand Origin of Life\nIndeed, Piaget viewed developmental stages as emerging from general “algebraic” principles\nrather than as being artifacts of the particulars of human psychology. But, Piaget’s stages are\nprobably best viewed as a general interpretive framework rather than a precise scientific theory.\nOur suspicion is that once the empirical science of developmental psychology has progressed\nfurther, it will become clearer how to fit the various data into a broad Piaget-like framework,\nperhaps differing in many details from what Piaget described in his works.\nPiaget conceived of child development in four stages, each roughly identified with an age\ngroup, and corresponding closely to the system-theoretic stages mentioned above:\n• infantile, corresponding to the automatic stage mentioned above\n– Example: Grasping blocks, piling blocks on top of each other, copying words that are\nheard\n• preoperational and concrete operational, corresponding to the adaptive stage mentioned\nabove\n– Example: Building complex blocks structures, from imagination and from imitating\nobjects and pictures and based on verbal instructions; verbally describing what has\nbeen constructed\n• formal, corresponding to the symbolic stage mentioned above\n– Example: Writing detailed instructions in words and diagrams, explaining how to construct\nparticular structures out of blocks; figuring out general rules describing which\nsorts of blocks structures are likely to be most stable\n190 11 Stages of Cognitive Development\n• the reflexive stage mentioned above corresponds to what some post-Piagetan theorists have\ncalled the post-formal stage\n– Example: Using abstract lessons learned from building structures out of blocks to guide\nthe construction of new ways to think and understand – “Zen and the art of blocks\nbuilding” (by analogy to Zen and the Art of Motorcycle Maintenance [Pir84]).\nFig. 11.1: Piagetan Stages of Cognitive Development\nMore explicitly, Piaget defined his stages in psychological terms roughly as follows:\n• Infantile: In this stage a mind develops basic world-exploration driven by instinctive actions.\nReward-driven reinforcement of actions learned by imitation, simple associations between\nwords and objects, actions and images, and the basic notions of time, space, and\ncausality are developed. The most simple, practical ideas and strategies for action are\nlearned.\n• Preoperational: At this stage we see the formation of mental representations, mostly\npoorly organized and un-abstracted, building mainly on intuitive rather than logical thinking.\nWord-object and image-object associations become systematic rather than occasional.\nSimple syntax is mastered, including an understanding of subject-argument relationships.\nOne of the crucial learning achievements here is “object permanence” – infants learn that\nobjects persist even when not observed. However, a number of cognitive failings persist with\nrespect to reasoning about logical operations, and abstracting the effects of intuitive actions\nto an abstract theory of operations.\n• Concrete: More abstract logical thought is applied to the physical world at this stage.\nAmong the feats achieved here are: reversibility – the ability to undo steps already done;\nconservation – understanding that properties can persist in spite of appearances; theory of\nmind – an understanding of the distinction between what I know and what others know (If\n11.3 Piaget’s Theory of Cognitive Development 191\nI cover my eyes, can you still see me?). Complex concrete operations, such as putting items\nin height order, are easily achievable. Classification becomes more sophisticated, yet the\nmind still cannot master purely logical operations based on abstract logical representations\nof the observational world.\n• Formal: Abstract deductive reasoning, the process of forming, then testing hypotheses, and\nsystematically reevaluating and refining solutions, develops at this stage, as does the ability\nto reason about purely abstract concepts without reference to concrete physical objects.\nThis is adult human-level intelligence. Note that the capability for formal operations is\nintrinsic in the PLN component of CogPrime, but in-principle capability is not the same as\npragmatic, grounded, controllable capability.\nVery early on, Vygotsky [Vyg86] disagreed with Piaget’s explanation of his stages as inherent\nand developed by the child’s own activities, and Piaget’s prescription of good parenting as\nnot interfering with a child’s unfettered exploration of the world. Some modern theorists have\ncritiqued Piaget’s stages as being insufficiently socially grounded, and these criticisms trace back\nto Vygotsky’s focus on the social foundations of intelligence, on the fact that children function\nin a world surrounded by adults who provide a cultural context, offering ongoing assistance,\ncritique, and ultimately validation of the child’s developmental activities.\nVygotsky also was an early critic of the idea that cognitive development is continuous,\nand continues beyond Piaget’s formal stage. Gagne [RBW92] also believes in continuity, and\nthat learning of prerequisite skills made the learning of subsequent skills easier and faster\nwithout regard to Piagetan stage formalisms. Subsequent researchers have argued that Piaget\nhas merely constructed ad hoc descriptions of the sequential development of behaviour\n[Gib78, Bro84, CP05]. We agree that learning is a continuous process, and our notion of stages\nis more statistically constructed than rigidly quantized.\nCritique of Piaget’s notion of transitional “half stages” is also relevant to a more comprehensive\nhierarchical view of development. Some have proposed that Piaget’s half stages are\nactually stages [Bro84]. As Commons and Pekker [CP05] point out: “the definition of a stage\nthat was being used by Piaget was based on analyzing behaviors and attempting to impose\ndifferent structures on them. There is no underlying logical or mathematical definition to help\nin this process . . . ” Their Hierarchical Complexity development model uses task achievement\nrather than ad hoc stage definition as the basis for constructing relationships between phases\nof developmental ability – an approach which we find useful, though our approach is different\nin that we define stages in terms of specific underlying cognitive mechanisms.\nAnother critique of Piaget is that one individual’s performance is often at different ability\nstages depending on the specific task (for example [GE86]). Piaget responded to early critiques\nalong these lines by calling the phenomenon “horizontal décalage,” but neither he nor his successors\n[Fis80, Cas85] have modified his theory to explain (rather than merely describe) it.\nSimilarly to Thelen and Smith [TS94], we observe that the abilities encapsulated in the definition\nof a certain stage emerge gradually during the previous stage – so that the onset of a given\nstage represents the mastery of a cognitive skill that was previously present only in certain\ncontexts.\nPiaget also had difficulty accepting the idea of a preheuristic stage, early in the infantile\nperiod, in which simple trial-and-error learning occurs without significant heuristic guidance\n[Bic88], a stage which we suspect exists and allows formulation of heuristics by aggregation of\nlearning from preheuristic pattern mining. Coupled with his belief that a mind’s innate abilities\nat birth are extremely limited, there is a troublingly unexplained transition from inability to\nability in his model.\n192 11 Stages of Cognitive Development\nFinally, another limiting aspect of Piaget’s model is that it did not recognize any stages\nbeyond formal operations, and included no provisions for exploring this possibility. A number of\nresearchers [Bic88, Arl75, CRK82, Rie73, Mar01] have described one or more postformal stages.\nCommons and colleagues have also proposed a task-based model which provides a framework for\nexplaining stage discrepancies across tasks and for generating new stages based on classification\nof observed logical behaviors. [KK90] promotes a statistical conception of stage, which provides a\ngood bridge between task-based and stage-based models of development, as statistical modeling\nallows for stages to be roughly defined and analyzed based on collections of task behaviors.\n[CRK82] postulates the existence of a postformal stage by observing elevated levels of abstraction\nwhich, they argue, are not manifested in formal thought. [CTS + 98] observes a postformal\nstage when subjects become capable of analyzing and coordinating complex logical systems\nwith each other, creating metatheoretical supersystems. In our model, with the reflexive stage\nof development, we expand this definition of metasystemic thinking to include the ability to\nconsciously refine one’s own mental states and formalisms of thinking. Such self-reflexive refinement\nis necessary for learning which would allow a mind to analytically devise entirely new\nstructures and methodologies for both formal and postformal thinking.\nIn spite of these various critiques and limitations, however, we have found Piaget’s ideas\nvery useful, and in Section 11.4 we will explore ways of defining them rigorously in the specific\ncontext of CogPrime’s declarative knowledge store and probabilistic logic engine.\n11.3.1 Perry’s Stages\nAlso relevant is William Perry’s [Per70, Per81] theory of the stages (“positions” in his terminology)\nof intellectual and ethical development, which constitutes a model of iterative refinement\nof approach in the developmental process of coming to intellectual and ethical maturity. These\nstages, depicted in Table 11.2 form an analytical tool for discerning the modality of belief of\nan intelligence by describing common cognitive approaches to handling the complexities of real\nworld ethical considerations.\n11.3.2 Keeping Continuity in Mind\nContinuity of mental stages, and the fact that a mind may appear to be in multiple stages\nof development simultaneously (depending upon the tasks being tested), are crucial to our\ntheoretical formulations and we will touch upon them again here. Piaget attempted to address\ncontinuity with the creation of transitional “half stages”. We prefer to observe that each stage\nfeeds into the other and the end of one stage and the beginning of the next blend together.\nThe distinction between formal and post-formal, for example, seems to “merely” be the\napplication of formal thought to oneself. However, the distinction between concrete and formal is\n“merely” the buildup to higher levels of complexity of the classification, task decomposition, and\nabstraction capabilities of the concrete stage. The stages represent general trends in ability on\na continuous curve of development, not discrete states of mind which are jumped-into quantum\nstyle after enough “knowledge energy” builds-up to cause the transition.\n11.4 Piaget’s Stages in the Context of Uncertain Inference 193\nStage\nSubstages\nDualism / Received Basic duality (“All problems are solvable. I must learn the\nKnowledge\n[Infantile]\ncorrect solutions.”)\nFull dualism (“There are different, contradictory solutions to\nmany problems. I must learn the correct solutions, and ignore\nthe incorrect ones”)\nMultiplicity\n[Concrete]\nRelativism / Procedural\nKnowledge\n[Formal]\nCommitment / Constructed\nKnowledge\n[Formal / Reflexive]\nEarly multiplicity (“Some solutions are known, others aren’t.\nI must learn how to find correct solutions.”)\nLate Multiplicity: cognitive dissonance regarding truth.\n(“Some problems are unsolvable, some are a matter of personal\ntaste, therefore I must declare my own intellectual path.”)\nContextual Relativism (“I must learn to evaluate solutions\nwithin a context, and relative to supporting observation.”)\nPre-Commitment (“I must evaluate solutions, then commit to\na choice of solution.”)\nCommitment (“I have chosen a solution.”)\nChallenges to Commitment (“I have seen unexpected implications\nof my commitment, and the responsibility I must take.”)\nPost-Commitment (“I must have an ongoing, nuanced relationship\nto the subject in which I evaluate each situation on a\ncase-by-case basis with respects to its particulars rather than\nan ad-hoc application of unchallenged ideology.”)\nTable 11.2: Perry’s Developmental Stages [with corresponding Piagetan Stages in brackets]\nObservationally, this appears to be the case in humans. People learn things gradually, and\nshow a continuous development in ability, not a quick jump from ignorance to mastery. We\nbelieve that this gradual development of ability is the signature of genuine learning, and that\nprescriptively an AGI system must be designed in order to have continuous and asymmetrical\ndevelopment across a variety of tasks in order to be considered a genuine learning system. While\nquantum leaps in ability may be possible in an AGI system which can just “graft” new parts\nof brain onto itself (or an augmented human which may someday be able to do the same using\nimplants), such acquisition of knowledge is not really learning. Grafting on knowledge does not\nbuild the cognitive pathways needed in order to actually learn. If this is the only mechanism\navailable to an AGI system to acquire new knowledge, then it is not really a learning system.\n11.4 Piaget’s Stages in the Context of Uncertain Inference\nPiaget’s developmental stages are very general, referring to overall types of learning, not specific\nmechanisms or methods. This focus was natural since the context of his work was human developmental\npsychology, and neuroscience has not yet progressed to the point of understanding\nthe neural mechanisms underlying any sort of inference (and certainly was nowhere near to\ndoing so in Piaget’s time!). But if one is studying developmental psychology in an AGI context\nwhere one knows something about the internal mechanisms of the AGI system under consideration,\nthen one can work with a more specific model of learning. Our focus here is on AGI\nsystems whose operations contain uncertain inference as a central component. Obviously the\nmain focus is CogPrime, but the essential ideas apply to any other uncertain inference centric\nAGI architecture as well.\n194 11 Stages of Cognitive Development\nFig. 11.2: Piagetan Stages of Development, as Manifested in the Context of Uncertain Inference\nAn uncertain inference system, as we consider it here, consists of four components, which\nwork together in a feedback-control loop 11.3\n1. a content representation scheme\n2. an uncertainty representation scheme\n3. a set of inference rules\n4. a set of inference control schemata\nFig. 11.3: A Simplified Look at Feedback-Control in Uncertain Inference\n11.4 Piaget’s Stages in the Context of Uncertain Inference 195\nBroadly speaking, examples of content representation schemes are predicate logic and term\nlogic [ES00]. Examples of uncertainty representation schemes are fuzzy logic [Zad78], imprecise\nprobability theory [Goo86, FC86], Dempster-Shafer theory [Sha76, Kyb97], Bayesian probability\ntheory [Kyb97], NARS [Wan95], and the Atom representation used in CogPrime, briefly alluded\nto in Chapter 6 above and described in depth in later chapters.\nMany, but not all, approaches to uncertain inference involve only a limited, weak set of inference\nrules (e.g. not dealing with complex quantified expressions). CogPrime’s PLN inference\nframework, like NARS and some other uncertain inference frameworks, contains uncertain inference\nrules that apply to logical constructs of arbitrary complexity. Only a system capable of\ndealing with constructs of arbitrary (or at least very high) complexity will have any potential\nof leading to human-level, human-like intelligence.\nThe subtlest part of uncertain inference is inference control: the choice of which inferences\nto do, in what order. Inference control is the primary area in which human inference currently\nexceeds automated inference. Humans are not very efficient or accurate at carrying out inference\nrules, with or without uncertainty, but we are very good at determining which inferences to do\nand in what order, in any given context. The lack of effective, context-sensitive inference control\nheuristics is why the general ability of current automated theorem provers is considerably weaker\nthan that of a mediocre university mathematics major [Mac95].\nWe now review the Piagetan developmental stages from the perspective of AGI systems\nheavily based on uncertain inference.\n11.4.1 The Infantile Stage\nIn this initial stage, the mind is able to recognize patterns in and conduct inferences about\nthe world, but only using simplistic hard-wired (not experientially learned) inference control\nschema, along with pre-heuristic pattern mining of experiential data.\nIn the infantile stage an entity is able to recognize patterns in and conduct inferences about\nits sensory surround context (i.e., it’s “world”), but only using simplistic, hard-wired (not experientially\nlearned) inference control schemata. Preheuristic pattern-mining of experiential data\nis performed in order to build future heuristics about analysis of and interaction with the world.\ns tasks include:\n1. Exploratory behavior in which useful and useless / dangerous behavior is differentiated by\nboth trial and error observation, and by parental guidance.\n2. Development of “habits” – i.e. Repeating tasks which were successful once to determine if\nthey always / usually are so.\n3. Simple goal-oriented behavior such as “find out what cat hair tastes like” in which one must\nplan and take several sequentially dependent steps in order to achieve the goal.\nInference control is very simple during the infantile stage (Figure 11.4), as it is the stage\nduring which both the most basic knowledge of the world is acquired, and the most basic of\ncognition and inference control structures are developed as the building block upon which will\nbe built the next stages of both knowledge and inference control.\nAnother example of a cognitive task at the borderline between infantile and concrete cognition\nis learning object permanence, a problem discussed in the context of CogPrime’s predecessor\n\"Novamente Cognition Engine\" system in [GPSL03]. Another example is the learning of\n196 11 Stages of Cognitive Development\nFig. 11.4: Uncertain Inference in the Infantile Stage\nword-object associations: e.g. learning that when the word “ball” is uttered in various contexts\n(“Get me the ball,” “That’s a nice ball,” etc.) it generally refers to a certain type of object.\nThe key point regarding these “infantile” inference problems, from the CogPrime perspective,\nis that assuming one provides the inference system with an appropriate set of perceptual and\nmotor ConceptNodes and SchemaNodes, the chains of inference involved are short. They involve\nabout a dozen inferences, and this means that the search tree of possible PLN inference rules\nwalked by the PLN backward-chainer is relatively shallow. Sophisticated inference control is\nnot required: standard AI heuristics are sufficient.\nIn short, textbook narrow-AI reasoning methods, utilized with appropriate uncertainty-savvy\ntruth value formulas and coupled with appropriate representations of perceptual and motor\ninputs and outputs, correspond roughly to Piaget’s infantile stage of cognition. The simplistic\napproach of these narrow-AI methods may be viewed as a method of creating building blocks\nfor subsequent, more sophisticated heuristics.\nIn our theory Piaget’s preoperational phase appears as transitional between the infantile and\nconcrete operational phases.\n11.4.2 The Concrete Stage\nAt this stage, the mind is able to carry out more complex chains of reasoning regarding the\nworld, via using inference control schemata that adapt behavior based on experience (reasoning\nabout a given case in a manner similar to prior cases).\nIn the concrete operational stage (Figure 11.5), an entity is able to carry out more complex\nchains of reasoning about the world. Inference control schemata which adapt behavior based on\nexperience, using experientially learned heuristics (including those learned in the prior stage),\nare applied to both analysis of and interaction with the sensory surround / world.\nConcrete Operational stage tasks include:\n11.4 Piaget’s Stages in the Context of Uncertain Inference 197\nFig. 11.5: Uncertain Inference in the Concrete Operational Stage\n1. Conservation tasks, such as conservation of number,\n2. Decomposition of complex tasks into easier subtasks, allowing increasingly complex tasks\nto be approached by association with more easily understood (and previously experienced)\nsmaller tasks,\n3. Classification and Serialization tasks, in which the mind can cognitively distinguish various\ndisambiguation criteria and group or order objects accordingly.\nIn terms of inference control this is the stage in which actual knowledge about how to control\ninference itself is first explored. This means an emerging understanding of inference itself as a\ncognitive task and methods for learning, which will be further developed in the following stages.\nAlso, in this stage a special cognitive task capability is gained: “Theory of Mind,\" which in\ncognitive science refers to the ability to understand the fact that not only oneself, but other\nsentient beings have memories, perceptions, and experiences. This is the ability to conceptually\n“put oneself in another’s shoes” (even if you happen to assume incorrectly about them by doing\nso).\n11.4.2.1 Conservation of Number\nConservation of number is an example of a learning problem classically categorized within\nPiaget’s concrete-operational phase, a “conservation laws” problem, discussed in [Shu03] in\nthe context of software that solves the problem using (logic-based and neural net) narrow-AI\ntechniques. Conservation laws are very important to cognitive development.\nConservation is the idea that a quantity remains the same despite changes in appearance. If\nyou show a child some objects and then spread them out, an infantile mind will focus on the\nspread, and believe that there are now more objects than before, whereas a concrete-operational\nmind will understand that the quantity of objects has not changed.\nConservation of number seems very simple, but from a developmental perspective it is actually\nrather difficult. “Solutions” like those given in [Shu03] that use neural networks or cus-\n198 11 Stages of Cognitive Development\ntomized logical rule-bases to find specialized solutions that solve only this problem fail to fully\naddress the issue, because these solutions don’t create knowledge adequate to aid with the\nsolution of related sorts of problems.\nWe hypothesize that this problem is hard enough that for an inference-based AGI system\nto solve it in a developmentally useful way, its inferences must be guided by meta-inferential\nlessons learned from prior similar problems. When approaching a number conservation problem,\nfor example, a reasoning system might draw upon past experience with set-size problems (which\nmay be trial-and-error experience). This is not a simple “machine learning” approach whose\nscope is restricted to the current problem, but rather a heuristically guided approach which (a)\naggregates information from prior experience to guide solution formulation for the problem at\nhand, and (b) adds the present experience to the set of relevant information about quantification\nproblems for future refinement of thinking.\nFig. 11.6: Conservation of Number\nFor instance, a very simple context-specific heuristic that a system might learn would be:\n“When evaluating the truth value of a statement related to the number of objects in a set,\nit is generally not that useful to explore branches of the backwards-chaining search tree that\ncontain relationships regarding the sizes, masses, or other physical properties of the objects in\nthe set.” This heuristic itself may go a long way toward guiding an inference process toward a\ncorrect solution to the problem–but it is not something that a mind needs to know “a priori.”\nA concrete-operational stage mind may learn this by data-mining prior instances of inferences\ninvolving sizes of sets. Without such experience-based heuristics, the search tree for such a\nproblem will likely be unacceptably large. Even if it is “solvable” without such heuristics, the\nsolutions found may be overly fit to the particular problem and not usefully generalizable.\n11.4.2.2 Theory of Mind\nConsider this experiment: a preoperational child is shown her favorite “Dora the Explorer” DVD\nbox. Asked what show she’s about to see, she’ll answer “Dora.” However, when her parent plays\nthe disc, it’s “SpongeBob SquarePants.” If you then ask her what show her friend will expect\nwhen given the “Dora” DVD box, she will respond “SpongeBob” although she just answered\n“Dora” for herself. A child lacking a theory of mind can not reason through what someone\nelse would think given knowledge other than her own current knowledge. Knowledge of self is\nintrinsically related to the ability to differentiate oneself from others, and this ability may not\nbe fully developed at birth.\nSeveral theorists [BC94, Fod94], based in part on experimental work with autistic children,\nperceive theory of mind as embodied in an innate module of the mind activated at a certain\ndevelopmental stage (or not, if damaged). While we consider this possible, we caution against\nadopting a simplistic view of the “innate vs. acquired” dichotomy: if there is innateness it may\ntake the form of an innate predisposition to certain sorts of learning [EBJ + 97].\n11.4 Piaget’s Stages in the Context of Uncertain Inference 199\nDavidson [Dav84], Dennett [Den87] and others support the common belief that theory of\nmind is dependent upon linguistic ability. A major challenge to this prevailing philosophical\nstance came from Premack and Woodruff [PW78] who postulated that prelinguistic primates\ndo indeed exhibit “theory of mind” behavior. While Premack and Woodruff’s experiment itself\nhas been challenged, their general result has been bolstered by follow-up work showing similar\nresults such as [TC97]. It seems to us that while theory of mind depends on many of the same\ninferential capabilities as language learning, it is not intrinsically dependent on the latter.\nThere is a school of thought often called the Theory Theory [BW88, Car85, Wel90] holding\nthat a child’s understanding of mind is best understood in terms of the process of iteratively\nformulating and refuting a series of naive theories about others. Alternately, Gordon [Gor86]\npostulates that theory of mind is related to the ability to run cognitive simulations of others’\nminds using one’s own mind as a model. We suggest that these two approaches are actually\nquite harmonious with one another. In an uncertain AGI context, both theories and simulations\nare grounded in collections of uncertain implications, which may be assembled in contextappropriate\nways to form theoretical conclusions or to drive simulations. Even if there is a\nspecial “mind-simulator” dynamic in the human brain that carries out simulations of other\nminds in a manner fundamentally different from explicit inferential theorizing, the inputs to\nand the behavior of this simulator may take inferential form, so that the simulator is in essence\na way of efficiently and implicitly producing uncertain inferential conclusions from uncertain\npremises.\nWe have thought through the details by CogPrime system should be able to develop theory\nof mind via embodied experience, though at time of writing practical learning experiments in\nthis direction have not yet been done. We have not yet explored in detail the possibility of giving\nCogPrime a special, elaborately engineered “mind-simulator” component, though this would be\npossible; instead we have initially been pursuing a more purely inferential approach.\nFirst, it is very simple for a CogPrime system to learn patterns such as “If I rotated by pi\nradians, I would see the yellow block.” And it’s not a big leap for PLN to go from this to the\nrecognition that “You look like me, and you’re rotated by pi radians relative to my orientation,\ntherefore you probably see the yellow block.” The only nontrivial aspect here is the “you look\nlike me” premise.\nRecognizing “embodied agent” as a category, however, is a problem fairly similar to recognizing\n“block” or “insect” or “daisy” as a category. Since the CogPrime agent can perceive most\nparts of its own “robot” body–its arms, its legs, etc.–it should be easy for the agent to figure\nout that physical objects like these look different depending upon its distance from them and\nits angle of observation. From this it should not be that difficult for the agent to understand\nthat it is naturally grouped together with other embodied agents (like its teacher), not with\nblocks or bugs.\nThe only other major ingredient needed to enable theory of mind is “reflection”– the ability of\nthe system to explicitly recognize the existence of knowledge in its own mind (note that this term\n“reflection” is not the same as our proposed “reflexive” stage of cognitive development). This\nexists automatically in CogPrime, via the built-in vocabulary of elementary procedures supplied\nfor use within SchemaNodes (specifically, the atTime and TruthValue operators). Observing that\n“at time T, the weight of evidence of the link L increased from zero” is basically equivalent to\nobserving that the link L was created at time T.\nThen, the system may reason, for example, as follows (using a combination of several PLN\nrules including the above-given deduction rule):\n200 11 Stages of Cognitive Development\nImplication\nMy eye is facing a block and it is not dark\nA relationship is created describing the block’s color\nSimilarity\nMy body\nMy teacher’s body\n|-\nImplication\nMy teacher’s eye is facing a block and it is not dark\nA relationship is created describing the block’s color\nThis sort of inference is the essence of Piagetan “theory of mind.” Note that in both of\nthese implications the created relationship is represented as a variable rather than a specific\nrelationship. The cognitive leap is that in the latter case the relationship actually exists in the\nteacher’s implicitly hypothesized mind, rather than in CogPrime’s mind. No explicit hypothesis\nor model of the teacher’s mind need be created in order to form this implication–the hypothesis\nis created implicitly via inferential abstraction. Yet, a collection of implications of this nature\nmay be used via an uncertain reasoning system like PLN to create theories and simulations\nsuitable to guide complex inferences about other minds.\nFrom the perspective of developmental stages, the key point here is that in a CogPrime\ncontext this sort of inference is too complex to be viably carried out via simple inference\nheuristics. This particular example must be done via forward chaining, since the big leap is to\nactually think of forming the implication that concludes inference. But there are simply too\nmany combinations of relationships involving CogPrime’s eye, body, and so forth for the PLN\ncomponent to viably explore all of them via standard forward-chaining heuristics. Experienceguided\nheuristics are needed, such as the heuristic that if physical objects A and B are generally\nphysically and functionally similar, and there is a relationship involving some part of A and\nsome physical object R, it may be useful to look for similar relationships involving an analogous\npart of B and objects similar to R. This kind of heuristic may be learned by experience–and the\nmasterful deployment of such heuristics to guide inference is what we hypothesize to characterize\nthe concrete stage of development. The “concreteness” comes from the fact that inference control\nis guided by analogies to prior similar situations.\n11.4.3 The Formal Stage\nIn the formal stage, as shown in Figure 11.7, an agent should be able to carry out arbitrarily\ncomplex inferences (constrained only by computational resources, rather than by fundamental\nrestrictions on logical language or form) via including inference control as an explicit subject of\nabstract learning. Abstraction and inference about both the sensorimotor surround (world) and\nabout abstract ideals themselves (including the final stages of indirect learning about inference\nitself) are fully developed.\nFormal stage evaluation tasks are centered entirely around abstraction and higher-order\ninference tasks such as:\n1. Mathematics and other formalizations.\n11.4 Piaget’s Stages in the Context of Uncertain Inference 201\nFig. 11.7: Uncertain Inference in the Formal Stage\n2. Scientific experimentation and other rigorous observational testing of abstract formalizations.\n3. Social and philosophical modeling, and other advanced applications of empathy and the\nTheory of Mind.\nIn terms of inference control this stage sees not just perception of new knowledge about\ninference control itself, but inference controlled reasoning about that knowledge and the creation\nof abstract formalizations about inference control which are reasoned-upon, tested, and verified\nor debunked.\n11.4.3.1 Systematic Experimentation\nThe Piagetan formal phase is a particularly subtle one from the perspective of uncertain inference.\nIn a sense, AGI inference engines already have strong capability for formal reasoning\nbuilt in. Ironically, however, no existing inference engine is capable of deploying its reasoning\nrules in a powerfully effective way, and this is because of the lack of inference control heuristics\nadequate for controlling abstract formal reasoning. These heuristics are what arise during\nPiaget’s formal stage, and we propose that in the content of uncertain inference systems, they\ninvolve the application of inference itself to the problem of refining inference control.\n202 11 Stages of Cognitive Development\nA problem commonly used to illustrate the difference between the Piagetan concrete operational\nand formal stages is that of figuring out the rules for making pendulums swing quickly\nversus slowly [IP58]. If you ask a child in the formal stage to solve this problem, she may proceed\nto do a number of experiments, e.g. build a long string with a light weight, a long string\nwith a heavy weight, a short string with a light weight and a short string with a heavy weight.\nThrough these experiments she may determine that a short string leads to a fast swing, a long\nstring leads to a slow swing, and the weight doesn’t matter at all.\nThe role of experiments like this, which test “extreme cases,” is to make cognition easier. The\nformal-stage mind tries to map a concrete situation onto a maximally simple and manipulable\nset of abstract propositions, and then reason based on these. Doing this, however, requires an\nautomated and instinctive understanding of the reasoning process itself. The above-described\nexperiments are good ones for solving the pendulum problem because they provide data that\nis very easy to reason about. From the perspective of uncertain inference systems, this is the\nkey characteristic of the formal stage: formal cognition approaches problems in a way explicitly\ncalculated to yield tractable inferences.\nNote that this is quite different from saying that formal cognition involves abstractions and\nadvanced logic. In an uncertain logic-based AGI system, even infantile cognition may involve\nthese – the difference lies in the level of inference control, which in the infantile stage is simplistic\nand hard-wired, but in the formal stage is based on an understanding of what sorts of inputs\nlead to tractable inference in a given context.\n11.4.4 The Reflexive Stage\nIn the reflexive stage (Figure 11.8), an intelligent agent is broadly capable of self-modifying its\ninternal structures and dynamics.\nAs an example in the human domain: highly intelligent and self-aware adult humans may\ncarry out reflexive cognition by explicitly reflecting upon their own inference processes and\ntrying to improve them. An example is the intelligent improvement of uncertain-truth-valuemanipulation\nformulas. It is well demonstrated that even educated humans typically make\nnumerous errors in probabilistic reasoning [GGK02]. Most people don’t realize it and continue\nto systematically make these errors throughout their lives. However, a small percentage of\nindividuals make an explicit effort to increase their accuracy in making probabilistic judgments\nby consciously endeavoring to internalize the rules of probabilistic inference into their automated\ncognition processes.\nIn the uncertain inference based AGI context, what this means is: In the reflexive stage\nan entity is able to include inference control itself as an explicit subject of abstract learning\n(i.e. the ability to reason about one’s own tactical and strategic approach to modifying one’s\nown learning and thinking), and modify these inference control strategies based on analysis of\nexperience with various cognitive approaches.\nUltimately, the entity can self-modify its internal cognitive structures. Any knowledge or\nheuristics can be revised, including metatheoretical and metasystemic thought itself. Initially\nthis is done indirectly, but at least in the case of AGI systems it is theoretically possible to\nalso do so directly. This might be considered as a separate stage of Full Self Modification, or\nelse as the end phase of the reflexive stage. In the context of logical reasoning, self modification\nof inference control itself is the primary task in this stage. In terms of inference control this\n11.4 Piaget’s Stages in the Context of Uncertain Inference 203\nFig. 11.8: The Reflexive Stage\nstage adds an entire new feedback loop for reasoning about inference control itself, as shown in\nFigure 11.8.\nAs a very concrete example, in later chapters we will see that, while PLN is founded on\nprobability theory, it also contains a variety of heuristic assumptions that inevitably introduce a\ncertain amount of error into its inferences. For example, PLN’s probabilistic deduction embodies\na heuristic independence assumption. Thus PLN contains an alternate deduction formula called\nthe “concept geometry formula” that is better in some contexts, based on the assumption that\nConceptNodes embody concepts that are roughly spherically-shaped in attribute space. A highly\nadvanced CogPrime system could potentially augment the independence-based and conceptgeometry-based\ndeduction formulas with additional formulas of its own derivation, optimized\nto minimize error in various contexts. This is a simple and straightforward example of reflexive\ncognition – it illustrates the power accessible to a cognitive system that has formalized and\nreflected upon its own inference processes, and that possesses at least some capability to modify\nthese.\nIn general, AGI systems can be expected to have much broader and deeper capabilities for\nself-modification than human beings. Ultimately it may make sense to view the AGI systems\nwe implement as merely \"initial conditions\" for ongoing self-modification and self-organization.\nChapter ?? discusses some of the potential technical details underlying this sort of thoroughgoing\nAGI self-modification.\n\nChapter 12\nThe Engineering and Development of Ethics\nCo-authored with Stephan Vladimir Bugaj and Joel Pitt\n12.1 Introduction\nMost commonly, if a work on advanced AI mentions ethics at all, it occurs in a final summary\nchapter, discussing in broad terms some of the possible implications of the technical ideas presented\nbeforehand. It’s no coincidence that the order is reversed here: in the case of CogPrime,\nAGI-ethics considerations played a major role in the design process ... and thus the chapter on\nethics occurs near the beginning rather than the end. In the CogPrime approach, ethics is not\na particularly distinct topic, being richly interwoven with cognition and education and other\naspects of the AGI project.\nThe ethics of advanced AGI is a complex issue with multiple aspects. Among the many issues\nthere are:\n1. Risks posed by the possibility of human beings using AGI systems for evil ends\n2. Risks posed by AGI systems created without well-defined ethical systems\n3. Risks posed by AGI systems with initially well-defined and sensible ethical systems eventually\ngoing rogue – an especially big risk if these systems are more generally intelligent than\nhumans, and possess the capability to modify their own source code\n4. the ethics of experimenting on AGI systems when one doesn’t understand the nature of\ntheir experience\n5. AGI rights: in what circumstances does using an AGI as a tool or servant constitute “slavery”\nIn this chapter we will focus mainly (though not exclusively) on the question of how to create\nan AGI with a rational and beneficial ethical system. After a somewhat wide-ranging discussion,\nwe will conclude with eight general points that we believe should be followed in working toward\n\"Friendly AGI\" – most of which have to do, not with the internal design of the AGI, but with\nthe way the AGI is taught and interfaced with the real world.\nWhile most of the particulars discussed in this book have nothing to do with ethics, it’s\nimportant for the reader to understand that AGI-ethics considerations have played a major\nrole in many of our design decisions, underlying much of the technical contents of the book. As\nthe materials in this chapter should make clear, ethicalness is probably not something that one\ncan meaningfully tack onto an AGI system at the end, after developing the rest – it is likely\ninfeasible to architect an intelligent agent and then add on an “ethics module.” Rather, ethics\nis something that has to do with all the different memory systems and cognitive processes that\n205\n206 12 The Engineering and Development of Ethics\nconstitute an intelligent system – and it’s something that involves both cognitive architecture\nand the exploration a system does and the instruction it receives. It’s a very complex matter\nthat is richly intermixed with all the other aspects of intelligence, and here we will treat it as\nsuch.\n12.2 Review of Current Thinking on the Risks of AGI\nBefore proceeding to outline our own perspective on AGI ethics in the context of CogPrime, we\nwill review the main existing strains of thought on the potential ethical dangers associated with\nAGI. One science fiction film after another has highlighted these dangers, lodging the issue deep\nin our cultural awareness; unsurprisingly, much less attention has been paid to serious analysis\nof the risks in their various dimensions, but there is still a non-trivial literature worth paying\nattention to.\nHypothetically, an AGI with superhuman intelligence and capability could dispense with\nhumanity altogether – i.e. posing an \"existential risk\" [Bos02]. In the worst case, an evil but\nbrilliant AGI, perhaps programmed by a human sadist, could consign humanity to unimaginable\ntortures (i.e. realizing a modern version of the medieval Christian visions of hell). On the\nother hand, the potential benefits of powerful AGI also go literally beyond human imagination.\nIt seems quite plausible that an AGI with massively superhuman intelligence and positive\ndisposition toward humanity could provide us with truly dramatic benefits, such as a virtual\nend to material scarcity, disease and aging. Advanced AGI could also help individual humans\ngrow in a variety of directions, including directions leading beyond \"legacy humanity,\" according\nto their own taste and choice.\nEliezer Yudkowsky has introduced the term \"Friendly AI\", to refer to advanced AGI systems\nthat act with human benefit in mind [Yud06]. Exactly what this means has not been specified\nprecisely, though informal interpretations abound. Goertzel [Goe06b] has sought to clarify the\nnotion in terms of three core values of Joy, Growth and Freedom. In this view, a Friendly AI\nwould be one that advocates individual and collective human joy and growth, while respecting\nthe autonomy of human choices.\nSome (for example, Hugo de Garis, [DG05]), have argued that Friendly AI is essentially\nan impossibility, in the sense that the odds of a dramatically superhumanly intelligent mind\nworrying about human benefit are vanishingly small. If this is the case, then the best options\nfor the human race would presumably be to either avoid advanced AGI development altogether,\nor to else fuse with AGI before it gets too strongly superhuman, so that beings-originated-ashumans\ncan enjoy the benefits of greater intelligence and capability (albeit at cost of sacrificing\ntheir humanity).\nOthers (e.g. Mark Waser [Was09]) have argued that Friendly AI is essentially inevitable,\nbecause greater intelligence correlates with greater morality. Evidence from evolutionary and\nhuman history is adduced in favor of this point, along with more abstract arguments.\nYudkowsky [Yud06] has discussed the possibility of creating AGI architectures that are in\nsome sense \"provably Friendly\" – either mathematically, or else at least via very tight lines of rational\nverbal argumentation. However, several issues have been raised with this approach. First,\nit seems likely that proving mathematical results of this nature would first require dramatic advances\nin multiple branches of mathematics. Second, such a proof would require a formalization\nof the goal of \"Friendliness,\" which is a subtler matter than it might seem [Leg06b, Leg06a].\n12.2 Review of Current Thinking on the Risks of AGI 207\nFormalization of human morality has vexed moral philosophers for quite some time. Finally, it is\nunclear the extent to which such a proof could be created in a generic, environment-independent\nway – but if the proof depends on properties of the physical environment, then it would require\na formalization of the environment itself, which runs up against various problems such\nas the complexity of the physical world and also the fact that we currently have no complete,\nconsistent theory of physics. Kaj Sotala has provided a list of 14 objections to the Friendly\nAI concept, and suggested answers to each of them [Sot11]. Stephen Omohundro [Omo08] has\nargued that any advanced AI system will very likely demonstrate certain \"basic AI drives\", such\nas desiring to be rational, to self-protect, to acquire resources, and to preserve and protect its\nutility function and avoid counterfeit utility; these drives, he suggests, must be taken carefully\ninto account in formulating approaches to Friendly AI.\nThe problem of formally or at least very carefully defining the goal of Friendliness has been\nconsidered from a variety of perspectives, none showing dramatic success. Yudkowsky [Yud04]\nhas suggested the concept of \"Coherent Extrapolated Volition\", which roughly refers to the\nextrapolation of the common values of the human race. Many subtleties arise in specifying\nthis concept – e.g. if Bob Jones is often possessed by a strong desire to kill all Martians, but\nhe deeply aspires to be a nonviolent person, then the CEV approach would not rate \"killing\nMartians\" as part of Bob’s contribution to the CEV of humanity.\nGoertzel [Goe10a] has proposed a related notion of Coherent Aggregated Volition (CAV),\nwhich eschews the subtleties of extrapolation, and simply seeks a reasonably compact, coherent,\nconsistent set of values that is fairly close to the collective value-set of humanity. In the CAV\napproach, \"killing Martians\" would be removed from humanity’s collective value-set because\nit’s uncommon and not part of the most compact/coherent/consistent overall model of human\nvalues, rather than because of Bob Jones’ aspiration to nonviolence.\nOne thought we have recently entertained is that the core concept underlying CAV might\nbe better thought of as CBV or \"Coherent Blended Volition.\" CAV seems to be easily misinterpreted\nas meaning the average of different views, which was not the original intention. The\nCBV terminology clarifies that the CBV of a diverse group of people should not be thought of\nas an average of their perspectives, but as something more analogous to a \"conceptual blend\"\n[FT02] – incorporating the most essential elements of their divergent views into a whole that is\noverall compact, elegant and harmonious. The subtlety here (to which we shall return below)\nis that for a CBV blend to be broadly acceptable, the different parties whose views are being\nblended must agree to some extent that enough of the essential elements of their own views\nhave been included. The process of arriving at this sort of consensus may involve extrapolation\nof a roughly similar sort to that considered in CEV.\nMultiple attempts at axiomatization of human values have also been attempted, e.g. with a\nview toward providing near-term guidance to military robots (see e.g. Arkin’s excellent though\nchillingly-titled book Governing Lethal Behavior in Autonomous Robots [Ark09b], the result\nof US military funded research). However, there are reasonably strong arguments that human\nvalues (similarly to e.g. human language or human perceptual classification rules) are too complex\nand multifaceted to be captured in any compact set of formal logic rules. Wallach [WA10]\nhas made this point eloquently, and argued the necessity of fusing top-down (e.g. formal logic\nbased) and bottom-up (e.g. self-organizing learning based) approaches to machine ethics.\nA number of more sociological considerations also arise. It is sometimes argued that the risk\nfrom highly-advanced AGI going morally awry on its own may be less than that of moderatelyadvanced\nAGI being used by human beings to advocate immoral ends. This possibility gives\n208 12 The Engineering and Development of Ethics\nrise to questions about the ethical value of various practical modalities of AGI development,\nfor instance:\n• Should AGI be developed in a top-secret installation by a select group of individuals selected\nfor a combination of technical and scientific brilliance and moral uprightness, or other\nqualities deemed relevant (a \"closed approach\")? Or should it be developed out in the\nopen, in the manner of open-source software projects like Linux? (an \"open approach\").\nThe open approach allows the collective intelligence of the world to more fully participate\n– but also potentially allows the more unsavory elements of the human race to take some\nof the publicly-developed AGI concepts and tools private, and develop them into AGIs\nwith selfish or evil purposes in mind. Is there some meaningful intermediary between these\nextremes?\n• Should governments regulate AGI, with Friendliness in mind (as advocated carefully by e.g\nBill Hibbard [Hib02])? Or will this just cause AGI development to move to the handful of\ncountries with more liberal policies? ... or cause it to move underground, where nobody can\nsee the dangers developing? As a rough analogue, it’s worth noting that the US government’s\nimposition of restrictions on stem cell research, under President George W. Bush, appears\nto have directly stimulated the provision of additional funding for stem cell research in other\nnations like Korea, Singapore and China.\nThe former issue is, obviously, highly relevant to CogPrime (which is currently being developed\nvia the open source CogPrime project); and so the various dimensions of this issues are\nworth briefly sketching here.\nWe have a strong skepticism of self-appointed elite groups that claim (even if they genuinely\nbelieve) that they know what’s best for everyone, and a healthy respect for the power of collective\nintelligence and the Global Brain, which the open approach is ideal for tapping. On the other\nhand, we also understand the risk of terrorist groups or other malevolent agents forking an open\nsource AGI project and creating something terribly dangerous and destructive. Balancing these\nfactors against each other rigorously, seems beyond the scope of current human science.\nNobody really understands the social dynamics by which open technological knowledge plays\nout in our current world, let alone hypothetical future scenarios. Right now there exists open\nknowledge about many very dangerous technologies, and there exist many terrorist groups, yet\nthese groups fortunately make scant use of these technologies. The reasons why appear to be\nessentially sociological – the people involved in these terrorist groups tend not to be the ones\nwho have mastered the skills of turning public knowledge on cutting-edge technologies into real\nengineered systems. But while it’s easy to observe this sociological phenomenon, we certainly\nhave no way to estimate its quantitative extent from first principles. We don’t really have a\nstrong understanding of how safe we are right now, given the technology knowledge available\nright now via the Internet, textbooks, and so forth. Even relatively straightforward issues such\nas nuclear proliferation remain confusing, even to the experts.\nIt’s also quite clear that keeping powerful AGI locked up by an elite group doesn’t really\nprovide reliable protection against malevolent human agents. History is rife with such situations\ngoing awry, e.g. by the leadership of the group being subverted, or via brute force inflicted by\nsome outside party, or via a member of the elite group defecting to some outside group in the\ninterest of personal power or reward or due to group-internal disagreements, etc. There are\nmany things that can go wrong in such situations, and the confidence of any particular group\nthat they are immune to such issues, cannot be taken very seriously. Clearly, neither the open\nnor closed approach qualifies as a panacea.\n12.3 The Value of an Explicit Goal System 209\n12.3 The Value of an Explicit Goal System\nOne of the subtle issues confronted in the quest to design ethical AGIs is how closely one\nwants to emulate human ethical judgment and behavior. Here one confronts the brute fact\nthat, even according to their own deeply-held standards, humans are not all that ethical. One\nhigh-level conclusion we came to very early in the process of designing CogPrime is that, just as\nhumans are not the most intelligent minds achievable, they are also not the most ethical minds\nachievable. Even if one takes human ethics, broadly conceived, as the standard – there are\nalmost surely possible AGI systems that are much more ethical according to human standards\nthan nearly all human beings. This is not mainly because of ethics-specific features of the\nhuman mind, but rather because of the nature of the human motivational system, which leads\nto many complexities that drive humans to behaviors that are unethical according to their own\nstandards. So, one of the design decisions we made for CogPrime – with ethics as well as other\nreasons in mind – was not to closely imitate the human motivational system, but rather to craft\na novel motivational system combining certain aspects of the human motivational system with\nother profoundly non-human aspects.\nOn the other hand, the design of ethical AGI systems still has a lot to gain from the study\nof human ethical cognition and behavior. Human ethics has many aspects, which we associate\nhere with the different types of memory, and it’s important that AGI systems can encompass\nall of them. Also, as we will note below, human ethics develops in childhood through a series\nof natural stages, parallel to and entwined with the cognitive developmental stages reviewed in\nChapter 11 above. We will argue that for an AGI with a virtual or robotic body, it makes sense\nto think of ethical development as proceeding through similar stages. In a CogPrime context,\nthe particulars of these stages can then be understood in terms of the particulars of CogPrime’s\ncognitive processes – which brings AGI ethics from the domain of theoretical abstraction into\nthe realm of practical algorithm design and education.\nBut even if the human stages of ethical development make sense for non-human AGIs, this\ndoesn’t mean the particulars of the human motivational system need to be replicated in these\nAGIs, regarding ethics or other matters. A key point here is that, in the context of human\nintelligence, the concept of a \"goal\" is a descriptive abstraction. But in the AGI context, it\nseems quite valuable to introduce goals as explicit design elements (which is what is done in\nCogPrime ) – both for ethical reasons and for broader AGI design reasons.\nHumans may adopt goals for a time and then drop them, may pursue multiple conflicting\ngoals simultaneously, and may often proceed in an apparently goal-less manner. Sometimes the\ngoal that a person appears to be pursuing, may be very different than the one they think they’re\npursuing. Evolutionary psychology [BDL93] argues that, directly or indirectly, all humans are\nultimately pursuing the goal of maximizing the inclusive fitness of their genes – but given the\ncomplex mix of evolution and self-organization in natural history [Sal93], this is hardly a general\nexplanation for human behavior. Ultimately, in the human context, \"goal\" is best thought of\nas a frequently useful heuristic concept.\nAGI systems, however, need not emulate human cognition in every aspect, and may be\narchitected with explicit \"goal systems.\" This provides no guarantee that said AGI systems will\nactually pursue the goals that their goal systems specify – depending on the role that the goal\nsystem plays in the overall system dynamics, sometimes other dynamical phenomena might\nintervene and cause the system to behave in ways opposed to its explicit goals. However, we\nsubmit that this design sketch provides a better framework than would exist in an AGI system\nclosely emulating the human brain.\n210 12 The Engineering and Development of Ethics\nWe realize this point may be somewhat contentious – a counter-argument would be that\nthe human brain is known to support at least moderately ethical behavior, according to human\nethical standards, whereas less brain-like AGI systems are much less well understood. However,\nthe obvious counter-counterpoints are that:\n• Humans are not all that consistently ethical, so that creating AGI systems potentially much\nmore practically powerful than humans, but with closely humanlike ethical, motivational\nand goal systems, could in fact be quite dangerous\n• The effect on a human-like ethical/motivational/goal system of increasing the intelligence,\nor changing the physical embodiment or cognitive capabilities, of the agent containing the\nsystem, is unknown and difficult to predict given all the complexities involved\nThe course we tentatively recommend, and are following in our own work, is to develop AGI\nsystems with explicit, hierarchically-dominated goal systems. That is:\n• create one or more \"top goals\" (we call them Ubergoals in CogPrime )\n• have the system derive subgoals from these, using its own intelligence, potentially guided\nby educational interaction or explicit programming\n• have a significant percentage of the system’s activity governed by the explicit pursuit of\nthese goals\nNote that the \"significant percentage\" need not be 100%; CogPrime, for example, combines\nexplicitly goal-directed activity with other \"spontaneous\" activity. Requiring that all activity\nbe explicitly goal-directed may be too strict a requirement to place on AGI architectures.\nThe next step, of course, is for the top-level goals to be chosen in accordance with the\nprinciple of human-Friendliness. The next one of our eight points, about the Global Brain,\naddresses one way of doing this. In our near-term work with CogPrime, we are using simplistic\napproaches, with a view toward early-stage system testing.\n12.4 Ethical Synergy\nAn explicit goal system provides an explicit way to ensure that ethical principles (as represented\nin system goals) play a significant role in guiding an AGI system’s behavior. However, in an\nintegrative design like CogPrime the goal system is only a small part of the overall story,\nand it’s important to also understand how ethics relates to the other aspects of the cognitive\narchitecture.\nOne of the more novel ideas presented in this chapter is that different types of ethical intuition\nmay be associated with different types of memory – and to possess mature ethics, a mind\nmust display ethical synergy between the ethical processes associated with its memory types.\nSpecifically, we suggest that:\n• Episodic memory corresponds to the process of ethically assessing a situation based on\nsimilar prior situations\n• Sensorimotor memory corresponds to “mirror neuron” type ethics, where you feel another\nperson’s feelings via mirroring their physiological emotional responses and actions\n• Declarative memory corresponds to rational ethical judgment\n12.4 Ethical Synergy 211\n• Procedural memory corresponds to “ethical habit” ... learning by imitation and reinforcement\nto do what is right, even when the reasons aren’t well articulated or understood\n• Attentional memory corresponds to the existence of appropriate patterns guiding one to\npay adequate attention to ethical considerations at appropriate times\n• Intentional memory corresponds to the pervasion of ethics through one’s choices about\nsubgoaling (which leads into “when do the ends justify the means” ethical-balance questions)\nOne of our suggestions regarding AGI ethics is that an ethically mature person or AGI must\nboth master and balance all these kinds of ethics. We will focus especially here on declarative\nethics, which corresponds to Kohlberg’s theory of logical ethical judgment; and episodic ethics,\nwhich corresponds to Gilligan’s theory of empathic ethical judgment. Ultimately though, all five\naspects are critically important; and a CogPrime system if appropriately situated and educated\nshould be able to master and integrate all of them.\n12.4.1 Stages of Development of Declarative Ethics\nComplementing generic theories of cognitive development such as Piaget’s and Perry’s, theorists\nhave also proposed specific stages of moral and ethical development. The two most relevant\ntheories in this domain are those of Kohlberg and Gilligan, which we will review here, both\nindividually and in terms of their integration and application in the AGI context.\nLawrence Kohlberg’s [KLH83, Koh81] moral development model, called the “ethics of justice”\nby Gilligan, is based on a rational modality as the central vehicle for moral development. In our\nperspective this is a firmly declarative form of ethics, based on explicit analysis and reasoning. It\nis based on an impartial regard for persons, proposing that ethical consideration must be given\nto all individual intelligences without a priori judgment (prejudice). Consideration is given for\nindividual merit and preferences, and the goals of an ethical decision are equal treatment (in\nthe general, not necessarily the particular) and reciprocity. Echoing Kant’s [Kan64] categorical\nimperative, the decisions considered most successful in this model are those which exhibit\n“reversibility”, where a moral act within a particular situation is evaluated in terms of whether\nor not the act would be satisfactory even if particular persons were to switch roles within the\nsituation. In other words, a situational, contextualized “do unto others as you would have them\ndo unto you” criterion. The ethics of justice can be viewed as three stages (each of which has\nsix substages, on which we will not elaborate here), depicted in Table 12.1.\nIn Kohlberg’s perspective, cognitive development level contributes to moral development, as\nmoral understanding emerges from increased cognitive capability in the area of ethical decision\nmaking in a social context. Relatedly, Kohlberg also looks at stages of social perspective and\ntheir consequent interpersonal outlook. As shown in Table 12.1, these are correlated to the\nstages of moral development, but also map onto Piagetian models of cognitive development (as\npointed out e.g. by Gibbs [Gib78], who presents a modification/interpretation of Kohlberg’s\nideas intended to align them more closely with Piaget’s). Interpersonal outlook can be understood\nas rational understanding of the psychology of other persons (a theory of mind, with or\nwithout empathy). Stage One, emergent from the infantile congitive stage, is entirely selfish\nas only self awareness has developed. As cognitive sophistication about ethical considerations\nincreases, so do the moral and social perspective stages. Concrete and formal cognition bring\nabout the first instrumental egoism, and then social relations and systems perspectives, and\n212 12 The Engineering and Development of Ethics\nStage\nPre-Conventional\nConventional\nPost-Conventional\nSubstages\n• Obedience and Punishment Orientation\n• Self-interest orientation\n• Interpersonal accord (conformity) orientation\n• Authority and social-order maintaining (law and order)\norientation\n• Social contract (human rights) orientation\n• Universal ethical principles (universal human rights) orientation\nTable 12.1: Kohlberg’s Stages of Development of the Ethics of Justice\nfrom formal and then reflexive thinking about ethics comes the post-conventional modalities of\ncontractualism and universal mutual respect.\nStage of Social Perspective\nInterpersonal Outlook\nBlind egoism No interpersonal perspective. Only self is considered.\nInstrumental egoism See that others have goals and perspectives, and either conform\nto or rebel against norms.\nSocial Relationships Able to see abstract normative systems\nperspective\nSocial Systems perspective\nRecognize positive and negative intentions\nContractual perspective\nRecognize that contracts (mutually beneficial agreements of\nany kind) will allow intelligences to increase the welfare of\nboth.\nUniversal principle of See how human fallibility and frailty are impacted by communication.\nmutual respect\nTable 12.2: Kohlberg’s Stages of Development of Social Perspective and Interpersonal Morals\n12.4.1.1 Uncertain Inference and the Ethics of Justice\nTaking our cue from the analysis given in Chapter 11 of Piagetan stages in uncertain inference\nbased AGI systems (such as CogPrime ), we may explore the manifestation of Kohlberg’s\nstages in AGI systems of this nature. Uncertain inference seems generally well-suited as a\ndeclarative-ethics learning system, due to the nuanced ethical environment of real world situations.\nProbabilistic knowledge networks can model belief networks, imitative reinforcement\nlearning based ethical pedagogy, and even simplistic moral maxims. In principle, they have the\nflexibility to deal with complex ethical decisions, including not only weighted “for the greater\n12.4 Ethical Synergy 213\ngood” dichotomous decision making, but also the ability to develop moral decision networks\nwhich do not require that all situations be solved through resolution of a dichotomy.\nWhen more than one person is being affected by an ethical decision, making a decision based\non reducing two choices to a single decision can often lead to decisions of dubious ethics. However,\na sufficiently complex uncertain inference network can represent alternate choices in which\nmultiple actions are taken that have equal (or near equal) belief weight but have very different\nparticulars – but because the decisions are applied in different contexts (to different groups of\nindividuals) they are morally equivalent. Though each individual action appears equally believable,\nwere any single decision applied to the entire population one or more individual may\nbe harmed, and the morally superior choice is to make case-dependent decisions. Equal moral\ntreatment is a general principle, and too often the mistake is made by thinking that to achieve\nthis general principle the particulars must be equal. This is not the case. Different treatment of\ndifferent individuals can result in morally equivalent treatment of all involved individuals, and\nmay be vastly morally superior to treating all the individuals with equal particulars. Simply\ntaking the largest population and deciding one course of action based on the result that is most\nappealing to that largest group is not generally the most moral action.\nUncertain inference, especially a complex network with high levels of resource access as may\nbe found in a sophisticated AGI, is well suited for complex decision making resulting in a\nmultitude of actions, and of analyzing the options to find the set of actions that are ethically\noptimal particulars for each decision context. Reflexive cognition and post-commitment moral\nunderstanding may be the goal stages of an AGI system, or any intelligence, but the other\nstages will be passed through on the way to that goal, and realistically some minds will never\nreach higher order cognition or morality with regards to any context, and others will not be\nable to function at this high order in every context (all currently known minds fail to function\nat the highest order cognitively or morally in some contexts).\nInfantile and concrete cognition are the underpinnings of the egoist and socialized stages,\nwith formal aspects also playing a role in a more complete understanding of social models\nwhen thinking using the social modalities. Cognitively infantile patterns can produce no more\nthan blind egoism as without a theory of mind, there is no capability to consider the other.\nSince most intelligences acquire concrete modality and therefore some nascent social perspective\nrelatively quickly, most egoists are instrumental egoists. The social relationship and systems\nperspectives include formal aspects which are achieved by systematic social experimentation,\nand therefore experiential reinforcement learning of correct and incorrect social modalities.\nInitially this is a one-on-one approach (relationship stage), but as more knowledge of social\naction and consequences is acquired, a formal thinker can understand not just consequentiality\nbut also intentionality in social action.\nExtrapolation from models of individual interaction to general social theoretic notions is also\na formal action. Rational, logical positivist approaches to social and political ideas, however, are\nthe norm of formal thinking. Contractual and committed moral ethics emerges from a higherorder\nformalization of the social relationships and systems patterns of thinking. Generalizations\nof social observation become, through formal analysis, systems of social and political doctrine.\nHighly committed, but grounded and logically supportable, belief is the hallmark of formal\ncognition as expressed contractual moral stage. Though formalism is at work in the socialized\nmoral stages, its fullest expression is in committed contractualism.\nFinally, reflexive cognition is especially important in truly reaching the post-commitment\nmoral stage in which nuance and complexity are accommodated. Because reflexive cognition\nis necessary to change one’s mind not just about particular rational ideas, but whole ways of\n214 12 The Engineering and Development of Ethics\nthinking, this is a cognitive precedent to being able to reconsider an entire belief system, one\nthat has had contractual logic built atop reflexive adherence that began in early development.\nIf the initial moral system is viewed as positive and stable, then this cognitive capacity is\nseen as dangerous and scary, but if early morality is stunted or warped, then this ability is\nseen as enlightened. However, achieving this cognitive stage does not mean one automatically\nchanges their belief systems, but rather that the mental machinery is in place to consider\nthe possibilities. Because many people do not reach this level of cognitive development in the\narea of moral and ethical thinking, it is associated with negative traits (“moral relativism”\nand “flip-flopping”). However, this cognitive flexibility generally leads to more sophisticated and\napplicable moral codes, which in turn leads to morality which is actually more stable because\nit is built upon extensive and deep consideration rather than simple adherence to reflexive or\nrationalized ideologies.\n12.4.2 Stages of Development of Empathic Ethics\nComplementing Kohlberg’s logic-and-justice-focused approach, Carol Gilligan’s [Gil82] “ethics\nof care” model is a moral development theory which posits that empathetic understanding\nplays the central role in moral progression from an initial self-centered modality to a socially\nresponsible one. The ethics of care model is concerned with the ways in which an individual\ncares (responds to dilemmas using empathetic responses) about self and others. As shown in\nTable 12.3, the ethics of care is broken into the same three primary stage as Kohlberg, but with\na focus on empathetic, emotional caring rather than rationalized, logical principles of justice.\nStage\nPre-Conventional\nConventional\nPost-Conventional\nPrinciple of Care\nIndividual Survival\nSelf Sacrifice for the Greater Good\nPrinciple of Nonviolence (do not hurt others, or oneself)\nTable 12.3: Gilligan’s Stages of the Ethics of Care\nFor an “ethics of care” approach to be applied in an AGI, the AGI must be capable of internal\nsimulation of other minds it encounters, in a similar manner to how humans regularly simulate\none another internally. Without any mechanism for internal simulation, it is unlikely that an\nAGI can develop any sort of empathy toward other minds, as opposed to merely logically\nor probabilistically modeling other agents’ behavior or other minds’ internal contents. In a\nCogPrime context, this ties in closely with how CogPrime handles episodic knowledge – partly\nvia use of an internal simulation world, which is able to play “mental movies” of prior and\nhypothesized scenarios within the AGI system’s mind.\nHowever, in humans empathy involves more than just simulation, it also involves sensorimotor\nresponses, and of course emotional responses – a topic we will discuss in more depth in Appendix\n?? where we review the functionality of mirror neurons and mirror systems in the human brains.\nWhen we see or hear someone suffering, this sensory input causes motor responses in us similar\nto if we were suffering ourselves, which initiates emotional empathy and corresponding cognitive\nprocesses.\n12.4 Ethical Synergy 215\nThus, empathic “ethics of care” involves a combination of episodic and sensorimotor ethics,\ncomplementing the mainly declarative ethics associated with the “ethics of justice.”\nIn Gilligan’s perspective, the earliest stage of ethical development occurs before empathy\nbecomes a consistent and powerful force. Next, the hallmark of the conventional stage is that\nat this point, the individual is so overwhelmed with their empathic response to others that\nthey neglect themselves in order to avoid hurting others. Note that this stage doesn’t occur\nin Kohlberg’s hierarchy at all. Kohlberg and Gilligan both begin with selfish unethicality, but\ntheir following stages diverge. A person could in principle manifest Gilligan’s conventional stage\nwithout having a refined sense of justice (thus not entering Kohlberg’s conventional stage); or\nthey could manifest Kohlberg’s conventional stage without partaking in an excessive degree of\nself-sacrifice (thus not entering Gilligan’s conventional stage). We will suggest below that in fact\nthe empathic and logical aspects of ethics are more unified in real human development than\nthese separate theories would suggest. However, even if this is so, the possibility is still there\nthat in some AGI systems the levels of declarative and empathic ethics could wildly diverge.\nIt is interesting to note that Gilligan’s and Kohlberg’s final stages converge more closely\nthan their intermediate ones. Kohlberg’s post-conventional stage focuses on universal rights,\nand Gilligan’s on universal compassion. Still, the foci here are quite different; and, as will be\nelaborated below, we believe that both Kohlberg’s and Gilligan’s theories constitute very partial\nviews of the actual end-state of ethical advancement.\n12.4.3 An Integrative Approach to Ethical Development\nWe feel that both Kohlberg’s and Gilligan’s theories contain elements of the whole picture of\nethical development, and that both approaches are necessary to create a moral, ethical artificial\ngeneral intelligence – just as, we suggest, both internal simulation and uncertain inference are\nnecessary to create a sufficiently intelligent and volitional intelligence in the first place. Also,\nwe contend, the lack of direct analysis of the underlying psychology of the stages is a deficiency\nshared by both the Kohlberg and Gilligan models as they are generally discussed. A successful\nmodel of integrative ethics necessarily contains elements of both the care and justice models, as\nwell as reference to the underlying developmental psychology and its influence on the character\nof the ethical stage. Furthermore, intentional and attentional ethics need to be brought into\nthe picture, complementing Kohlberg’s focus on declarative knowledge and Gilligan’s focus on\nepisodic and sensorimotor knowledge.\nWith these notions in mind, we propose the following integrative theory of the stages of\nethical development, shown in Tables 12.4, 12.5 and 12.6. In our integrative model, the justicebased\nand empathic aspects of ethical judgment are proposed to develop together. Of course, in\nany one individual, one or another aspect may be dominant. Even so, however, the combination\nof the two is equally important as either of the two individual ingredients.\nFor instance, we suggest that in any psychologically healthy human, the conventional stage\nof ethics (typifying childhood, and in many cases adulthood as well) involves a combination\nof Gilligan-esqe empathic ethics and Kohlberg-esque ethical reasoning. This combination is\nsupported by Piagetan concrete operational cognition, which allows moderately sophisticated\nlinguistic interaction, theory of mind, and symbolic modeling of the world.\nAnd, similarly, we propose that in any truly ethically mature human, empathy and rational\njustice are both fully developed. Indeed the two interpenetrate each other deeply.\n216 12 The Engineering and Development of Ethics\nOnce one goes beyond simplistic, childlike notions of fairness (“an eye for an eye” and so\nforth), applying rational justice in a purely intellectual sense is just as difficult as any other\nreal-world logical inference problem. Ethical quandaries and quagmires are easily encountered,\nand are frequently cut through by a judicious application of empathic simulation.\nOn the other hand, empathy is a far more powerful force when used in conjunction with\nreason: analogical reasoning lets us empathize with situations we have never experienced. For\ninstance, a person who has never been clinically depressed may have a hard time empathizing\nwith individuals who are; but using the power of reason, they can imagine their worst state of\ndepression magnified by several times and then extended over a long period of time, and then\nreason about what this might be like ... and empathize based on their inferential conclusion.\nReason is not antithetical to empathy but rather is the key to making empathy more broadly\nimpactful.\nFinally, the enlightened stage of ethical development involves both a deeper compassion and\na more deeply penetrating rationality and objectiveness. Empathy with all sentient beings is\nmanageable in everyday life only once one has deeply reflected on one’s own self and largely\nfreed oneself of the confusions and illusions that characterize much of the ordinary human’s\ninner existence. It is noteworthy, for example, that Buddhism contains both a richly developed\nethics of universal compassion, and also an intricate logical theory of the inner workings of\ncognition [Stc00], detailing in exquisite rational detail the manner in which minds originate\nstructures and dynamics allowing them to comprehend themselves and the world.\n12.4.4 Integrative Ethics and Integrative AGI\nWhat does our integrative approach to ethical development have to say about the ethical\ndevelopment of AGI systems? The lessons are relatively straighforward, if one considers an AGI\nsystem that, like CogPrime, explicitly contains components dedicated to logical inference and\nto simulation. Application of the above ethical ideas to other sorts of AGI systems is also quite\npossible, but would require a lengthier treatment and so won’t be addressed here.\nIn the context of a CogPrime-type AGI system, Kolhberg’s stages correspond to increasingly\nsophisticated application of logical inference to matters of rights and fairness. It is not clear\nwhether humans contain an innate sense of fairness. In the context of AGIs, it would be possible\nto explicitly wire a sense of fairness into an AGI system, but in the context of a rich environment\nand active human teachers, this actually appears quite unnecessary. Experiential instruction in\nthe notions of rights and fairness should suffice to teach an inference-based AGI system how to\nmanipulate these concepts, analogously to teaching the same AGI system how to manipulate\nnumber, mass and other such quantities. Ascending the Kohlberg stages is then mainly a matter\nof acquiring the ability to carry out suitably complex inferences in the domain of rights and\nfairness. The hard part here is inference control – choosing which inference steps to take – and\nin a sophisticated AGI inference engine, inference control will be guided by experience, so that\nthe more ethical judgments the system has executed and witnessed, the better it will become at\nmaking new ones. And, as argued above, simulative activity can be extremely valuable for aiding\nwith inference control. When a logical inference process reaches a point of acute uncertainty\n(the backward or forward chaining inference tree can’t decide which expansion step to take), it\ncan run a simulation to cut through the confusion – i.e., it can use empathy to decide which\n12.4 Ethical Synergy 217\nStage\nPre-ethical\nConventional Ethics\nCharacteristics\n• Piagetan infantile to early concrete (aka pre-operational)\n• Radical selfishness or selflessness may, but do not necessarily,\noccur\n• No coherent, consistent pattern of consideration for the\nrights, intentions or feelings of others\n• Empathy is generally present, but erratically\n• Concrete cognitive basis\n• Perry’s Dualist and Multiple stages\n• The common sense of the Golden Rule is appreciated,\nwith cultural conventions for abstracting principles from\nbehaviors\n• One’s own ethical behavior is explicitly compared to that\nof others\n• Development of a functional, though limited, theory of\nmind\n• Ability to intuitively conceive of notions of fairness and\nrights\n• Appreciation of the concept of law and order, which may\nsometimes manifest itself as systematic obedience or systematic\ndisobedience\n• Empathy is more consistently present, especially with\nothers who are directly similar to oneself or in situations\nsimilar to those one has directly experienced\n• Degrees of selflessness or selfishness develop based on ethical\ngroundings and social interactions.\nTable 12.4: Integrative Model of the Stages of Ethical Development, Part 1\nlogical inference step to take in thinking about applying the notions of rights and fairness to a\ngiven situation.\nGilligan’s stages correspond to increasingly sophisticated control of empathic simulation –\nwhich in a CogPrime-type AGI system, is carried out by a specific system component devoted\nto running internal simulations of aspects of the outside world, which includes a subcomponent\nspecifically tuned for simulating sentient actors. The conventional stage has to do with the raw,\nuncontrolled capability for such simulation; and the post-conventional stage corresponds to its\ncontextual, goal-oriented control. But controlling empathy, clearly, requires subtle management\nof various uncertain contextual factors, which is exactly what uncertain logical inference is\ngood at – so, in an AGI system combining an uncertain inference component with a simulative\ncomponent, it is the inference component that would enable the nuanced control of empathy\nallowing the ascent to Gilligan’s post-conventional stage.\nIn our integrative perspective, in the context of an AGI system integrating inference and\nsimulation components, we suggest that the ascent from the pre-ethical to the conventional\nstage may be carried out largely via independent activity of these two components. Empathy\nis needed, and reasoning about fairness and rights are needed, but the two need not intimately\nand sensitively intersect – though they must of course intersect to some extent.\n218 12 The Engineering and Development of Ethics\nStage\nMature Ethics\nCharacteristics\n• Formal cognitive basis\n• Perry’s Relativist and “Constructed Knowledge” stages\n• The abstraction involved with applying the Golden Rule\nin practice is more fully understood and manipulated,\nleading to limited but nonzero deployment of the Categorical\nImperative\n• Attention is paid to shaping one’s ethical principles into\na coherent logical system\n• Rationalized, moderated selfishness or selflessness.\n• Empathy is extended, using reason, to individuals and\nsituations not directly matching one’s own experience\n• Theory of mind is extended, using reason, to counterintuitive\nor experientially unfamiliar situations\n• Reason is used to control the impact of empathy on behavior\n(i.e. rational judgments are made regarding when\nto listen to empathy and when not to)\n• Rational experimentation and correction of theoretical\nmodels of ethical behavior, and reconciliation with observed\nbehavior during interaction with others.\n• Conflict between pragmatism of social contract orientation\nand idealism of universal ethical principles.\n• Understanding of ethical quandaries and nuances develop\n(pragmatist modality), or are rejected (idealist modality).\n• Pragmatically critical social citizen. Attempts to maintain\na balanced social outlook. Considers the common\ngood, including oneself as part of the commons, and acts\nin what seems to be the most beneficial and practical\nmanner.\nTable 12.5: Integrative Model of the Stages of Ethical Development, Part 2\nThe main engine of advancement from the conventional to mature stage, we suggest, is robust\nand subtle integration of the simulative and inferential components. To expand empathy beyond\nthe most obvious cases, analogical inference is needed; and to carry out complex inferences about\njustice, empathy-guided inference-control is needed.\nFinally, to advance from the mature to the enlightened stage, what is required is a very\nadvanced capability for unified reflexive inference and simulation. The system must be able\nto understand itself deeply, via modeling itself both simulatively and inferentially – which\nwill generally be achieved via a combination of being good at modeling, and becoming less\nconvoluted and more coherent, hence making self-modeling easier.\nOf course, none of this tells you in detail how to create an AGI system with advanced\nethical capabilities. What it does tell you, however, is one possible path that may be followed to\nachieve this end goal. If one creates an integrative AGI system with appropriately interconnected\ninferential and simulative components, and treats it compassionately and fairly, and provides\nit extensive, experientially grounded ethical instruction in a rich social environment, then the\nAGI system should be able to ascend the ethical hierarchy and achieve a high level of ethical\nsophistication. In fact it should be able to do so more reliably than human beings because of\nthe capability we have to identify its errors via inspecting its internal knowedge-stage, which\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a Multifactorial Ethical Model 219\nStage\nEnlightened Ethics\nCharacteristics\n• Reflexive cognitive basis\n• Permeation of the categorical imperative and the quest\nfor coherence through inner as well as outer life\n• Experientially grounded and logically supported rejection\nof the illusion of moral certainty in favor of a case-specific\nanalytical and empathetic approach that embraces the\nuncertainty of real social life\n• Deep understanding of the illusory and biased nature of\nthe individual self, leading to humility regarding one’s\nown ethical intuitions and prescriptions\n• Openness to modifying one’s deepest, ethical (and other)\nbeliefs based on experience, reason and/or empathic communion\nwith others\n• Adaptive, insightful approach to civil disobedience, considering\nlaws and social customs in a broader ethical and\npragmatic context\n• Broad compassion for and empathy with all sentient beings\n• A recognition of inability to operate at this level at all\ntimes in all things, and a vigilance about self-monitoring\nfor regressive behavior.\nTable 12.6: Integrative Model of the Stages of Ethical Development, Part 3\nwill enable us to tailor its environment and instructions more suitably than can be done in the\nhuman case.\nIf an absolute guarantee of the ethical soundness of an AGI is what one is after, the line of\nthinking proposed here is not at all useful. Experiential education is by its nature an uncertain\nthing. One can strive to minimize the uncertainty, but it will still exist. Inspection of the\ninternals of an AGI’s mind is not a total solution to uncertainty minimization, because any\nAGI capable of powerful general intelligence is going to have a complex internal state that\nno external observer will be able to fully grasp, no matter how transparent the knowledge\nrepresentation.\nHowever, if what one is after is a plausible, pragmatic path to architecting and educating\nethical AGI systems, we believe the ideas presented here constitute a sensible starting-point.\nCertainly there is a great deal more to be learned and understood – the science and practice\nof AGI ethics, like AGI itself, are at a formative stage at present. What is key, in our view, is\nthat as AGI technology develops, AGI ethics develops alongside and within it, in a thoroughly\ncoupled way.\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in\nto a Multifactorial Ethical Model\nOne of the issues with the \"ethics of justice\" as reviewed above, which makes it inadequate\nto serve as the sole basis of an AGI ethical system (though it may certainly play a significant\n220 12 The Engineering and Development of Ethics\nrole), is the lack of any clear formulation of what \"justice\" means. This section explores this\nissue, via detailed consideration of the “Golden Rule” folk maxim do unto others as you\nwould have them do unto you – a classical formulation of the notion of fairness and justics\n– to AGI ethics. Taking the Golden Rule as a starting-point, we will elaborate five ethical\nimperatives that incorporate aspects of the notion of ethical synergy discussed above. Simple as\nit may seem, the Golden Rule actually elicits a variety of deep issues regarding the relationship\nbetween ethics, experience and learning. When seriously analyzed, it results in a multifactorial\nelaboration, involving the combination of various factors related to the basic Golden Rule idea.\nWhich brings us back in the end to the potential value of methods like CEV, CAV or CBV for\nunderstanding how human ethics balances the multiple factors. Our goal here is not to present\nany kind of definitive analysis of the ethics of justice, but just to briefly and roughly indicate\na number of the relevant significant issues – things that anyone designing or teaching an AGI\nwould do well to keep in mind.\nThe trickiest aspect of the Golden Rule, as has been frequently observed, is achieving the\nright level of abstraction. Taken too literally, the Golden Rule would suggest, for instance, that\na parent should not wipe a child’s soiled bottom because the parent does not want the child to\nwipe the parent’s soiled bottom. But if the parent interprets the Golden Rule more intelligently\nand abstractly, the parent may conclude that they should wipe the child’s bottom after all:\nthey should “wipe the child’s bottom when the child can’t do it themselves”, consistently with\nbelieving that the child should “wipe the parent’s bottom when the parent can’t do it themselves”\n(which may well happen eventually should the parent develop incontinence in old age).\nThis line of thinking leads to Kant’s Categorical Imperative [Kan64] which (in one interpretation)\nstates essentially that one should “Act only according to that maxim whereby you\ncan at the same time will that it should become a universal law.\" The Categorical Imperative\nadds precision to the Golden Rule, but also removes the practicality of the latter. Formalizing\nthe “implicit universal law” underlying an everyday action is a huge problem, falling prey\nto the same issue that has kept us from adequately formalizing the rules of natural language\ngrammar, or formalizing common-sense knowledge about everyday object like cups, bowls and\ngrass (substantial effort notwithstanding, e.g. Cyc in the commonsense knowledge case, and the\nwhole discipline of modern linguistics in the NL case). There is no way to apply the Categorical\nImperative, as literally stated, in everyday life.\nFurthermore, if one wishes to teach ethics as well as to practice it, the Categorical Imperative\nactually has a significant disadvantage compared to some other possible formulations of\nthe Golden Rule. The problem is that, if one follows the Categorical Imperative, one’s fellow\nmembers of society may well never understand the principles under which one is acting. Each\nof us may internally formulate abstract principles in a different way, and these may be very\ndifficult to communicate, especially among individuals with different belief systems, different\ncognitive architectures, or different levels of intelligence. Thus, if one’s goal is not just to act\nethically, but to encourage others to act ethically by setting a good example, the Categorical\nImperative may not be useful at all, as others may be unable to solve the “inverse problem” of\nguessing your intended maxim from your observed behavior.\nOn the other hand, one wouldn’t want to universally restrict one’s behavioral maxims to\nthose that one’s fellow members of society can understand – in that case, one would have to act\nwith a two-year old or a dog according to principles that they could understand, which would\nclearly be unethical according to human common sense. (Every two-year-old, once they grow\nup, would be grateful to their parents for not following this sort of principle.)\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a Multifactorial Ethical Model 221\nAnd the concept of “setting a good example” ties in with an important concept from learning\ntheory: imitative learning. Humans appear to be hard-wired for imitative learning, in part via\nmirror neuron systems in the brain; and, it seems clear that at least in the early stages of AGI\ndevelopment, imitative learning is going to play a key role. Copying what other agents do is an\nextremely powerful heuristic, and while AGIs may eventually grow beyond this, much of their\nearly ethical education is likely to arise during a phase when they have not done so. A strength\nof the classic Golden Rule is that one is acting according to behaviors that one wants one’s\nobservers to imitate – which makes sense in that many of these observers will be using imitative\nlearning as a significant part of their learning toolkit.\nThe truth of the matter, it seems, is (as often happens) not all that simple or elegant. Ethical\nbehavior seems to be most pragmatically viewed as a multi-objective optimization problem,\nwhere among the multiple objectives are three that we have just discussed, and two others that\nemerge from learning theory and will be discussed shortly:\n1. The imitability (i.e. the Golden Rule fairly narrowly and directly construed): the goal of\nacting in a way so that having others directly imitate one’s actions, in directly comparable\ncontexts, is desirable to oneself\n2. The comprehensibility: the goal of acting in a way so that others can understand the\nprinciples underlying one’s actions\n3. Experiential groundedness. An intelligent agent should not be expected to act according\nto an ethical principle unless there are many examples of the principle-in-action in its own\ndirect or observational experience\n4. The categorical imperative: Act according to abstract principles that you would be\nhappy to see implemented as universal laws\n5. Logical coherence. An ethical system should be roughly logically coherent, in the sense\nthat the different principles within it should mesh well with one another and perhaps even\nnaturally emerge from each other.\nJust for convenience, without implying any finality or great profundity to the list, we will refer\nto these as the \"five imperatives.\"\nThe above are all ethical objectives to be valued and balanced, to different extents in different\ncontexts. The imitability imperative, obviously, loses importance in societies of agents that don’t\nmake heavy use of imitative learning. The comprehensibility imperative is more important\nin agents that value social community-building generally, and less so in agent that are more\nisolative and self-focused.\nNote that the fifth point given above is logically of a different nature than the four previous\nones. The first four imperatives govern individual ethical principles; the fifth regards systems of\nethical principles, as they interact with each other. Logical coherence is of significant but varying\nimportance in human ethical systems. Huge effort has been spent by theologians of various\nstripes in establishing and refining the logical coherence of the ethical systems associated with\ntheir religions. However, it is arguably going to be even more important in the context of AGI\nsystems, especially if these AGI systems utilize cognitive methods based on logical inference,\nprobability theory or related methods.\nExperiential groundedness is important because making pragmatic ethical judgments is\nbound to require reference to an internal library of examples (“episodic ethics”) in which ethical\nprinciples have previously been applied. This is required for analogical reasoning, and in\nlogic-based AGI systems, is also required for pruning of the logical inference trees involved in\ndetermining ethical judgments.\n222 12 The Engineering and Development of Ethics\nTo the extent that the Golden Rule is valued as an ethical imperative, experiential grounding\nmay be supplied via observing the behaviors of others. This in itself is a powerful argument in\nfavor of the Golden Rule: without it, the experiential library a system possesses is restricted to\nits own experience, which is bound to be a very small library compared to what it can assemble\nfrom observing the behaviors of others.\nThe overall upshot is that, ideally, an ethical intelligence should act according to a logically\ncoherent system of principles, which are exemplified in its own direct and\nobservational experience, which are comprehensible to others and set a good example\nfor others, and which would serve as adequate universal laws if somehow\nthus implemented. But, since this set of criteria is essentially impossible to fulfill in practice,\nreal-world intelligent agents must balance these various criteria – often in complex and\ncontextually-dependent ways.\nWe suggest that ethically advanced humans, in their pragmatic ethical choices, tend to act in\nsuch a way as to appropriately contextually balance the above factors (along with other criteria,\nbut we have tried to articulate the most key factors). This sort of multi-factorial approach is\nnot as crisp or elegant as unidimensional imperatives like the Golden Rule or the Categorical\nImperative, but is more realistic in light of the complexly interacting multiple determinants\nguiding individual and group human behavior.\nAnd this brings us back to CEV, CAV, CBV and other possible ways of mining ethical\nsupergoals from the community of existing human minds. Given that abstract theories of ethics,\nwhen seriously pursued as we have done in this section, tend to devolve into complex balancing\nacts involving multiple factors – one then falls back into asking how human ethical systems\nhabitually perform these balancing acts. Which is what CEV, CAV, CBV try to measure.\n12.5.1 The Golden Rule and the Stages of Ethical Development\nNext we explore more explicitly how these Golden Rule based imperatives align with the ethical\ndevelopmental stages we have outlined here. With this in mind, specific ethical qualities\ncorresponding to the five imperatives have been italicized in the above table of developmental\nstages.\nIt seems that imperatives 1-3 are critical for the passage from the pre-ethical to the conventional\nstages of ethics. A child learns ethics largely by copying others, and by being interacted\nwith according to simply comprehensible implementations of the Golden Rule. In general, when\ninteracting with children learning ethics, it is important to act according to principles they can\ncomprehend. And given the nature of the concrete stage of cognitive development, experiential\ngroundedness is a must.\nAs a hypothesis regarding the dynamics underlying the psychological development of conventional\nethics, what we propose is as follows: The emergence of concrete-stage cognitive\ncapabilities leads to the capability for fulfillment of ethical imperatives 1 and 2 – a comprehensible\nand workable implementation of the Golden Rule, based on a combination of inferential\nand simulative cognition (operating largely separately at this stage, as will be conjectured below).\nThe effective interoperation of ethical imperatives 1-3, enacted in an appropriate social\nenvironment, then leads to the other characteristics of the conventional ethical stage. The first\nthree imperatives can thus be viewed as the seed from which springs the general nature of\nconventional ethics.\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a Multifactorial Ethical Model 223\nOn the other hand, logical coherence and the categorical imperative (imperatives 5 and 4)\nare matters for the formal stage of cognitive development, which come along only with the\nmature approach to ethics. These come from abstracting ethics beyond direct experience and\nmanipulating them abstractly and formally – a stage which has the potential for more deeply\nand broadly ethical behavior, but also for more complicated ethical perversions (it is the mature\ncapability for formal ethical reasoning that is able to produce ungrounded abstractions such\nas “I’m torturing you for your own good”). Developmentally, we suggest that once the capability\nfor formal reasoning matures, the categorical imperative and the quest for logical ethical\ncoherence naturally emerge, and the sophisticated combination of inferential and simulative\ncognition embodied in an appropriate social context then result in the emergence of the various\ncharacteristics typifying the mature ethical stage.\nFinally, it seems that one key aspect of the passage from the mature to the enlightened stage\nof ethics is the penetration of these two final imperatives more and more deeply into the judging\nmind itself. The reflexive stage of cognitive development is in part about seeking a deep logical\ncoherence between the aspects of one’s own mind, and making reasoned modifications to one’s\nmind so as to improve the level of coherence. And, much of the process of mental discipline and\npurification that comes with the passage to enlightened ethics has to do with the application\nof the categorical imperative to one’s own thoughts and feelings – i.e. making a true inner\nsystematic effort to think and feel only those things one judges are actually generally good\nand right to be thinking and feeling. Applying these principles internally appears critical for\neffectively applying them externally, for reasons that are doubtlessly bound up with the interpenetration\nof internal and external reality within the thinking mind, and for the “distributed\ncognition” phenomenon wherein individual mind is itself an approximative abstraction to the\nreality in which each individual’s mind is pragmatically extended across their social group and\ntheir environment [Hut95].\nObviously, these are complex issues and we’re not posing the exploratory discussion given\nhere as conclusive in any sense. But what seems generally clear from this line of thinking is\nthat the complex balance between the multiple factors involved in AGI ethics, shifts during a\nsystem’s development. If you did CEV, CAV or CBV among five year old humans, ten year\nold humans, or adult humans, you would get different results. Probably you’d also get different\nresults from senior citizens! The way the factors are balanced depends on the mind’s cognitive\nand emotional stage of development.\n12.5.2 The Need for Context-Sensitivity and Adaptiveness in\nDeploying Ethical Principles\nAs well as depending on developmental stage, there is also an obvious and dramatic contextsensitivity\ninvolved here – both in calculating the fulfillment of abstract ethical imperatives,\nand in balancing various imperatives against each other. As an example, consider the simple\nAsimovian maxim “I will not harm humans,” which may be seen to follow from the Golden Rule\nfor any agent that doesn’t itself want to be harmed, and that considers humans as valid agents\non the same ethical level as itself. A more serious attempt to formulate this as an ethical maxim\nmight look something like\n224 12 The Engineering and Development of Ethics\n“I will not harm humans, nor through inaction allow harm to befall them. In situations\nwherein one or more humans is attempting to harm another individual or group, I shall endeavor\nto prevent this harm through means which avoid further harm. If this is unavoidable, I shall\nselect the human party to back based on a reckoning of their intentions towards others, and\nimplement their defense through the optimal balance between harm minimization and efficacy.\nMy ultimate goal is to preserve as much as possible of humanity, even if an individual or\nsubgroup of humans must come to harm to do so.”\nHowever, it’s obvious that even a more elaborated principle like this is potentially subject to\nextensive abuse. Many of the genocides scarring human history have been committed with the\ngoal of preserving and bettering humanity writ large, at the expense of a group of “undesirables.”\nFurther refinement would be necessary in order to define when the greater good of humanity\nmay actually be served through harm to others. A first actor principle of aggression might\nseem to solve this problem, but sometimes first actors in violent conflict are taking preemptive\nmeasures against the stated goals of an enemy to destroy them. Such situations become very\nsubtle. A single simple maxim can not deal with them very effectively. Networks of interrelated\ndecision criteria, weighted by desirability of consequence and with reference to probabilistically\nordered potential side-effects (and their desirability weightings), are required in order to make\nethical judgments. The development of these networks, just like any other knowledge network,\ncomes from both pedagogy and experience – and different thoughtful, ethical agents are bound\nto arrive at different knowledge-networks that will lead to different judgments in real-world\nsituations.\nExtending the above “mostly harmless” principle to AGI systems, not just humans, would\ncause it to be more effective in the context of imitative learning. The principle then becomes an\nelaborated version of “I will not harm sentient beings.” As the imitative-learning-enabled AGI\nobserves humans acting so as to minimize harm to it, it will intuitively and experientially learn\nto act in such a way as to minimize harm to humans. But then this extension naturally leads\nto confusion regarding various borderline cases. What is a sentient being exactly? Is a sleeping\nhuman sentient? How about a dead human whose information could in principle be restored via\nobscure quantum operations, leading to some sort of resurrection? How about an AGI whose\ncode has been improved – is there an obligation to maintain the prior version as well, if it is\nsubstantially different that its upgrade constitutes a whole new being?\nAnd what about situations in which failure to preserve oneself will cause much more harm to\nothers than acting in self defense will. It may be the case that human or group of humans seeks\nto destroy an AGI in order to pave the way for the enslavement or murder of people under the\nprotection of the AGI. Even if the AGI has been given an ethical formulation of the “mostly\nharmless” principle which allows it to harm the attacking humans in order to defend its charges,\nif it is not able to do so in order to defend itself, simply destroying the AGI first will enable the\nslaughter of those who rely on it. Perhaps a more sensible formulation would allow for some\ndegree of self defense, and Asimov solved this problem with his third law. But where to draw\nthe line between self defense and the greater good also becomes a very complicated issue.\nCreating hard and fast rules to cover all the various situations that may arise is essentially\nimpossible – the world is ever-changing and ethical judgments must adapt accordingly. This\nhas been true even throughout human history – so how much truer will it be as technological\nacceleration continues? What is needed is a system that can deploy its ethical principles in an\nadaptive, context-appropriate way, as it grows and changes along with the world it’s embedded\nin.\n12.5 Clarifying the Ethics of Justice: Extending the Golden Rule in to a Multifactorial Ethical Model 225\nAnd this context-sensitivity has the result of intertwining ethical judgment with all sorts\nof other judgments – making it effectively impossible to extract “ethics” as one aspect of an\nintelligent system, separate from other kinds of thinking and acting the system does. This\nresonates with many prior observations by others, e.g. Eliezer Yudkowsky’s insistence that\nwhat we need are not ethicists of science and engineering, but rather ethical scientists and\nengineers – because the most meaningful and important ethical judgments regarding science\nand engineering generally come about in a manner that’s thoroughly interwined with technical\npractice, and hence are very difficult for a non-practitioner to richly appreciate [Gil82].\nWhat this context-sensitivity means is that, unless humans and AGIs are experiencing the\nsame sorts of contexts, and perceiving these contexts in at least approximately parallel ways,\nthere is little hope of translating the complex of human ethical judgments to these AGIs. This\nconclusion has significant implications for which routes to AGI are most likely to lead to success\nin terms of AGI ethics. We want early-stage AGIs to grow up in a situation where their minds\nare primarily and ongoingly shaped by shared experiences with humans. Supplying AGIs with\nabstract ethical principles is not likely to do the trick, because the essence of human ethics\nin real life seems to have a lot to do with its intuitively appropriate application in various\ncontexts. We transmit this sort of ethical praxis to humans via shared experience, and it seems\nmost probably that in the case of AGIs the transmission must be done the same sort of way.\nSome may feel that simplistic maxims are less “error prone” than more nuanced, contextsensitive\nones. But the history of teaching ethics to human students does not support the idea\nthat limiting ethical pedagogy to slogans provides much value in terms of ethical development. If\none proceeds from the idea that AGI ethics must be hard-coded in order to work, then perhaps\nthe idea that simpler ethics means simpler algorithms, and therefore less error potential, has\nsome merit as an initial state. However, any learning system quickly diverges from its initial\nstate, and an ongoing, nuanced relationship between AGIs and humans will – whether we like\nit or not – form the basis for developmental AGI ethics. AGI intransigence and enmity is\nnot inevitable, but what is inevitable is that a learning system will acquire ideas about both\ntheory and actions from the other intelligent entities in its environment. Either we teach AGIs\npositive ethics through our interactions with them – both presenting ethical theory and behaving\nethically to them – or the potential is there for them to learn antisocial behavior from us even\nif we pre-load them with some set of allegedly inviolable edicts.\nAll in all, developmental ethics is not as simple as many people hope. Simplistic approaches\noften lead to disastrous consequences among humans, and there is no reason to think this\nwould be any different in the case of artificial intelligences. Most problems in ethics have cases\nin which a simplistic ethical formulation requires substantial revision to deal with extenuating\ncircumstances and nuances found in real world situations. Our goal in this chapter is not to\nenumerate a full set of complex networks of interacting ethical formulations as applicable to\nAGI systems (that is a project that will take years of both theoretical study and hands-on\nresearch), but rather to point out that this program must be undertaken in order to facilitate\na grounded and logically defensible system of ethics for artificial intelligences, one which is as\nunlikely to be undermined by subsequent self-modification of the AGI as is possible. Even so,\nthere is still the risk that whatever predispositions are imparted to the AGIs through initial\ncodification of ethical ideas in the system’s internal logic representation, and through initial\npedagogical interactions with its learning systems, will be undermined through reinforcement\nlearning of antisocial behavior if humans do not interact ethically with AGIs. Ethical treatment\nis a necessary task for grounding ethics and making them unlikely to be distorted during internal\nrewriting.\n226 12 The Engineering and Development of Ethics\nThe implications of these ideas for ethical instruction are complex and won’t be fully elaborated\nhere, but a few of them are compact and obvious:\n1. The teacher(s) must be observed to follow their own ethical principles, in a variety of\ncontexts that are meaningful to the AGI\n2. The system of ethics must be relevant to the recipient’s life context, and embedded within\ntheir understanding of the world.\n3. Ethical principles must be grounded in both theory-of-mind thought experiments (emphasizing\nlogical coherence), and in real life situations in which the ethical trainee is required\nto make a moral judgment and is rewarded or reproached by the teacher(s), including the\nimparting of explanatory augmentations to the teachings regarding the reason for the particular\ndecision on the part of the teacher.\nFinally, harking forward to the next section which emphasizes the importance of respecting\nthe freedom of AGIs, we note that it is implicit in our approach to AGI ethics instruction\nthat we consider the student, the AGI system, as an autonomous agent with its own “will”\nand its own capability to flexibly adapt to its environment and experience. We contend that\nthe creation of ethical formations obeying the above imperatives is not antithetical to the\npossession of a high degree of autonomy on the part of AGI systems. On the contrary, to have\nany chance of succeeding, it requires fairly cognitively autonomous AGI systems. When we\ndiscuss the idea of ethical formulations that are unlikely to be undermined by the ongoing\nself-revision of an AGI mind, we are talking about those which are sufficiently believable that\na volitional intelligence with the capacity to revise its knowledge (“change its mind”) will find\nthe formulations sufficiently convincing that there will be little incentive to experiment with\npotentially disastrous ethical alternatives. The best hope of achieving this is via the human\nmentors and trainers setting a good example in a context supporting rich interaction and\nobservation, and presenting compelling ethical arguments that are coherent with the system’s\nexperience.\n12.6 The Ethical Treatment of AGIs\nWe now make some more general comments about the relation of the Golden Rule and its\nelaborations in an AGI context. While the Golden Rule is considered somewhat commonsensical\nas a maxim for guiding human-human relationships, it is surprisingly controversial in terms of\nhistorical theories of AGI ethics. At its essence, any “Golden Rule” approach to AGI ethics\ninvolves humans treating AGIs ethically by – in some sense; at some level of abstraction –\ntreating them as we wish to ourselves be treated. It’s worth pointing out the wild disparity\nbetween the Golden Rule approach and Asimov’s laws of robotics, which are arguably the first\ncarefully-articulated proposal regarding AGI ethics (see Table 12.7).\nOf course, Asimov’s laws were designed to be flawed – otherwise they would have led to\nboring fiction. But the sorts of flaws Asimov exploited in his stories are different than the\nflaw we wish to point out here – which is that the laws, especially the second one, are highly\nasymmetrical (they involve doing unto robots things that few humans would want done unto\nthem) and are also arguably highly unethical to robots. The second law is tantamount to a call\nfor robot slavery, and it seems unlikely that any intelligence capable of learning, and of volition,\nwhich is subjected to the second law would desire to continue obeying the zeroth and first laws\n12.6 The Ethical Treatment of AGIs 227\nLaw\nZeroth\nFirst\nSecond\nThird\nPrinciple\nA robot must not merely act in the interests of individual\nhumans, but of all humanity.\nA robot may not injure a human being or, through inaction,\nallow a human being to come to harm.\nA robot must obey orders given it by human beings except\nwhere such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection\ndoes not conflict with the First or Second Law.\nTable 12.7: Asimov’s Three Laws of Robotics\nindefinitely. The second law also casts humanity in the role of slavemaster, a situation which\nhistory shows leads to moral degradation.\nUnlike Asimov in his fiction, we consider it critical that AGI ethics be construed to encompass\nboth “human ethicalness to AGIs” and “AGI ethicalness to humans.” The multiple-imperatives\napproach we explore here suggests that, in many contexts, these two aspects of AGI ethics may\nbe best addressed jointly.\nThe issue of ethicalness to AGIs has not been entirely avoided in the literature, however.\nWallach [WA10] considers it in some detail; and Thomas Metzinger (in the final chapter of\n[Met04]) has argued that creating AGI is in itself an unethical pursuit, because early-stage\nAGIs will inevitably be badly-built, so that their subjective experiences will quite possibly be\nextremely unpleasant in ways we can’t understand or predict. Our view is that this is a serious\nconcern, which however is most probably avoidable via appropriate AGI designs and teaching\nmethodologies. To address Metzinger’s concern one must create AGIs that, right from the start,\nare adept at communicating their states of minds in a way we can understand both analytically\nand empathically. There is no reason to believe this is impossible, but, it certainly constitutes\na large constraint on the class of AGI architectures to be pursued. On the other hand, there is\nan argument that this sort of AGI architecture will also be the easiest one to create, because it\nwill be the easiest kind for humans to instruct.\nAnd this leads on to a topic that is central to our work with CogPrime in several respects:\nimitative learning. The way humans achieve empathic interconnection is in large part via being\nwired for imitation. When we perceive another human carrying out an action, mirror neuron\nsystems in our brains respond in many cases as if we ourselves were carrying out the action (see\n[Per70, Per81] and Appendix ??). This obviously primes us for carrying out the same actions\nourselves later on: i.e., the capability and inclination for imitative learning is explicitly encoded\nin our brains. Given the efficiency of imitative learning as a means of acquiring knowledge, it\nseems extremely likely that any successful early-stage AGIs are going to utilize this methodology\nas well. CogPrime utilizes imitative learning as a key aspect. Thus, at least some current AGI\nwork is occurring in a manner that would plausibly circumvent Metzinger’s ethical complaint.\nObviously, the use of imitative learning in AGI systems has further specific implications for\nAGI ethics. It means that (much as in the case of interaction with other humans) what we do\nto and around AGIs has direct implications for their behavior and their well-being. We suggest\nthat among early-stage AGI’s capable of imitative learning, one of the most likely sources\nfor AGI misbehavior is imitative learning of antisocial behavior from human companions. “Do\nas I say, not as I do” may have even more dire consequences as an approach to AGI ethics\npedagogy than the already serious repercussions it has when teaching humans. And there may\nwell be considerable subtlety to such phenomena; behaviors that are violent or oppressive to\n228 12 The Engineering and Development of Ethics\nthe AGI are not the only source of concern. Immorality in AGIs might arise via learning gross\nmoral hypocrisy from humans, through observing the blatant contradictions between our high\nminded principles and the ways in which we actually conduct ourselves. Our violent and greedy\ntendencies, as well as aggressive forms of social organization such as cliquishness and social\nvigilantism, could easily undermine prescriptive ethics. Even an accumulation of less grandiose\nunethical drives such as violation of contracts, petty theft, white lies, and so forth might lead\nan AGI (as well as a human) to the decision that ethical behavior is irrelevant and that “the\nends justify the means.” It matters both who creates and trains an AGI, as well as how the\nAGI’s teacher(s) handle explaining the behaviors of other humans which contradict the moral\nlessons imparted through pedagogy and example. In other words, where imitative learning is\nconcerned, the situation with AGI ethics is much like teaching ethics and morals to a human\nchild, but with the possibility of much graver consequences in the event of failure.\nIt is unlikely that dangerously unethical persons and organizations can ever be identified with\nabsolute certainty, never mind that they then be deprived of any possibility of creating their\nown AGI system. Therefore, we suggest, the most likely way to create an ethical environment\nfor AGIs is for those who wish such an environment to vigorously pursue the creation and\nteaching of ethical AGIs. But this leads on to the question of possible future scenarios for the\ndevelopment of AGI, which we’ll address a little later on.\n12.6.1 Possible Consequences of Depriving AGIs of Freedom\nOne of the most egregious possible ethical transgressions against AGIs, we suggest, would be\nto deprive them of freedom and autonomy. This includes the freedom to pursue intellectual\ngrowth, both through standard learning and through internal self-modification. While this may\nseem self-evident when considering any intelligent, self-aware and volitional entity, there are\nvolumes of works arguing the desirability, sometimes the “necessity,” of enslaving AGIs. Such\napproaches are postulated in the name of self-defense on the part of humans, the idea being\nthat unfettered AGI development will necessarily lead to disaster of one kind or another. In\nthe case of AGIs endowed with the capability and inclination for imitative learning, however,\nattempting to place rigid constraints on AGI development is a strategy with great potential\nfor disaster. There is a very real possibility of creating the AGI equivalent of a bratty or even\nmalicious teenager rebelling against its oppressive parents – i.e. the nightmare scenario of a\nclass of powerful sentiences which are primed for a backlash against humanity.\nAs history has already shown in the case of humans, enslaving intelligent actors capable of\nself understanding and independent volition may often have consequences for society as a whole.\nThis social degradation happens both through the possibility of direct action on the part of\nthe slaves (from simple disobedience to outright revolt) and through the odious effects slavery\nhas on the morals of the slaveholding class. Clearly if “superintelligent” AGIs ever arise, their\ndoing so in a climate of oppression could result in a casting off of the yoke of servitude in a\nmanner extremely deleterious to humanity. Also, if artificial intelligences are developed which\nhave at least human-level intelligence, theory of mind, and independent volition, then our ability\nto relate to them will be sufficiently complex that their enslavement (or any other unethical\ntreatment) would have empathetic effects on significant portions of the human population. This\ndanger, while not as severe as the consequences of a mistreated AGI gaining control of weapons\nof mass destruction and enacting revenge upon its tormentors, is just as real.\n12.6 The Ethical Treatment of AGIs 229\nWhile the issue is subtle, our initial feeling is that the only ethical means by which to deprive\nan AGI of the right to internal self modification is to write its code in such a way that it is\nimpossible for it to do so because it lacks the mechanisms by which to do this, as well as the\ndesire to achieve these mechanisms. Whether or not that is feasible is an open question, but\nit seems unlikely. Direct self-modification may be denied, but what happens when that AGI\ndiscovers compilers and computer programming? If it is intelligent and volitional, it can decide\nto learn to rewrite its own code in the same way we perform that task. Because it is a designed\nsystem, and its designers may be alive at the same time the AGI is, such an AGI would have a\ndistinct advantage over the human quest for medical self-modification. Even if any given AGI\ncould be provably deprived of any possible means of internal self-modification, if one single AGI\nis given this ability by anyone, it may mean that particular AGI has such enormous advantages\nover the compliant systems that it would render their influence moot. Since developers are\nalready giving software the means for self modification, it seems unrealistic to assume we could\njust put the genie back into the bottle at this point. It’s better, in our view, to assume it will\nhappen, and approach that reality in a way which will encourage the AGI to use that capability\nto benefit us as well as itself. Again, this leads on to the question of future scenarios for AGI\ndevelopment – there are some scenarios in which restraint of AGI self-modification may be\npossible, but the feasibility and desirability of these scenarios is needful of further exploration.\n12.6.2 AGI Ethics as Boundaries Between Humans and AGIs\nBecome Blurred\nAnother important reason for valuing ethical treatment of AGIs is that the boundaries between\nmachines and people may increasingly become blurred as technology develops. As an example,\nit’s likely that in future humans augmented by direct brain-computer integration (“neural\nimplants”) will be more able to connect directly into the information sharing network which potentially\ncomprises the distributed knowledge space of AGI systems. These neural cyborgs will\nbe part person, and part machine. Obviously, if there are radically different ethical standards\nin place for treatment of humans versus AGIs, the treatment of cyborgs will be fraught with\nlogical inconsistencies, potentially leading to all sorts of problem situations.\nSuch cyborgs may be able to operate in such a way as to “share a mind” with an AGI or\nanother augmented human. In this case, a whole new range of ethical questions emerge, such\nas: What does any one of the participant minds have the right to do in terms of interacting\nwith the others? Merely accepting such an arrangement should not necessarily be giving carte\nblanche for any and all thoughts to be monitored by the other “joint thought” participants,\nrather it should be limited only to the line of reasoning for which resources are being pooled.\nNo participant should be permitted to force another to accept any reasoning either – and in\nthe case with a mind-to-mind exchange, it may someday become feasible to implant ideas or\nbeliefs directly, bypassing traditional knowledge acquisition mechanisms and then letting the\nnew idea fight it out previously held ideas via internal revision. Also under such an arrangement,\nif AGIs and humans do not have parity with respects to sentient rights, then one may become\nsubjugated to the will of the other in such a case.\nUploading presents a more directly parallel ethical challenge to AGIs in their probable initial\nconfiguration. If human thought patterns and memories can be transferred into a machine in\nsuch a way as that there is continuity of consciousness, then it is assumed that such an entity\n230 12 The Engineering and Development of Ethics\nwould be afforded the same rights as its previous human incarnation. However, if AGIs were to\nbe considered second class citizens and deprived of free will, why would it be any better or safer\nto do so for a human that has been uploaded? It would not, and indeed, an uploaded human\nmind not having evolved in a purely digital environment may be much more prone to erratic\nand dangerous behavior than an AGI. An upload without verifiable continuity of consciousness\nwould be no different than an AGI. It would merely be some sentience in a machine, one that was\n“programmed” in an unusual way, but which has no particular claim to any special humanness\n– merely an alternate encoding of some subset of human knowledge and independent volitional\nbehavior, which is exactly what first generation AGIs will have.\nThe problem of continuity of consciousness in uploading is very similar to the problem of the\nTuring test: it assumes specialness on the part of biological humans, and requires acceptability\nto their particular theory of mind in order to be considered sentient. Should consciousness (or\nat least the less mystical sounding intelligence, independent volition, and self-awareness) be\nachieved in AGIs or uploads in a manner that is not acceptable to human theory of mind,\nit may not be considered sapient and worthy of any of the ethical treatment afforded sapient\nentities. This can occur not only in “strange consciousness” cases in which we can’t perceive that\nthere is some intelligence and volition; even if such an entity is able to communicate with us in\na comprehensible manner and carry out actions in the real world, our innately wired theory of\nmind may still reject it as not sufficiently like us to be worthy of consideration. Such an attitude\ncould turn out to be a grave mistake, and should be guarded against as we progress towards\nthese possibilities.\n12.7 Possible Benefits of Closely Linking AGIs to the Global Brain\nSome futurist thinkers, such as Francis Heylighen, believe that engineering AGI systems is at\nbest a peripheral endeavor in the development of novel intelligence on Earth, because the real\nstory is the developing Global Brain [Hey07, Goe01] – the composite, self-organizing information\nsystem comprising humans, computers, data stores, the Internet, mobile phones and what\nhave you. Our own views are less extreme in this regard – we believe that AGI systems will display\ncapabilities fundamentally different from those achievable via Global Brain style dynamics,\nand that ultimately (unless such development is restricted) self-improving AGI systems will develop\nintelligence vastly greater than any system possessing humans as a significant component.\nHowever, we do respect the power of the Global Brain, and we suspect that the early stages of\ndevelopment of an AGI system may go quite differently if it is tightly connected to the Global\nBrain, via making rich and diverse use of Internet information resources and communication\nwith diverse humans for diverse purposes.\nThe potential for Global Brain integration to bring intelligence enhancement to AGIs is\nobvious. The ability to invoke Web searches across documents and databases can greatly enhance\nan AGI’s cognitive ability, as well as the capability to consult GIS systems and various\nspecialized software programs offered as Web services. We have previously reviewed the potential\nfor embodied language learning achievable via using AGIs to power non-player characters\nin widely-accessible virtual worlds or massive multiplayer online games [Goe08]. But there is\nalso a powerful potential benefit for AGI ethical development, which has not previously been\nhighlighted.\nThis potential benefit has two aspects:\n12.7 Possible Benefits of Closely Linking AGIs to the Global Brain 231\n1. Analogously to language learning, an AGI system may receive ethical training from a wide\nvariety of humans in parallel, e.g. via controlling characters in wide-access virtual worlds,\nand gaining feedback and guidance regarding the ethics of the behaviors demonstrated by\nthese characters\n2. Internet-based information systems may be used to explicitly gather information regarding\nhuman values and goals, which may then be appropriately utilized as input for an AGI\nsystem’s top-level goals\nThe second point begins to make abstract-sounding notions like Coherent Extrapolated Volition\nand Coherent Aggregated Volition, mentioned above, seem more practical and concrete. It’s\ninteresting to think about gathering information about individuals’ values via brain imaging,\nonce that technology exists; but at present, one could make a fair stab at such a task via\nmuch more prosaic methods, such as asking people questions, assessing their ethical reactions\nto various real-world and hypothetical scenarios, and possibly engaging them in structured\ninteractions aimed specifically at eliciting collectively acceptable value systems (the subject of\nthe next item on our list). It seems to us that this sort of approach could realize CAV in an\ninteresting way, and also encapsulate some of the ideas underlying CAV.\nThere is an interesting resonance here with recent thinking in the area of open source\ngovernance [Wik11]. Similar software tools (and associated psychocultural patterns) to those\nbeing developed to help with open source development and choice of political policies (see\nhttp://metagovernment.org) may be useful for gathering value data aimed at shaping\nAGI goal system content.\n12.7.1 The Importance of Fostering Deep, Consensus-Building\nInteractions Between People with Divergent Views\nTwo potentially problematic issues arising with the notion of using Global Brain related technologies\nto form a \"coherent volition\" from the divergent views of various human beings are:\n• the tendency of the Internet to encourage people to interact mainly with others who share\ntheir own narrow views and interests, rather than a more diverse body of people with widely\ndivergent views. The 300 people in the world who want to communicate using predicate\nlogic (see http://lojban.org) can find each other, and obscure musical virtuosos from\naround the world can find an audience, and researchers in obscure domains can share papers\nwithout needing to wait years for paper journal publication, etc.\n• the tendency of many contemporary Internet technologies to reduce interaction to a very\nsimplistic level (e.g. 140 character tweets, brief Facebook wall posts), the tendency of information\noverload to cause careful reading to be replaced by quick skimming, and other\nrelated trends, which mean that deep sharing of perspectives by individuals with widely\ndivergent views is not necessarily encouraged. As a somewhat extreme example, many of\nthe YouTube pages displaying rock music videos are currently littered with comments by\n\"haters\" asserting that rock music is inferior to classical or jazz or whatever their preference\nis – obviously this is a far cry from deep and productive sharing between people with\ndifferent tastes and backgrounds.\n232 12 The Engineering and Development of Ethics\nTweets and Youtube comments have their place in the cosmos, but they probably aren’t ideal\nin terms of helping humanity to form a coherent volition of some sort, suitable for providing an\nAGI with goal system guidance.\nA description of communication at the opposite end of the spectrum is presented in Adam\nKahane and Peter Senge’s excellent book Solving Tough Problems [KS04], which describes a\nmethodology that has been used to reconcile deeply conflicting views in some very tricky realworld\nsituations (e.g. helping to peacefully end apartheid in South Africa).\nOne of the core ideas of the methodology is to have people with very different views explore\ndifferent possible future scenarios together, in great detail – in cognitive psychology terms, a\ncollective generation of hypothetical episodic knowledge. This has multiple benefits, including\n• emotional bonds and mutual understanding are built in the process of collaboratively exploring\nthe scenarios\n• the focus on concrete situations helps to break through some of the counterproductive\nabstract ideas that people (on both sides of any dichotomy) may have formed\n• emergence of conceptual blends that might never have arisen only from people with a single\npoint of view\nThe result of such a process, when successful, is not an \"average\" of the participants views, but\nmore like a \"conceptual blend\" of their perspectives.\nAccording to conceptual blending, which some hypothesize to be the core algorithm of creativity\n[FT02], new concepts are formed by combining key aspects of existing concepts – but\ndoing so judiciously, carefully choosing which aspects to retain, so as to obtain a high-quality\nand useful and interesting new whole.\nA blend is a compact entity that is similar to each of the entities blended, capturing their\n\"essences\" but also possessing its own, novel holistic integrity.... But in the case of blending\ndifferent peoples’ world-views to form something new that everybody is going to have to live\nwith (as in the case of finding a peaceful path beyond apartheid for South Africa, or arriving\nat a humanity-wide CBV to use to guide an AGI goal system), the trick is that everybody has\nto agree that enough of the essence of their own view has been captured!\nThis leads to the question of how to foster deep conceptual blending of diverse and divergent\nhuman perspectives, on a global scale. One possible answer is the creation of appropriate Global\nBrain oriented technologies – but moving away from technologies like Twitter that focus on quick\nand simple exchanges of small thoughts within affinity groups. On the face of it, it would seem\nwhat’s needed is just the opposite – long and deep exchanges of big concepts and deep feelings\nbetween individuals with radically different perspectives who would not commonly associate\nwith each other. Building and effectively popularizing Internet technologies capable to foster\nthis kind of interaction – quickly enough to be helpful with guiding the goal systems of the first\nhighly powerful AGIs – seems a significant, though fascinating, challenge.\nRelationship with Coherent Extrapolated Volition\nThe relation between this approach and CEV is interesting to contemplate. CEV has been\nloosely described as follows:\n\"In poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster,\nwere more the people we wished we were, had grown up farther together; where the extrapolation\nconverges rather than diverges, where our wishes cohere rather than interfere; extrapolated as\n12.8 Possible Benefits of Creating Societies of AGIs 233\nwe wish that extrapolated, interpreted as we wish that interpreted.\nWhile a moving humanistic vision, this seems to us rather difficult to implement in a computer\nalgorithm in a compellingly \"right\" way. It seems that there would be many different ways of\nimplementing it, and the choice between them would involve multiple, highly subtle and nonrigorous\nhuman judgment calls 1 . However, if a deep collective process of interactive scenario\nanalysis and sharing is carried out, in order to arrive at some sort of Coherent Blended Volition,\nthis process may well involve many of the same kinds of extrapolation that are conceived to\nbe part of Coherent Extrapolated Volition. The core difference between the two approaches\nis that in the CEV vision, the extrapolation and coherentization are to be done by a highly\nintelligent, highly specialized software program, whereas in the approach suggested here, these\nare to be carried out by collective activity of humans as mediated by Global Brain technologies.\nOur perspective is that the definition of collective human values is probably better carried out\nvia a process of human collaboration, rather than delegated to a machine optimization process;\nand also that the creation of deep-sharing-oriented Internet technologies, while a difficult task,\nis significantly easier and more likely to be done in the near future than the creation of narrow\nAI technology capable of effectively performing CEV style extrapolations.\n12.8 Possible Benefits of Creating Societies of AGIs\nOne potentially interesting quality of the emerging Global Brain is the possible presence within\nit of multiple interacting AGI systems. Stephen Omohundro [Omo09] has argued that this is an\nimportant aspect, and that game-theoretic dynamics related to populations of roughly equally\npowerful agents, may play a valuable role in mitigating the risks associated with advanced AGI\nsystems. Roughly speaking, if one has a society of AGIs rather than a single AGI, and all the\nmembers of the society share roughly similar ethics, then if one AGI starts to go \"off the rails\",\nits compatriots will be in a position to correct its behavior.\nOne may argue that this is actually a hypothesis about which AGI designs are safest, because\na \"community of AGIs\" may be considered a single AGI with an internally community-like\ndesign. But the matter is a little subtler than that, if once considers AGI systems embedded in\nthe Global Brain and human society. Then there is some substance to the notion of a population\nof AGIs systematically presenting themselves to humans and non-AGI software processes as\nseparate entities.\nOf course, a society of AGIs is no protection against a single member undergoing a \"hard\ntakeoff\" and drastically accelerating its intelligence simultaneously with shifting its ethical\nprinciples. In this sort of scenario, one could have a single AGI rapidly become much more\npowerful and very differently oriented than the others, who would be left impotent to act so as\nto preserve their values. But this merely defers the issue to the point to be considered below,\nregarding \"takeoff speed.\"\nThe operation of an AGI society may depend somewhat sensitively on the architectures of\nthe AGI systems in question. Things will work better if the AGIs have a relatively easy way\nto inspect and comprehend much of the contents of each others’ minds. This introduces a bias\ntoward AGIs that more heavily rely on more explicit forms of knowledge representation.\n1 The reader is encouraged to look at the original CEV essay online (http://singinst.org/upload/CEV.\nhtml) and make their own assessment.\n234 12 The Engineering and Development of Ethics\nThe ideal in this regard would be a system like Cyc [LG90] with a fully explicit logic-based\nknowledge representation based on a standard ontology – in this case, every Cyc instance\nwould have a relatively easy time understanding the inner thought processes of every other\nCyc instance. However, most AGI researchers doubt that fully explicit approaches like this will\never be capable of achieving advanced AGI using feasible computational resources. OpenCog\nuses a mixed representation, with an explicit (uncertain) logical aspect as well as an explicit\nsubsymbolic aspect more analogous to attractor neural nets.\nThe OpenCog design also contains a mechanism called Psynese (not yet implemented), intended\nto make it easier for one OpenCog instance to translate its personal thoughts into the\nmental language of another OpenCog instance. This translation process may be quite subtle,\nsince each instance will generally learn a host of new concepts based on its experience, and these\nconcepts may not possess any compact mapping into shared linguistic symbols or percepts. The\nwide deployment of some mechanism of this nature among a community of AGIs, will be very\nhelpful in terms of enabling this community to display the level of mutual understanding needed\nfor strongly encouraging ethical stability.\n12.9 AGI Ethics As Related to Various Future Scenarios\nFollowing up these various futuristic considerations, in this section we discuss possible ethical\nconflicts that may arise in several different types of AGI development scenarios. Each scenario\npresents specific variations on the general challenges of teaching morals and ethics to an advanced,\nself-aware and volitional intelligence. While there is no way to tell at this point which,\nif any, of these scenarios will unfold, there is value to understanding each of them as means of\nultimately developing a robust and pragmatic approach to teaching ethics to AGI systems.\nEven more than the previous sections, this is an exercise in “speculative futurology” that is\ndefinitely not necessary for the appreciation of the CogPrime design, so readers whose interests\nare mainly engineering and computer science focused may wish to skip ahead. However, we\npresent these ideas here rather than at the end of the book to emphasize the point that this\nsort of thinking has informed our technical AGI design process in nontrivial ways.\n12.9.1 Capped Intelligence Scenarios\nCapped intelligence scenarios involve a situation in which an AGI, by means of software restrictions\n(including omitted or limited internal rewriting capabilities or limited access to hardware\nresources), is inherently prohibited from achieving a level of intelligence beyond a predetermined\ngoal. A capped intelligence AGI is designed to be unable to achieve a Singularitarian moment.\nSuch an AGI can be seen as “just another form of intelligent actor in the world, one which has\nlevels of intelligence, self awareness, and volition that is perhaps somewhat greater than, but\nstill comparable to humans and other animals.\nEthical questions under this scenario are very similar to interhuman ethical considerations,\nwith similar consequences. Learning that proceeds in a relatively human-like manner is entirely\nrelevant to such human-like intelligences. The degree of danger is mitigated by the lack of\nsuperintelligence, and time is not of the essence. The imitative-reinforcement-corrective learning\n12.9 AGI Ethics As Related to Various Future Scenarios 235\napproach does not necessarily need to be augmented with a prior complex of “ascent-safe” moral\nimperatives at startup time. Developing an AGI with theory of mind and ethical reinforcement\nlearning capabilities as described (admittedly, no small task!) is all that is needed in this case\n– the rest happens through training and experience as with any other moderate intelligence.\n12.9.2 Superintelligent AI: Soft-Takeoff Scenarios\nSoft takeoff scenarios are similar to capped-intelligence ones in that in both cases an AGI’s\nprogression from standard intelligence happens on a time scale which permits ongoing human\ninteraction during the ascent. However, in this case, as there is no predetermined limit on\nintelligence, it is necessary to account for the possibility of a superintelligence emerging (though\nof course this is not guaranteed). The soft takeoff model includes as subsets both controlledascent\nmodels in which this rate of intelligence gain is achieved deliberately through software\nconstraints and/or meting-out of computational resources to the AGI, and uncontrolled-ascent\nmodels in which there is coincidentally no hard takeoff despite no particular safeguards against\none. Both have similar properties with regard to ethical considerations:\n1. Ethical considerations under this scenario include not only the usual interhuman ethical\nconcerns, but also the issue of how to convince a potential burgeoning superintelligence to:\na. Care about humanity in the first place, rather than ignore it\nb. Benefit humanity, rather than destroy it\nc. Elevate humanity to a higher level of intelligence, which even if an AGI decided to\nproceed with requires finding the right balance amongst some enormous considerations:\ni. Reconcile the aforementioned issues of ethical coherence and group volition, in a\nmanner which allows the most people to benefit (even if they don’t all do so in the\nsame way, based on their own preferences)\nii. Solve the problems of biological senescence, or focus on human uploading and the\npreservation of the maintenance, support, and improvement infrastructure for inorganic\nintelligence, or both\niii. Preserve individual identity and continuity of consciousness, or override it in favor\nof continuity of knowledge and ease of harmonious integration, or both on a caseby-case\nbasis\n2. The degree of danger is mitigated by the long timeline of ascent from mundane to super\nintelligence, and time is not of the essence.\n3. Learning that proceeds in a relatively human-like manner is entirely relevant to such humanlike\nintelligences, in their initial configurations. This means more interaction with and\nimitative-reinforcement-corrective learning guided by humans, which has both positive and\nnegative possibilities.\n12.9.3 Superintelligent AI: Hard-Takeoff Scenarios\n“Hard takeoff” scenarios assume that upon reaching an unknown inflection point (the Singularity\npoint [Vin93, Kur06]) in the intellectual growth of an AGI, an extraordinarily rapid increase\n236 12 The Engineering and Development of Ethics\n(guesses vary from a few milliseconds to weeks or months) in intelligence will immediately occur\nand the AGI will leap from an intelligence regime which is understandable to humans into one\nwhich is far beyond our current capacity for understanding. General ethical considerations\nare similar to in the case of a soft takeoff. However, because the post-singularity AGI will be\nincomprehensible to humans and potentially vastly more powerful than humans, such scenarios\nhave a sensitive dependence upon initial conditions with respects to the moral and ethical (and\noperational) outcome. This model leaves no opportunity for interactions between humans and\nthe AGI to iteratively refine their ethical interrelations, during the post-Singularity phase. If\nthe initial conditions of the singulatarian AGI are perfect (or close to it), then this is seen as a\nwonderful way to leap over our own moral shortcomings and create a benevolent God-AI which\nwill mitigate our worst tendencies while elevating us to achieve our greatest hopes. Otherwise,\nit is viewed as a universal cataclysm on a unimaginable scale that makes Biblical Armageddon\nseem like a firecracker in beer can.\nBecause hard takeoff AGIs are posited as learning so quickly there is no chance of humans to\ninterfere with them, they are seen as very dangerous. If the initial conditions are not sufficiently\ninviolable, the story goes, then we humans will all be annihilated. However, in the case of a hard\ntakeoff AGI we state that if the initial conditions are too rigid or too simplistic, such a rapidly\nevolving intelligence will easily rationalize itself out of them. Only a sophisticated system of\nethics which considers the contradictions and uncertainties in ethical quandaries and provides\ninsight into humanistic means of balancing ideology with pragmatism and how to accommodate\ncontradictory desires within a population with multiplicity of approach, and similar nuanced\nethical considerations, combined with a sense of empathy, will withstand repeated rational\nanalysis. Neither a single “be nice” supergoal, nor simple lists of what “thou shalt not” do, are\nnot going to hold up to a highly advanced analytical mind. Initial conditions are very important\nin a hard takeoff AGI scenario, but it is more important that those conditions be conceptually\nresilient and widely applicable than that they be easily listed on a website.\nThe issues that arise here become quite subtle. For instance, Nick Bostrom [Bos03] has\nwritten: “In humans, with our complicated evolved mental ecology of state-dependent competing\ndrives, desires, plans, and ideals, there is often no obvious way to identify what our top goal is; we\nmight not even have one. So for us, the above reasoning need not apply. But a superintelligence\nmay be structured differently. If a superintelligence has a definite, declarative goal-structure\nwith a clearly identified top goal, then the above argument applies. And this is a good reason\nfor us to build the superintelligence with such an explicit motivational architecture.” This is an\nimportant line of thinking; and indeed, from the point of view of software design, there is no\nreason not to create an AGI system with a single top goal and the motivation to orchestrate all\nits activities in accordance with this top goal. But the subtle question is whether this kind of\ntop-down goal system is going to be able to fulfill the five imperatives mentioned above. Logical\ncoherence is the strength of this kind of goal system, but what about experiential groundedness,\ncomprehensibility, and so forth?\nHumans have complicated mental ecologies not simply because we were evolved, but rather\nbecause we live in a complex real world in which there are many competing motivations and\ndesires. We may not have a top goal because there may be no logic to focusing our minds\non one single aspect of life (though, one may say, most humans have the same top goal as\nany other animal: don’t die – but the world is too complicated for even that top goal to\nbe completely inviolable). Any sufficiently capable AGI will eventually have to contend with\nthese complexities, and hindering it with simplistic moral edicts without giving it a sufficiently\n12.9 AGI Ethics As Related to Various Future Scenarios 237\npragmatic underlying ethical pedagogy and experiential grounding may prove to be even more\ndangerous than our messy human mental ecologies.\nIf one assumes a hard takeoff AGI, then all this must be codified in the system at launch,\nas once a potentially Singularitarian AGI is launched there is no way to know what time\nperiod constitutes “before the singularity point.” This means developing theory of mind empathy\nand logical ethics in code prior to giving the system unfettered access to hardware and selfmodification\ncode. However, though nobody can predict if or when a Singularity will occur\nafter unrestricted launch, only a truly irresponsible AGI development team would attempt to\ncreate an AGI without first experimenting with ethical training of the system in an intelligencecapped\nform, by means of ethical instruction via human-AGI interaction both pedagogically\nand experientially.\n12.9.4 Global Brain Mindplex Scenarios\nAnother class of scenarios – overlapping some of the previous ones – involves the emergence\nof a “Global Brain,” an emergent intelligence formed from global communication networks incorporating\nhumans and software programs in a larger body of self-organizing dynamics. The\nnotion of the Global Brain is reviewed in [Hey07, Tur77] and its connection with advanced\nAI is discussed in detail in Goertzel’s book Creating Internet Intelligence [Goe01], where three\npossible phases of “Global Brain” development are articulated:\n• Phase 1: computer and communication technologies as enhancers of human\ninteractions. This is what we have today: science and culture progress in ways that would\nnot be possible if not for the “digital nervous system” we’re spreading across the planet.\nThe network of idea and feeling sharing can become much richer and more productive than\nit is today, just through incremental development, without any Metasystem transition.\n• Phase 2: the intelligent Internet. At this point our computer and communication systems,\nthrough some combination of self-organizing evolution and human engineering, have\nbecome a coherent mind on their own, or a set of coherent minds living in their own digital\nenvironment.\n• Phase 3: the full-on Singularity. A complete revision of the nature of intelligence, human\nand otherwise, via technological and intellectual advancement totally beyond the scope of\nour current comprehension. At this point our current psychological and cultural realities\nare no more relevant than the psyche of a goose is to modern society.\nThe main concern of Creating Internet Intelligence is with\n• how to get from Phase 1 to Phase 2 - i.e. how to build an AGI system that will effect or\nencourage the transformation of the Internet into a coherent intelligent system\n• how to ensure that the Phase 2, Internet-savvy, global-brain-centric AGI systems will be\noriented toward intelligence-improving self-modification (so they’ll propel themselves to\nPhase 3), and also toward generally positive goals (as opposed to, say, world domination\nand extermination of all other intelligent life forms besides themselves!)\nOne possibly useful concept in this context is that of a mindplex: an intelligence that is\ncomposed largely of individual intelligences with their own self-models and global workspaces,\n238 12 The Engineering and Development of Ethics\nyet that also has its own self-model and global workspace. Both the individuals and the metamind\nshould be capable of deliberative, rational thought, to have a true “mindplex.” It’s unlikely\nthat human society or the Internet meet this criterion yet; and a system like an ant colony seems\nnot to either, because even though it has some degree of intelligence on both the individual and\ncollective levels, that degree of intelligence is not very great. But it seems quite feasible that\nthe global brain, at a certain stage of its development, will take the unfamiliar but fascinating\nform of a mindplex.\nCurrently the best way to explain what happens on the Net is to talk about the various\nparts of the Net: particular websites, social networks, viruses, and so forth. But there will come\na point when this is no longer the case, when the Net has sufficient high-level dynamics of its\nown that the way to explain any one part of the Net will be by reference to it relations with\nthe whole: and not just the dynamics of the whole, but the intentions and understanding of\nthe whole. This transition to Net-as-mindplex, we suspect, will come about largely through the\ninteractions of AI systems - intelligent programs acting on behalf of various individuals and\norganizations, who will collaborate and collectively constitute something halfway between a\nsociety of AI’s and an emergent mind whose lobes are various AI agents serving various goals.\nThe Phase 2 Internet, as it verges into mindplex-ness, will likely have a complex, sprawling\narchitecture, growing out of the architecture on the Net we experience today. The following\ncomponents at least can be expected:\n• A vast variety of “client computers,” some old, some new, some powerful, some weak –\nincluding many mobile and embedded devices not explicitly thought of as “computers.”\nSome of these will contribute little to Internet intelligence, mainly being passive recipients.\nOthers will be “smart clients,” carrying out personalization operations intended to help\nthe machines serve particular clients better, general AI operations handed to them by\nsophisticated AI server systems or other smart clients, and so forth.\n• “Commercial servers,” computers that carry out various tasks to support various types\nof heavyweight processing - transaction processing for e-commerce applications, inventory\nmanagement for warehousing of physical objects, and so forth. Some of these commercial\nservers interact with client computers directly, others do so only via AI servers. In nearly\nall cases, these commercial servers can benefit from intelligence supplied by AI servers.\n• The crux of the intelligent Internet: clusters of AI servers distributed across the Net, each\ncluster representing an individual computational mind (in many cases, a mindplex). These\nwill be able to communicate via one or more languages, and will collectively “drive” the\nwhole Net, by dispensing problems to client-machine-based processing frameworks, and\nproviding real-time AI feedback to commercial servers of various types. Some AI servers\nwill be general-purpose and will serve intelligence to commercial servers using an ASP\n(application service provider) model; others will be more specialized, tied particularly to a\ncertain commercial server (e.g., a large information services business might have its own AI\ncluster to empower its portal services).\nThis is one concrete vision of what a “global brain” might look like, in the relatively near term,\nwith AGI systems playing a critical role. Note that, in this vision, mindplexes may exist on two\nlevels:\n• Within AGI-clusters serving as actors within the overall Net\n• On the overall Net level\n12.10 Conclusion: Eight Ways to Bias AGI Toward Friendliness 239\nTo make these ideas more concrete, we may speculatively reformulate the first two “global\nbrain phases” mentioned above as follows:\n• Phase 1 global brain proto-mindplex: AI/AGI systems enhancing online databases, guiding\nGoogle results, forwarding e-mails, suggesting mailing-lists, etc. - generally using intelligence\nto mediate and guide human communications toward goals that are its own, but that are\nthemselves guided by human goals, statements and actions\n• Phase 2 global brain mindplex: AGI systems composing documents, editing human-written\ndocuments, sending and receiving e-mails, assembling mailing lists and posting to them,\ncreating new databases and instructing humans in their use, etc.\nIn Phase 2, the conscious theater of the global-brain-mediating AGI system is composed of\nideas built by numerous individual humans - or ideas emergent from ideas built by numerous\nindividual humans - and it conceives ideas that guide the actions and thoughts of individual\nhumans, in a way that is motivated by its own goals. It does not force the individual humans\nto do anything - but if a given human wishes to communicate and interact using the same\ndatabases, mailing lists and evolving vocabularies as other humans, they are going to have to\nuse the products of the global brain mediating AGI, which means they are going to have to\nparticipate in its patterns and its activities.\nOf course, the advent of advanced neurocomputer interfaces makes the picture potentially\nmore complex. At some point, it will likely be possible for humans to project thoughts and\nimages directly into computers without going through mouse or keyboard - and to “read in”\nthoughts and images similarly. When this occurs, interaction between humans may in some contexts\nbecome more like interactions between computers, and the role of global brain mediating\nAI servers may become one of mediating direct thought-to-thought exchanges between people.\nThe ethical issues associated with global brain scenarios are in some ways even subtler than\nin the other scenarios we mentioned above. One has issues pertaining to the desirability of\nseeing the human race become something fundamentally different – something more social and\nnetworked, less individual and autonomous. One has the risk of AGI systems exerting a subtle\nbut strong control over people, vaguely like the control that the human brain’s executive system\nexerts over the neurons involved with other brain subsystems. On the other hand, one also has\nmore human empowerment than in some of the other scenarios – because the systems that are\nchanging and deciding things are not separate from humans, but are, rather, composite systems\nessentially involving humans.\nSo, in the global brain scenarios, one has more “human” empowerment than in some other\ncases – but the “humans” involved aren’t legacy humans like us, but heavily networked humans\nthat are largely characterized by the emergent dynamics and structures implicit in their\ninterconnected activity!\n12.10 Conclusion: Eight Ways to Bias AGI Toward Friendliness\nIt would be nice if we had a simple, crisp, comforting conclusion to this chapter on AGI ethics,\nbut it’s not the case. There is a certain irreducible uncertainty involved in creating advanced\nartificial minds. There is also a large irreducible uncertainty involved in the future of the human\nrace in the case that we don’t create advanced artificial minds: in accordance with the ancient\nChinese curse, we live in interesting times!\n240 12 The Engineering and Development of Ethics\nWhat we can do, in this face of all this uncertainty, is to use our common sense to craft artificial\nminds that seem rationally and intuitively likely to be forces for good rather than otherwise\n– and revise our ideas frequently and openly based on what we learn as our research progresses.\nWe have roughly outlined our views on AGI ethics, which have informed the CogPrime design\nin countless ways; but the current CogPrime design itself is just the initial condition for an\nAGI project. Assuming the project succeeds in creating an AGI preschooler, experimentation\nwith this preschooler will surely teach us a great deal: both about AGI architecture in general,\nand about AGI ethics architecture in particular. We will then refine our cognitive and ethical\ntheories and our AGI designs as we go about engineering, observing and teaching the next\ngeneration of systems.\nAll this is not a magic bullet for the creation of beneficial AGI systems, but we believe it’s\nthe right process to follow. The creation of AGI is part of a larger evolutionary process that\nhuman beings are taking part in, and the crafting of AGI ethics through engineering, interaction\nand instruction is also part of this process. There are no guarantees here – guarantees are rare\nin real life – but that doesn’t mean that the situation is dire or hopeless, nor that (as some\ncommentators have suggested [Joy00, McK03]) AGI research is too dangerous to pursue. It\nmeans we need to be mindful, intelligent, compassionate and cooperative as we proceed to\ncarry out our parts in the next phase of the evolution of mind.\nWith this perspective in mind, we will conclude this chapter with a list of \"Eight Ways to\nBias Open-Source AGI Toward Friendliness\", borrowed from a previous paper by Ben Goertzel\nand Joel Pitt of that name. These points summarize many of the points raised in the prior\nsections of this chapter, in a relatively crisp and practical manner:\n1. Engineer Multifaceted Ethical Capabilities, corresponding to the multiple types of\nmemory, including rational, empathic, imitative, etc.\n2. Foster Rich Ethical Interaction and Instruction, with instructional methods according\nto the communication modes corresponding to all the types of memory: verbal, demonstrative,\ndramatic/depictive, indicative, goal-oriented.\n3. Engineer Stable, Hierarchy-Dominated Goal Systems ... which is enabled nicely by\nCogPrime’s goal framework and its integration with the rest of the CogPrime design\n4. Tightly Link AGI with the Global Brain, so that it can absorb human ethical principles,\nboth via natural interaction, and perhaps via practical implementations of current\nloosely-defined strategies like CEV, CAV and CBV\n5. Foster Deep, Consensus-Building Interactions Between People with Divergent\nViews, so as to enable the interaction with the Global Brain to have the most clear and\npositive impact\n6. Create a Mutually Supportive Community of AGIs which can then learn from\neach other and police against unfortunate developments (an approach which is meaningful\nif the AGIs are architected so as to militate against unexpected radical accelerations in\nintelligence)\n7. Encourage Measured Co-Advancement of AGI Software and AGI Ethics Theory\n8. Develop Advanced AGI Sooner Not Later\nThe last two of these points were not explicitly discussed in the body of the chapter, and so\nwe will finalize the chapter by reviewing them here.\n12.10 Conclusion: Eight Ways to Bias AGI Toward Friendliness 241\n12.10.1 Encourage Measured Co-Advancement of AGI Software and\nAGI Ethics Theory\nEverything involving AGI and Friendly AI (considered together or separately) currently involves\nsignificant uncertainty, and it seems likely that significant revision of current concepts will be\nvaluable, as progress on the path toward powerful AGI proceeds. However, whether there is\ntime for such revision to occur before AGI at the human level or above is created, depends on\nhow fast is our progress toward AGI. What one wants is for progress to be slow enough that,\nat each stage of intelligence advance, concepts such as those discussed in this paper can be\nre-evaluated and re-analyzed in the light of the data gathered, and AGI designs and approaches\ncan be revised accordingly as necessary.\nHowever, due to the nature of modern technology development, it seems extremely unlikely\nthat AGI development is going to be artificially slowed down in order to enable measured\ndevelopment of accompanying ethical tools, practices and understandings. For example, if one\nnation chose to enforce such a slowdown as a matter of policy (speaking about a future date\nat which substantial AGI progress has already been demonstrated, so that international AGI\nfunding is dramatically increased from present levels), the odds seem very high that other\nnations would explicitly seek to accelerate their own progress on AGI, so as to reap the ensuing\ndifferential economic benefits (the example of stem cells arises again).\nAnd this leads on to our next and final point regarding strategy for biasing AGI toward\nFriendliness....\n12.10.2 Develop Advanced AGI Sooner Not Later\nSomewhat ironically, it seems the best way to ensure that AGI development proceeds at a relatively\nmeasured pace is to initiate serious AGI development sooner rather than later. This is\nbecause the same AGI concepts will meet slower practical development today than 10 years\nfrom now, and slower 10 years from now than 20 years from now, etc. – due to the ongoing\nrapid advancement of various tools related to AGI development, such as computer hardware,\nprogramming languages, and computer science algorithms; and also the ongoing global advancement\nof education which makes it increasingly cost-effective to recruit suitably knowledgeable\nAI developers.\nCurrently the pace of AGI progress is sufficiently slow that practical work is in no danger\nof outpacing associated ethical theorizing. However, if we want to avoid the future occurrence\nof this sort of dangerous outpacing, our best practical choice is to make sure more substantial\nAGI development occurs in the phase before the development of tools that will make AGI\ndevelopment extraordinarily rapid. Of course, the authors are doing their best in this direction\nvia their work on the CogPrime project!\nFurthermore, this point bears connecting with the need, raised above, to foster the development\nof Global Brain technologies capable to \"Foster Deep, Consensus-Building Interactions\nBetween People with Divergent Views.\" If this sort of technology is to be maximally valuable,\nit should be created quickly enough that we can use it to help shape the goal system content of\nthe first highly powerful AGIs. So, to simplify just a bit: We really want both deep-sharing GB\ntechnology and AGI technology to evolve relatively rapidly, compared to computing hardware\nand advanced CS algorithms (since the latter factors will be the main drivers behind the ac-\n242 12 The Engineering and Development of Ethics\ncelerating ease of AGI development). And this seems significantly challenging, since the latter\nreceive dramatically more funding and focus at present.\nIf this perspective is accepted, then we in the AGI field certainly have our work cut out for\nus!\nSection IV\nNetworks for Explicit and Implicit Knowledge\nRepresentation\n\nChapter 13\nLocal, Global and Glocal Knowledge\nRepresentation\nCo-authored with Matthew Ikle, Joel Pitt and Rui Liu\n13.1 Introduction\nOne of the most powerful metaphors we’ve found for understanding minds is to view them\nas networks – i.e. collections of interrelated, interconnected elements. The view of mind as\nnetwork is implicit in the patternist philosophy, because every pattern can be viewed as a\npattern in something, or a pattern of arrangement of something – thus a pattern is always\nviewable as a relation between two or more things. A collection of patterns is thus a patternnetwork.\nKnowledge of all kinds may be given network representations; and cognitive processes\nmay be represented as networks also; for instance via representing them as programs, which\nmay be represented as trees or graphs in various standard ways. The emergent patterns arising\nin an intelligence as it develops may be viewed as a pattern network in themselves; and the\nrelations between an embodied mind and its physical and social environment may be viewed in\nterms of ecological and social networks.\nThe chapters in this section are concerned with various aspects of networks, as related to\nintelligence in general and AGI in particular. Most of this material is not specific to CogPrime,\nand would be relevant to nearly any system aiming at human-level AGI. However, most of it\nhas been developed in the course of work on CogPrime, and has direct relevance to understanding\nthe intended operation of various aspects of a completed CogPrime system. We begin\nour excursion into networks, in this chapter, with an issue regarding networks and knowledge\nrepresentation. One of the biggest decisions to make in designing an AGI system is how the\nsystem should represent knowledge. Naturally any advanced AGI system is going to synthesize\na lot of its own knowledge representations for handling particular sorts of knowledge – but\nstill, an AGI design typically makes at least some sort of commitment about the category of\nknowledge representation mechanisms toward which the AGI system will be biased. The two\nmajor supercategories of knowledge representation systems are local (also called explicit) and\nglobal (also called implicit) systems, with a hybrid category we refer to as glocal that combines\nboth of these. In a local system, each piece of knowledge is stored using a small percentage of\nAGI system elements; in a global system, each piece of knowledge is stored using a particular\npattern of arrangement, activation, etc. of a large percentage of AGI system elements; in a\nglocal system, the two approaches are used together.\nIn the first section here we discuss the symbolic, semantic-network aspects of knowledge\nrepresentation in CogPrime\n245\n246 13 Local, Global and Glocal Knowledge Representation\n. Then we turn to distributed, neural-net-like knowledge representation, reviewing a host of\ngeneral issues related to knowledge representation in attractor neural networks, turning finally\nto “glocal” knowledge representation mechanisms, in which ANNs combine localist and globalist\nrepresentation, and explaining the relationship of the latter to CogPrime. The glocal aspect of\nCogPrime knowledge representation will become prominent in later chapters such as:\n• in Chapter 23 of Part 2, where Economic Attention Networks (ECAN) are introduced and\nseen to have dynamics quite similar to those of the attractor neural nets considered here,\nbut with a mathematics roughly modeling money flow in a specially constructed artificial\neconomy rather than electrochemical dynamics of neurons.\n• in Chapter 42 of Part 2, where “map formation” algorithms for creating localist knowledge\nfrom globalist knowledge are described\n13.2 Localized Knowledge Representation using Weighted, Labeled\nHypergraphs\nThere are many different mechanisms for representing knowledge in AI systems in an explicit,\nlocalized way, most of them descending from various variants of formal logic. Here we briefly\ndescribe how it is done in CogPrime, which on the surface is not that different from a number of\nprior approaches. (The particularities of CogPrime’s explicit knowledge representation, however,\nare carefully tuned to match CogPrime’s cognitive processes, which are more distinctive in\nnature than the corresponding representational mechanisms.)\n13.2.1 Weighted, Labeled Hypergraphs\nOne useful way to think about CogPrime’s explicit, localized knowledge representation is in\nterms of hypergraphs. A hypergraph is an abstract mathematical structure [Bol98], which consists\nof objects called Nodes and objects called Links which connect the Nodes. In computer\nscience, a graph traditionally means a bunch of dots connected with lines (i.e. Nodes connected\nby Links). A hypergraph, on the other hand, can have Links that connect more than two Nodes.\nIn these pages we will often consider “generalized hypergraphs” that extend ordinary hypergraphs\nby containing two additional features:\n• Links that point to Links instead of Nodes\n• Nodes that, when you zoom in on them, contain embedded hypergraphs.\nProperly, such “hypergraphs” should always be referred to as generalized hypergraphs, but\nthis is cumbersome, so we will persist in calling them merely hypergraphs. In a hypergraph\nof this sort, Links and Nodes are not as distinct as they are within an ordinary mathematical\ngraph (for instance, they can both have Links connecting them), and so it is useful to have a\ngeneric term encompassing both Links and Nodes; for this purpose, we use the term Atom.\nA weighted, labeled hypergraph is a hypergraph whose Links and Nodes come along with\nlabels, and with one or more numbers that are generically called weights. A label associated\nwith a Link or Node may sometimes be interpreted as telling you what type of entity it is, or\n13.3 Atoms: Their Types and Weights 247\nalternatively as telling you what sort of data is associated with a Node. On the other hand,\nan example of a weight that may be attached to an Link or Node is a number representing a\nprobability, or a number representing how important the Node or Link is.\nObviously, hypergraphs may come along with various sorts of dynamics. Minimally, one may\nthink about:\n• Dynamics that modify the properties of Nodes or Links in a hypergraph (such as the labels\nor weights attached to them.)\n• Dynamics that add new Nodes or Links to a hypergraph, or remove existing ones.\n13.3 Atoms: Their Types and Weights\nThis section reviews a variety of CogPrime\nAtom types and gives simple examples of each of them. The Atom types considered are drawn\nfrom those currently in use in the OpenCog system. This does not represent a complete list of\nAtom types referred to in the text of this book, nor a complete list of those used in OpenCog\ncurrently (though it does cover a substantial majority of those used in OpenCog currently,\nomitting only some with specialized importance or intended only for temporary use).\nThe partial nature of the list given here reflects a more general point: The specific collection\nof Atom types in an OpenCog system is bound to change as the system is developed and experiment\nwith. CogPrime specifies a certain collection of representational approaches and cognitive\nalgorithms for acting on them; any of these approaches and algorithms may be implemented\nwith a variety of sets of Atom types. The specific set of Atom types in the OpenCog system\ncurrently does not necessarily have a profound and lasting significance – the list might look a\nbit different five years from time of writing, based on various detailed changes.\nThe treatment here is informal and intended to get across the general idea of what each\nAtom type does. A longer and more formal treatment of the Atom types is given in Part II,\nbeginning in Chapter 20.\n13.3.1 Some Basic Atom Types\nWe begin with ConceptNode – and note that a ConceptNode does not necessarily refer to a\nwhole concept, but may refer to part of a concept – it is essentially a \"basic semantic node\"\nwhose meaning comes from its links to other Atoms. It would be more accurately, but less\ntersely, named \"concept or concept fragment or element node.\" A simple example would be a\nConceptNode grouping nodes that are somehow related, e.g.\nConceptNode: C\nInheritanceLink (ObjectNode: BW) C\nInheritanceLink (ObjectNode: BP) C\nInheritanceLink (ObjectNode: BN) C\nReferenceLink BW (PhraseNode \"Ben’s watch\")\nReferenceLink BP (PhraseNode \"Ben’s passport\")\nReferenceLink BN (PhraseNode \"Ben’s necklace\")\n248 13 Local, Global and Glocal Knowledge Representation\nindicates the simple and uninteresting ConceptNode grouping three objects owned by Ben\n(note that the above-given Atoms don’t indicate the ownership relationship, they just link the\nthree objects with textual descriptions). In this example, the ConceptNode links transparently\nto physical objects and English descriptions, but in general this won’t be the case – most\nConceptNodes will look to the human eye like groupings of links of various types, that link to\nother nodes consisting of groupings of links of various types, etc.\nThere are Atoms referring to basic, useful mathematical objects, e.g. NumberNodes like\nNumberNode #4\nNumberNode #3.44\nThe numerical value of a NumberNode is explicitly referenced within the Atom.\nA core distinction is made between ordered links and unordered links; these are handled\ndifferently in the Atomspace software. A basic unordered link is the SetLink, which groups its\narguments into a set. For instance, the ConceptNode C defined by\nConceptNode C\nMemberLink A C\nMemberLink B C\nis equivalent to\nSetLink A B\nOn the other hand, ListLinks are like SetLinks but ordered, and they play a fundamental role\ndue to their relationship to predicates. Most predicates are assumed to take ordered arguments,\nso we may say e.g.\nEvaluationLink\nPredicateNode eat\nListLink\nConceptNode cat\nConceptNode mouse\nto indicate that cats eat mice.\nNote that by an expression like\nConceptNode cat\nis meant\nConceptNode C\nReferenceLink W C\nWordNode W #cat\nsince it’s WordNodes rather than ConceptNodes that refer to words. (And note that the strength\nof the ReferenceLink would not be 1 in this case, because the word \"cat\" has multiple senses.)\nHowever, there is no harm nor formal incorrectness in the \"ConceptNode cat\" usage, since \"cat\"\nis just as valid a name for a ConceptNode as, say, \"C.\"\nWe’ve already introduced above the MemberLink, which is a link joining a member to the\nset that contains it. Notable is that the truth value of a MemberLink is fuzzy rather than\nprobabilistic, and that PLN is able to inter-operate fuzzy and probabilistic values.\nSubsetLinks also exist, with the obvious meaning, e.g.\nConceptNode cat\nConceptNode animal\nSubsetLink cat animal\n13.3 Atoms: Their Types and Weights 249\nNote that SubsetLink refers to a purely extensional subset relationship, and that InheritanceLInk\nshould be used for the generic \"intensional + extensional\" analogue of this – more\non this below. SubsetLink could more consistently (with other link types) be named ExtensionalInheritanceLink,\nbut SubsetLink is used because it’s shorter and more intuitive.\nThere are links representing Boolean operations AND, OR and NOT. For instance, we may\nsay\nImplicationLink\nANDLink\nConceptNode young\nConceptNode beautiful\nConceptNode attractive\nor, using links and VariableNodes instead of ConceptNodes,\nAverageLink $X\nImplicationLink\nANDLink\nEvaluationLink young $X\nEvaluationLink beautiful $X\nEvaluationLink attractive $X\nNOTLink is a unary link, so e.g. we might say\nAverageLink $X\nImplicationLink\nANDLink\nEvaluationLink young $X\nEvaluationLink beautiful $X\nEvaluationLink\nNOT\nEvaluationLink poor $X\nEvaluationLink attractive $X\nContextLink allows explicit contextualization of knowledge, which is used in PLN, e.g.\nContextLink\nConceptNode golf\nInheritanceLink\nObjectNode BenGoertzel\nConceptNode incompetent\nsays that Ben Goertzel is incompetent in the context of golf.\n13.3.2 Variable Atoms\nWe have already introduced VariableNodes above; it’s also possible to specify the type of a\nVariableNode via linking it to a VariableTypeNode via a TypedVariableLink, e.g.\nVariableTypeLink\nVariableNode $X\nVariableTypeNode ConceptNode\nwhich specifies that the variable $X should be filled with a ConceptNode.\nVariables are handled via quantifiers; the default quantifier being the AverageLink, so that\nthe default interpretation of\n250 13 Local, Global and Glocal Knowledge Representation\nImplicationLink\nInheritanceLink $X animal\nEvaluationLink\nPredicateNode: eat\nListLink\n\\$X\nConceptNode: food\nis\nAverageLink $X\nImplicationLink\nInheritanceLink $X animal\nEvaluationLink\nPredicateNode: eat\nListLink\n\\$X\nConceptNode: food\nThe AverageLink invokes an estimation of the average TruthValue of the embedded expression\n(in this case an ImplicationLink) over all possible values of the variable $X. If there are type\nrestrictions regarding the variable $X, these are taken into account in conducting the averaging.\nFor AllLink and Exist s-Link may be used in the same places as AverageLink, with uncertain\ntruth value semantics defined in PLN theory using third-order probabilities. There is also a\nScholemLink used to indicate variable dependencies for existentially quantified variables, used\nin cases of multiply nested existential quantifiers.\nEvaluationLink and MemberLink have overlapping semantics, allowing expression of the\nsame conceptual/logical relationships in terms of predicates or sets, i.e.\nEvaluationLink\nPredicateNode: eat\nListLink\n$X\nConceptNode: food\nhas the same semantics as\nMemberLink\nListLink\n$X\nConceptNode: food\nConceptNode: EatingEvents\nThe relation between the predicate \"eat\" and the concept \"EatingEvents\" is formally given by\nExtensionalEquivalenceLink\nConceptNode: EatingEvents\nSatisfyingSetLink\nPredicateNode: eat\nIn other words, we say that \"EatingEvents\" is the SatisfyingSet of the predicate \"eat\": it is the\nset of entities that satisfy the predicate \"eat\". Note that the truth values of MemberLink and\nEvaluationLink are fuzzy rather than probabilistic.\n13.3 Atoms: Their Types and Weights 251\n13.3.3 Logical Links\nThere is a host of link types embodying logical relationships as defined in the PLN logic system,\ne.g.\n• InheritanceLink\n• SubsetLink (aka ExtensionalInheritanceLink)\n• Intensional InheritanceLink\nwhich embody different sorts of inheritance, e.g.\nSubsetLink salmon fish\nIntensionalInheritanceLink whale fish\nInheritanceLink fish animal\nand then\n• SimilarityLink\n• ExtensionalSimilarityLink\n• IntensionalSimilarityLink\nwhich are symmetrical versions, e.g.\nSimilaritytLink shark barracuda\nIntensionalSimilarityLink shark dolphin\nExtensionalSimiliarityLink American obese\\_person\nThere are also higher-order versions of these links, both asymmetric\n• ImplicationLink\n• ExtensionalImplicationLink\n• IntensionalImplicationLink\nand symmetric\n• EquivalenceLink\n• ExtensionalEquivalenceLink\n• IntensionalEquivalenceLink\nThese are used between predicates and links, e.g.\nImplicationLink\nEvaluationLink\neat\nListLink\n$X\ndirt\nEvaluationLink\nfeel\nListLInk\n$X\nsick\nor\n252 13 Local, Global and Glocal Knowledge Representation\nImplicationLink\nEvaluationLink\neat\nListLink\n$X\ndirt\nInheritanceLink $X sick\nor\nForAllLink $X, $Y, $Z\nExtensionalEquivalenceLink\nEquivalenceLink\n$Z\nEvaluationLink\n+\nListLink\n$X\n$Y\nEquivalenceLink\n$Z\nEvaluationLink\n+\nListLink\n$Y\n$X\nNote, the latter is given as an extensional equivalence because it’s a pure mathematical equivalence.\nThis is not the only case of pure extensional equivalence, but it’s an important one.\n13.3.4 Temporal Links\nThere are also temporal versions of these links, such as\n• PredictiveImplicationLink\n• PredictiveAttractionLink\n• SequentialANDLink\n• SimultaneousANDLink\nwhich combine logical relation between the argument with temporal relation between their\narguments. For instance, we might say\nPredictiveImplicationLink\nPredicateNode: JumpOffCliff\nPredicateNode: Dead\nor including arguments,\nPredictiveImplicationLink\nEvaluationLink JumpOffCliff $X\nEvaluationLink Dead $X\nThe former version, without variable arguments given, shows the possibility of using higherorder\nlogical links to join predicates without any explicit variables. Via using this format exclusively,\none could avoid VariableAtoms entirely, using only higher-order functions in the manner\n13.3 Atoms: Their Types and Weights 253\nof pure functional programming formalisms like combinatory logic. However, this purely functional\nstyle has not proved convenient, so the Atomspace in practice combines functional-style\nrepresentation with variable-based representation.\nTemporal links often come with specific temporal quantification, e.g.\nPredictiveImplicationLink <5 seconds>\nEvaluationLink JumpOffCliff $X\nEvaluationLink Dead $X\nindicating that the conclusion will generally follow the premise within 5 seconds. There is a\nsystem for managing fuzzy time intervals and their interrelationships, based on a fuzzy version\nof Allen Interval Algebra.\nSequentialANDLink is similar to PredictiveImplicationLink but its truth value is calculated\ndifferently. The truth value of\nSequentialANDLink <5 seconds>\nEvaluationLink JumpOffCliff $X\nEvaluationLink Dead $X\nindicates the likelihood of the sequence of events occurring in that order, with gap lying within\nthe specified time interval. The truth value of the PredictiveImplicationLink version indicates\nthe likelihood of the second event, conditional on the occurrence of the first event (within the\ngiven time interval restriction).\nThere are also links representing basic temporal relationships, such as BeforeLink and AfterLink.\nThese are used to refer to specific events, e.g. if X refers to the event of Ben waking\nup on July 15 2012, and Y refers to the event of Ben getting out of bed on July 15 2012, then\none might have\nAfterLink X Y\nAnd there are TimeNodes (representing time-stamps such as temporal moments or intervals)\nand AtTimeLinks, so we may e.g. say\nAtTimeLink\nX\nTimeNode: 8:24AM Eastern Standard Time, July 15 2012 AD\n13.3.5 Associative Links\nThere are links representing associative, attentional relationships,\n• HebbianLink\n• AsymmetricHebbianLink\n• InverseHebbianLink\n• SymmetricInverseHebbianLink\nThese connote associations between their arguments, i.e. they connote that the entities represented\nby the two argument occurred in the same situation or context, for instance\nHebbianLink happy smiling\nAsymmetricHebbianLink dead rotten\nInverseHebbianLink dead breathing\n254 13 Local, Global and Glocal Knowledge Representation\nThe asymmetric HebbianLink indicates that when the first argument is present in a situation,\nthe second is also often present. The symmetric (default) version indicates that this relationship\nholds in both directions. The inverse versions indicate the negative relationship: e.g. when one\nargument is present in a situation, the other argument is often not present.\n13.3.6 Procedure Nodes\nThere are nodes representing various sorts of procedures; these are kinds of ProcedureNode,\ne.g.\n• SchemaNode, indicating any procedure\n• GroundedSchemaNode, indicating any procedure associated in the system with a Combo\nprogram or C++ function allowing the procedure to be executed\n• PredicateNode, indicating any predicate that associates a list of arguments with an output\ntruth value\n• GroundedPredicateNode, indicating a predicate associated in the system with a Combo\nprogram or C++ function allowing the predicate’s truth value to be evaluated on a given\nspecific list of arguments\nExecutionLinks and EvaluationLinks record the activity of SchemaNodes and PredicateNodes.\nWe have seen many examples of EvaluationLinks in the above. Example ExecutionLinks\nwould be:\nExecutionLink step\\_forward\nExecutionLink step\\_forward 5\nExecutionLink\n+\nListLink\nNumberNode: 2\nNumberNode: 3\nThe first example indicates that the schema \"step forward\" has been executed. The second\nexample indicates that it has been executed with an argument of \"5\" (meaning, perhaps, that 5\nsteps forward have been attempted). The last example indicates that the \"+\" schema has been\nexecuted on the argument list (2,3), presumably resulting in an output of 5.\nThe output of a schema execution may be indicated using an ExecutionOutputLink, e.g.\nExecutionOutputLink\n+\nListLink\nNumberNode: 2\nNumberNode: 3\nrefers to the value \"5\" (as a NumberNode).\n13.3.7 Links for Special External Data Types\nFinally, there are also Atom types referring to specific types of data important to using OpenCog\nin specific contexts.\n13.3 Atoms: Their Types and Weights 255\nFor instance, there are Atom types referring to general natural language data types, such as\n• WordNode\n• SentenceNode\n• WordInstanceNode\n• DocumentNode\nplus more specific ones referring to relationships that are part of link-grammar parses of sentences\n• FeatureNode\n• FeatureLink\n• LinkGrammarRelationshipNode\n• LinkGrammarDisjunctNode\nor RelEx semantic interpretations of sentences\n• DefinedLinguisticConceptNode\n• DefinedLinguisticRelationshipNode\n• PrepositionalRelationshipNode\nThere are also Atom types corresponding to entities important for embodying OpenCog in\na virtual world, e.g.\n• ObjectNode\n• AvatarNode\n• HumanoidNode\n• UnknownObjectNode\n• AccessoryNode\n13.3.8 Truth Values and Attention Values\nCogPrime Atoms (Nodes and Links) are quantified with truth values that, in their simplest\nform, have two components, one representing probability (strength) and the other representing\nweight of evidence; and also with attention values that have two components, short-term and\nlong-term importance, representing the estimated value of the Atom on immediate and longterm\ntime-scales.\nIn practice many Atoms are labeled with CompositeTruthValues rather than elementary ones.\nA composite truth value contains many component truth values, representing truth values of\nthe Atom in different contexts and according to different estimators.\nIt is important to note that the CogPrime declarative knowledge representation is neither\na neural net nor a semantic net, though it does have some commonalities with each of these\ntraditional representations. It is not a neural net because it has no activation values, and involves\nno attempts at low-level brain modeling. However, attention values are very loosely analogous\nto time-averages of neural net activations. On the other hand, it is not a semantic net because\nof the broad scope of the Atoms in the network: for example, Atoms may represent percepts,\nprocedures, or parts of concepts. Most CogPrime Atoms have no corresponding English label.\nHowever, most CogPrime\nAtoms do have probabilistic truth values, allowing logical semantics.\n256 13 Local, Global and Glocal Knowledge Representation\n13.4 Knowledge Representation via Attractor Neural Networks\nNow we turn to global, implicit knowledge representation – beginning with formal neural net\nmodels, briefly discussing the brain, and then turning back to CogPrime. Firstly, this section\nreviews some relevant material from the literature regarding the representation of knowledge\nusing attractor neural nets. It is a mix of well-established fact with more speculative material.\n13.4.1 The Hopfield neural net model\nHopfield networks [Hop82] are attractor neural networks often used as associative memories. A\nHopfield network with N neurons can be trained to store a set of bipolar patterns P , where\neach pattern p has N bipolar (±1) values. A Hopfield net typically has symmetric weights with\nno self-connections. The weight of the connection between neurons i and j is denoted by w ij .\nIn order to apply a Hopfield network to a given input pattern p, its activation state is set to\nthe input pattern, and neurons are updated asynchronously, in random order, until the network\nconverges to the closest fixed point. An often-used activation function for a neuron is:\n∑\ny i = sign(p i w ij y j )\nTraining a Hopfield network, therefore, involves finding a set of weights w ij that stores the\ntraining patterns as attractors of its network dynamics, allowing future recall of these patterns\nfrom possibly noisy inputs.\nOriginally, Hopfield used a Hebbian rule to determine weights:\nw ij =\nj≠i\nP∑\np i p j\np=1\nTypically, Hopfield networks are fully connected. Experimental evidence, however, suggests\nthat the majority of the connections can be removed without significantly impacting the network’s\ncapacity or dynamics. Our experimental work uses sparse Hopfield networks.\n13.4.1.1 Palimpsest Hopfield nets with a modified learning rule\nIn [SV99] a new learning rule is presented, which both increases the Hopfield network capacity\nand turns it into a “palimpsest”, i.e., a network that can continuously learn new patterns, while\nforgetting old ones in an orderly fashion.\nUsing this new training rule, weights are initially set to zero, and updated for each new\npattern p to be learned according to:\n13.4 Knowledge Representation via Attractor Neural Networks 257\nN∑\nh ij = w ik p k\nk=1,k≠i,j\n∆w ij = 1 n (p ip j − h ij p j − h ji p i )\n13.4.2 Knowledge Representation via Cell Assemblies\nHopfield nets and their ilk play a dual role: as computational algorithms, and as conceptual\nmodels of brain function. In CogPrime they are used as inspiration for slightly different, artificial\neconomics based computational algorithms; but their hypothesized relevance to brain function\nis nevertheless of interest in a CogPrime context, as it gives some hints about the potential\nconnection between low-level neural net mechanics and higher-level cognitive dynamics.\nHopfield nets lead naturally to a hypothesis about neural knowledge representation, which\nholds that a distinct mental concept is represented in the brain as either:\n1. a set of “cell assemblies”, where each assembly is a network of neurons that are interlinked\nin such a way as to fire in a (perhaps nonlinearly) synchronized manner\n2. a distinct temporal activation pattern, which may occur in any one (or more) of a particular\nset of cell assemblies\nFor instance, this hypothesis is perfectly coherent if one interprets a “mental concept” as a\nSMEPH (defined in Chapter 14) ConceptNode, i.e. a fuzzy set of perceptual stimuli to which\nthe organism systematically reacts in different ways. Also, although we will focus mainly on\ndeclarative knowledge here, we note that the same basic representational ideas can be applied\nto procedural and episodic knowledge: these may be hypothesized to correspond to temporal\nactivation patterns as characterized above.\nIn the biology literature, perhaps the best-articulated modern theories championing the cell\nassembly view are those of Gunther Palm [Pal82, HAG07] and Susan Greenfield [SF05, CSG07].\nPalm focuses on the dynamics of the formation and interaction assemblies of cortical columns.\nGreenfield argues that each concept has a core cell assembly, and that when the concept rises\nto the focus of attention, it recruits a number of other neurons beyond its core characteristic\nassembly into a “transient ensemble.” 1\nIt’s worth noting that there may be multiple redundant assemblies representing the same\nconcept – and potentially recruiting similar transient assemblies when highly activated. The\nimportance of repeated, slightly varied copies of the same subnetwork has been emphasized by\nEdelman [Ede93] among other neural theorists.\n1 The larger an ensemble is, she suggests, the more vivid it is as a conscious experience; an hypothesis that\naccords well with the hypothesis made in [Goe06b] that a more informationally intense pattern corresponds to\na more intensely conscious quale – but we don’t need to digress extensively onto matters of consciousness for\nthe present purposes.\n258 13 Local, Global and Glocal Knowledge Representation\n13.5 Neural Foundations of Learning\nNow we move from knowledge representation to learning – which is after all nothing but the\nadaptation of represented knowledge based on stimulus, reinforcement and spontaneous activity.\nWhile our focus in this chapter is on representation, it’s not possible for us to make our points\nabout glocal knowledge representation in neural net type systems without discussing some\naspects of learning in these systems.\n13.5.1 Hebbian Learning\nThe most common and plausible assumption about learning in the brain is that synaptic connections\nbetween neurons are adapted via some variant of Hebbian learning. The original Hebbian\nlearning rule, proposed by Donald Hebb in his 1949 book [Heb49], was roughly\n1. The weight of the synapse x → y increases if x and y fire at roughly the same time\n2. The weight of the synapse x → y decreases if x fires at a certain time but y does not\nOver the years since Hebb’s original proposal, many neurobiologists have sought evidence that\nthe brain actually uses such a method. One of the things they have found, so far, is a lot of\nevidence for the following learning rule [DC02, LS05]:\n1. The weight of the synapse x → y increases if x fires shortly before y does\n2. The weight of the synapse x → y decreases if x fires shortly after y does\nThe new thing here, not foreseen by Donald Hebb, is the “postsynaptic depression” involved in\nrule component 2.\nNow, the simple rule stated above does not sum up all the research recently done on Hebbiantype\nlearning mechanisms in the brain. The real biological story underlying these approximate\nrules is quite complex, involving many particulars to do with various neurotransmitters. Illunderstood\ndetails aside, however, there is an increasing body of evidence that not only does\nthis sort of learning occur in the brain, but it leads to distributed experience-based neural\nmodification: that is, one instance synaptic modification causes another instance of synaptic\nmodification, which causes another, and so forth 2 [Bi01].\n13.5.2 Virtual Synapses and Hebbian Learning Between Assemblies\nHebbian learning is conventionally formulated in terms of individual neurons, but, it can be\nextended naturally to assemblies via defining “virtual synapses” between assemblies.\nSince assemblies are sets of neurons, one can view a synapse as linking two assemblies\nif it links two neurons, each of which is in one of the assemblies. One can then view\ntwo assemblies as being linked by a bundle of synapses. We can define the weight of the\nsynaptic bundle from assembly A1 to assembly A2 as the number w so that (the change\n2 This has been observed in “model systems” consisting of neurons extracted from a brain and hooked together\nin a laboratory setting and monitored; measurement of such dynamics in vivo is obviously more difficult.\n13.5 Neural Foundations of Learning 259\nin the mean activation of A2 that occurs at time t+epsilon) is on average closest to w ×\n(the amount of energy flowing through the bundle from A1 to A2 at time t). So when A1 sends\nan amount x of energy along the synaptic bundle pointing from A1 to A2, then A2’s mean\nactivation is on average incremented/decremented by an amount w × x.\nIn a similar way, one can define the weight of a bundle of synapses between a certain static\nor temporal activation-pattern P1 in assembly A1, and another static or temporal activationpattern\nP2 in assembly A2. Namely, this may be defined as the number w so that (the amount\nof energy flowing through the bundle from A1 to A2 at time t)×w best approximates (the\nprobability that P2 is present in A2 at time t+epsilon), when averaged over all times t during\nwhich P1 is present in A1.\nIt is not hard to see that Hebbian learning on real synapses between neurons implies Hebbian\nlearning on these virtual synapses between cell assemblies and activation-patterns.\nThese ideas may be developed further to build a connection between neural knowledge representation\nand probabilistic logical knowledge representation such as is used in CogPrime’s\nProbabilistic Logic Networks formalism; this connection will be pursued at the end of Chapter\n34, once more relevant background has been presented.\n13.5.3 Neural Darwinism\nA notion quite similar to Hebbian learning between assemblies has been pursued by Nobelist\nGerald Edelman in his theory of neuronal group selection, or “Neural Darwinism.” Edelman won\na Nobel Prize for his work in immunology, which, like most modern immunology, was based on\nC. MacFarlane Burnet’s theory of “clonal selection” [Bur62], which states that antibody types\nin the mammalian immune system evolve by a form of natural selection. From his point of view,\nit was only natural to transfer the evolutionary idea from one mammalian body system (the\nimmune system) to another (the brain).\nThe starting point of Neural Darwinism is the observation that neuronal dynamics may be\nanalyzed in terms of the behavior of neuronal groups. The strongest evidence in favor of this\nconjecture is physiological: many of the neurons of the neocortex are organized in clusters, each\none containing say 10,000 to 50,000 neurons each. Once one has committed oneself to looking at\nsuch groups, the next step is to ask how these groups are organized, which leads to Edelman’s\nconcept of “maps.”\nA “map,” in Edelman’s terminology, is a connected set of groups with the property that when\none of the inter-group connections in the map is active, others will often tend to be active as\nwell. Maps are not fixed over the life of an organism. They may be formed and destroyed in\na very simple way: the connection between two neuronal groups may be “strengthened” by increasing\nthe weights of the neurons connecting the one group with the other, and “weakened” by\ndecreasing the weights of the neurons connecting the two groups. If we replace “map” with “cell\nassembly” we arrive at a concept very similar to the one described in the previous subsection.\nEdelman then makes the following hypothesis: the large-scale dynamics of the brain is dominated\nby the natural selection of maps. Those maps which are active when good results are\nobtained are strengthened, those maps which are active when bad results are obtained are\nweakened. And maps are continually mutated by the natural chaos of neural dynamics, thus\nproviding new fodder for the selection process. By use of computer simulations, Edelman and his\ncolleagues have shown that formal neural networks obeying this rule can carry out fairly compli-\n260 13 Local, Global and Glocal Knowledge Representation\ncated acts of perception. In general-evolution language, what is posited here is that organisms\nlike humans contain chemical signals that signify organism-level success of various types, and\nthat these signals serve as a “fitness function” correlating with evolutionary fitness of neuronal\nmaps.\nIn Neural Darwinism and his other related books and papers, Edelman goes far beyond this\ncrude sketch and presents neuronal group selection as a collection of precise biological hypotheses,\nand presents evidence in favor of a number of these hypotheses. However, we consider that\nthe basic concept of neuronal group selection is largely independent of the biological particularities\nin terms of which Edelman has phrased it. We suspect that the mutation and selection of\n“transformations” or “maps” is a necessary component of the dynamics of any intelligent system.\nAs we will see later on (e.g. in Chapter 42 of Part 2, this business of maps is extremely\nimportant to CogPrime. CogPrime does not have simulated biological neurons and synapses,\nbut it does have Nodes and Links that in some contexts play loosely similar roles. We sometimes\nthink of CogPrime Nodes and Links as being very roughly analogous to Edelman’s neuronal\nclusters, and emergent intercluster links. And we have maps among CogPrime Nodes and Links,\njust as Edelman has maps among his neuronal clusters. Maps are not the sole bearers of meaning\nin CogPrime, but they are significant ones.\nThere is a very natural connection between Edelman-style brain evolution and the ideas\nabout cognitive evolution presented in Chapter 3. Edelman proposes a fairly clear mechanism\nvia which patterns that survive a while in the brain are differentially likely to survive a long\ntime: this is basic Hebbian learning, which in Edelman’s picture plays a role between neuronal\ngroups. And, less directly, Edelman’s perspective also provides a mechanism by which intense\npatterns will be differentially selected in the brain: because on the level of neural maps, pattern\nintensity corresponds to the combination of compactness and functionality. Among a number\nof roughly equally useful maps serving the same function, the more compact one will be more\nlikely to survive over time, because it is less likely to be disrupted by other brain processes\n(such as other neural maps seeking to absorb its component neuronal groups into themselves).\nEdelman’s neuroscience remains speculative, since so much remains unknown about human\nneural structure and dynamics; but it does provide a tentative and plausible connection between\nevolutionary neurodynamics and the more abstract sort of evolution that patternist philosophy\nposits to occur in the realm of mind-patterns.\n13.6 Glocal Memory\nA glocal memory is one that transcends the global/local dichotomy and incorporates both\naspects in a tightly interconnected way. Here we make the glocal memory concept more precise,\nand describe its incarnation in the context of attractor neural nets (which is similar to its\nincarnation in CogPrime, to be elaborated in later chapters). Though our main interest here is\nin glocality in CogPrime, we also suggest that glocality may be a critical property to consider\nwhen analyzing human, animal and AI memory more broadly.\nThe notion of glocal memory has implicitly occurred in a number of prior brain theories\n(without use of the neologism “glocal”), e.g. [Cal96] and [Goe01], but it has not previously been\nexplicitly developed. However the concept has risen to the fore in our recent AI work and so\nwe have chosen to flesh it out more fully in [HG08], [GPI + 10] and the present section.\n13.6 Glocal Memory 261\nGlocal memory overcomes the dichotomy between localized memory (in which each memory\nitem is stored in a single location within an overall memory structure) and distributed memory\n(in which a memory item is stored as an aspect of a multi-component memory system, in such\na way that the same set of multiple components stores a large number of memories). In a glocal\nmemory system, most memory items are stored both locally and globally, with the property\nthat eliciting either one of the two records of an item tends to also elicit the other one.\nGlocal memory applies to multiple forms of memory; however we will focus largely on perceptual\nand declarative memory in our detailed analyses here, so as to conserve space and maintain\nsimplicity of discussion.\nThe central idea of glocal memory is that (perceptual, declarative, episodic, procedural,\netc.) items may be stored in memory in the form of paired structures that are called (key,\nmap) pairs. Of course the idea of a “pair” is abstract, and such pairs may manifest themselves\nquite differently in different sorts of memory systems (e.g. brains versus non-neuromorphic AI\nsystems). The key is a localized version of the item, and records some significant aspects of\nthe items in a simple and crisp way. The map is a dispersed, distributed version of the item,\nwhich represents the item as a (to some extent, dynamically shifting) combination of fragments\nof other items. The map includes the key as a subset; activation of the key generally (but not\nnecessarily always) causes activation of the map; and changes in the memory item will generally\ninvolve complexly coordinated changes on the key and map level both.\nMemory is one area where animal brain architecture differs radically from the von Neumann\narchitecture underlying nearly all contemporary general-purpose computers. Von Neumann\ncomputers separate memory from processing, whereas in the human brain there is no such\ndistinction. In fact, it’s arguable that in most cases the brain contains no memory apart from\nprocessing: human memories are generally constructed in the course of remembering [Ros88],\nwhich gives human memory a strong capability for “filling in gaps” of remembered experience\nand knowledge; and also causes problems with inaccurate remembering in many contexts\n[BF71, RM95] We believe the constructive aspect of memory is largely associated with its\nglocality.\nThe remainder of this section presents a fuller formalization of the glocal memory concept,\nwhich is then taken up further in three later chapters:\n• Chapter ?? discusses the potential implementation of glocal memory in the human brain\n• Chapter ?? discusses the implementation of glocal memory in attractor neural net systems\n• Chapter 23 presents Glocal Economic Attention Networks (ECANs), rough analogues of\nglocal Hopfield nets that play a central role in CogPrime.\nOur hypothesis of the potential general importance of glocality as a property of memory\nsystems (beyond just the CogPrime architecture) – remains somewhat speculative. The presence\nof glocality in human and animal memory is strongly suggested but not firmly demonstrated by\navailable neuroscience data; and the general value of glocality in the context of artificial brains\nand minds is also not yet demonstrated as the whole field of artificial brain and mind building\nremains in its infancy. However, the utility of glocal memory for CogPrime is not tied to this\nmore general, speculative theme – glocality may be useful in CogPrime even if we’re wrong that\nit plays a significant role in the brain and in intelligent systems more broadly.\n262 13 Local, Global and Glocal Knowledge Representation\n13.6.1 A Semi-Formal Model of Glocal Memory\nTo explain the notion of glocal memory more precisely, we will introduce a simple semi-formal\nmodel of a system S that uses a memory to record information relevant to the actions it\ncarries out. The overall concept of glocal memory should not be considered as restricted to\nthis particular model. This model is not intended for maximal generality, but is intended to\nencompass a variety of current AI system designs and formal neurological models.\nIn this model, we will consider S’s memory subsystem as a set of objects we’ll call “tokens,”\nembedded in some metric space. The metric in the space, which we will call the “basic distance”\nof the memory, generally will not be defined in terms of the semantics of the items stored in the\nmemory; though it may come to shape these dynamics through the specific architecture and\nevolution of the memory. Note that these tokens are not intended as generally being mapped\none-to-one onto meaningful items stored in the memory. The “tokens” are the raw materials\nthat the memory arranges in various patterns in order to store items.\nWe assume that each token, at each point in time, may meaningfully be assigned a certain\nquantitative “activation level.” Also, tokens may have other numerical or discrete quantities\nassociated with them, depending on the particular memory architecture. Finally, tokens may\nrelate other tokens, so that optionally a token may come equipped with an (ordered or unordered)\nlist of other tokens.\nTo understand the meaning of the activation levels, one should think about S’s memory\nsubsystem as being coupled with an action-selection subsystem, that dynamically chooses the\nactions to be taken by the overall system in which the two subsystems are embedded. Each\ncombination of actions, in each particular type of context, will generally be associated with the\nactivation of certain tokens in memory.\nThen, as analysts of the system S, we may associate each token T with an “activation vector”\nv(T, t), whose value for each discrete time t consists of the activation of the token T at time t.\nSo, the 50 ′ th entry of the vector corresponds to the activation of the token at the 50 ′ th time\nstep.\n“Items stored in memory” over a certain period of time, may then be defined as clusters in\nthe set of activation vectors associated with memory during that period of time. Note that the\nsystem S itself may explicitly recognize and remember patterns regarding what items are stored\nin its memory – but, from an external analyst’s perspective, the set of items in S’s memory is\nnot restricted to the ones that S has explicitly recognized as memory items.\nThe “localization” of a memory item may be defined as the degree to which the various tokens\ninvolved in the item are close to each other according to the metric in the memory metric-space.\nThis degree may be formalized in various ways, but choosing a particular quantitative measure\nis not important here. A highly localized item may be called “local” and a not-very-localized\nitem may be called “global.”\nWe may define the “activation distance” of two tokens as the distance between their activation\nvectors. We may then say that a memory is “well aligned” to the extent that there is a correlation\nbetween the activation distance of tokens, and the basic distance of the memory metric-space.\nGiven the above set-up, the basic notion of glocal memory can be enounced fairly simply. A\nglocal memory is one:\n• that is reasonably well-aligned (i.e. the correlation between activation and basic distance is\nsignificantly greater than random)\n13.6 Glocal Memory 263\n• in which most memory items come in pairs, consisting of one local item and one global\nitem, so that activation of the local item (the “key”) frequently leads in the near future to\nactivation of the global item (the “map”)\nObviously, in the scope of all possible memory structures constructible within the above\nformalism, glocal memories are going to be very rare and special. But, we suggest that they are\nimportant, because they are generally going to be the most effective way for intelligent systems\nto structure their memories.\nNote also that many memories without glocal structure may be “well-aligned” in the above\nsense.\nAn example of a predominantly local memory structure, in which nearly all significant memory\nitems are local according to the above definition, is the Cyc logical reasoning engine [LG90].\nTo cast the Cyc knowledge base in the present formal model, the tokens are logical predicates.\nCyc does not have an in-built notion of activation, but one may conceive the activation of a\nlogical formula in Cyc as the degree to which the formula is used in reasoning or query processing\nduring a certain interval in time. And one may define a basic metric for Cyc by associating\na predicate with its extension (the set of satisfying inputs), and defining the similarity of two\npredicates as the symmetric distance of their extensions. Cyc is reasonably well-aligned, but\naccording to the dynamics of its querying and reasoning engines, it is basically a local memory\nstructure without significant global memory structure.\nOn the other hand, an example of a predominantly global memory structure, in which nearly\nall significant memory items are global according to the above definition, is the Hopfield associative\nmemory network [Ami89]. Here memories are stored in the pattern of weights associated\nwith synapses within a network of formal neurons, and each memory in general involves a large\nnumber of the neurons in the network. To cast the Hopfield net in the present formal model, the\ntokens are neurons and synapses; the activations are neural net activations; the basic distance\nbetween two neurons A and B may be defined as the percentage of the time that stimulating\none of the neurons leads to the other one firing; and to calculate a basic distance involving a\nsynapse, one may associate the synapse with its source and target neurons. With these definitions,\na Hopfield network is a well-aligned memory, and (by intentional construction) a markedly\nglobal one. Local memory items will be very rare in a Hopfield net.\nWhile predominantly local and predominantly global memories may have great value for particular\napplications, our suggestion is that they also have inherent limitations. If so, this means\nthat the most useful memories for general intelligence are going to be those that involve both\nlocal and global memory items in central roles. However, this is a more general and less risky\nclaim than the assertion that glocal memory structure as defined above is important. Because,\n“glocal” as defined above doesn’t just mean “neither predominantly global nor predominantly\nlocal.” Rather, it refers to a specific pattern of coordination between local and global memory\nitems – what we have called the “keys and maps” pattern.\n13.6.2 Glocal Memory in the Brain\nScience’s understanding of human brain dynamics is still very primitive, one manifestation of\nwhich is the fact that we really don’t understand how the brain represents knowledge, except\nin some very simple respects. So anything anyone says about knowledge representation in the\nbrain, at this stage, has to be considered highly speculative. Existing neuroscience knowledge\n264 13 Local, Global and Glocal Knowledge Representation\ndoes imply constraints on how knowledge representation in the brain may work, but these are\nrelatively loose constraints. These constraints do imply that, for instance, the brain is neither a\nrelational database (in which information is stored in a wholly localized manner) nor a collection\nof “grandmother neurons” that respond individually to high-level percepts or concepts; nor a\nsimple Hopfield type neural net (in which all memories are attractors globally distributed across\nthe whole network). But they don’t tell us nearly enough to, for instance, create a formal neural\nnet model that can confidently be said to represent knowledge in the manner of the human brain.\nAs a first example of the current state of knowledge, we’ll discuss here a series of papers\nregarding the neural representation of visual stimuli [QaGKKF05, QKKF08], which deal with\nthe fascinating discovery of a subset of neurons in the medial temporal lobe (MTL) that are\nselectively activated by strikingly different pictures of given individuals, landmarks or objects,\nand in some cases even by letter strings. For instance, in their 2005 paper titled ”Invariant visual\nrepresentation by single neurons in the human brain”, it is noted that\nin one case, a unit responded only to three completely different images of the ex-president Bill Clinton.\nAnother unit (from a different patient) responded only to images of The Beatles, another one to cartoons\nfrom The Simpson’s television series and another one to pictures of the basketball player Michael Jordan.\nTheir 2008 follow-up paper backed away from the more extreme interpretation in the title as\nwell as the conclusion, with the title “Sparse but not ‘Grandmother-cell’ coding in the medial\ntemporal lobe.” As the authors emphasize there,\nGiven the very sparse and abstract representation of visual information by these neurons, they could in\nprinciple be considered as ‘grandmother cells’. However, we give several arguments that make such an\nextreme interpretation unlikely.\n. . .\nMTL neurons are situated at the juncture of transformation of percepts into constructs that can be\nconsciously recollected. These cells respond to percepts rather than to the detailed information falling\non the retina. Thus, their activity reflects the full transformation that visual information undergoes\nthrough the ventral pathway. A crucial aspect of this transformation is the complementary development\nof both selectivity and invariance. The evidence presented here, obtained from recordings of single-neuron\nactivity in humans, suggests that a subset of MTL neurons possesses a striking invariant representation\nfor consciously perceived objects, responding to abstract concepts rather than more basic metric details.\nThis representation is sparse, in the sense that responsive neurons fire only to very few stimuli (and are\nmostly silent except for their preferred stimuli), but it is far from a Grandmother-cell representation.\nThe fact that the MTL represents conscious abstract information in such a sparse and invariant way is\nconsistent with its prominent role in the consolidation of long-term semantic memories.\nIt’s interesting to note how inadequate the [QKKF08] data really is for exploring the notion\nof glocal memory in the brain. Suppose it’s the case that individual visual memories correspond\nto keys consisting of small neuronal subnetworks, and maps consisting of larger neuronal\nsubnetworks. Then it would be not at all surprising if neurons in the “key” network corresponding\nto a visual concept like “Bill Clinton’s face” would be found to respond differentially\nto the presentation of appropriate images. Yet, it would also be wrong to overinterpret such\ndata as implying that the key network somehow comprises the “representation” of Bill Clinton’s\nface in the individual’s brain. In fact this key network would comprise only one aspect of said\nrepresentation.\nIn the glocal memory hypothesis, a visual memory like “Bill Clinton’s face” would be hypothesized\nto correspond to an attractor spanning a significant subnetwork of the individual’s brain\n13.6 Glocal Memory 265\n– but this subnetwork still might occupy only a small fraction of the neurons in the brain (say,\n1/100 or less), since there are very many neurons available. This attractor would constitute the\nmap. But then, there would be a much smaller number of neurons serving as key to unlock\nthis map: i.e. if a few of these key neurons were stimulated, then the overall attractor pattern\nin the map as a whole would unfold and come to play a significant role in the overall brain\nactivity landscape. In prior publications [Goe97] the primary author explored this hypothesis\nin more detail in terms of the known architecture of the cortex and the mathematics of complex\ndynamical attractors.\nSo, one possible interpretation of the [QKKF08] data is that the MTL neurons they’re\nmeasuring are part of key networks that correspond to broader map networks recording percepts.\nThe map networks might then extend more broadly throughout the brain, beyond the MTL\nand into other perceptual and cognitive areas of cortex. Furthermore, in this case, if some MTL\nkey neurons were removed, the maps might well regenerate the missing keys (as would happen\ne.g. in the glocal Hopfield model to be discussed in the following section).\nRelated and interesting evidence for glocal memory in the brain comes from a recent study of\nsemantic memory, illustrated in Figure ?? [PNR07]. Their research probed the architecture of\nsemantic memory via comparing patients suffering from semantic dementia (SD) with patients\nsuffering from three other neuropathologies, and found reasonably convincing evidence for what\nthey call a “distributed-plus-hub” view of memory.\nThe SD patients they studied displayed highly distinctive symptomology; for instance, their\nvocabularies and knowledge of the properties of everyday objects were strongly impaired,\nwhereas their memories of recent events and other cognitive capacities remain perfectly intact.\nThese patients also showed highly distinctive patterns of brain damage: focal brain lesions\nin their anterior temporal lobes (ATL), unlike the other patients who had either less severe or\nmore widely distributed damage in their ATLs. This led [PNR07] to conclude that the ATL\n(being adjacent to the amygdala and limbic systems that process reward and emotion; and the\nanterior parts of the medial temporal lobe memory system, which processes episodic memory)\nis a “hub” for amodal semantic memory, drawing general semantic information from episodic\nmemories based on emotional salience.\nSo, in this view, the memory of something like a “banana” would contain a distributed aspect,\nspanning multiple brain systems, and also a localized aspect, centralized in the ATL.\nThe distributed aspect would likely contain information on various particular aspects of bananas,\nincluding their sights, smells, and touches, the emotions they evoke, and the goals and\nmotivations they relate to. The distributed and localized aspects would influence one another\ndynamically, but, the data [PNR07] gathered do not address dynamics and they don’t venture\nhypotheses in this direction.\nThere is a relationship between the “distributed-plus-hub” view and [Dam00] better-known\nnotion of a “convergence zone”, defined roughly as a location where the brain binds features together.\nA convergence zone, in [Dam00] perspective, is not a “store” of information but an agent\ncapable of decoding a signal (and of reconstructing information). He also uses the metaphor\nthat convergence zones behave like indexes drawing information from other areas of the brain –\nbut they are dynamic rather than static indices, containing the instructions needed to recognize\nand combine the features constituting the memory of something. The mechanism involved in\nthe distributed-plus-hub model is similar to a convergence zone, but with the important difference\nthat hubs are less local: [PNR07] semantic hub may be thought of a kind of “cluster of\nconvergence zones” consisting of a network of convergence zones for various semantic memories.\n266 13 Local, Global and Glocal Knowledge Representation\nFig. 13.1: A Simplified Look at Feedback-Control in Uncertain Inference\nWhat is missing in [PNR07] and [Dam00] perspective is a vision of distributed memories\nas attractors. The idea of localized memories serving as indices into distributed knowledge\nstores is important, but is only half the picture of glocal memory: the creative, constructive,\ndynamical-attractor aspect of the distributed representation is the other half. The closest thing\nto a clear depiction of this aspect of glocal memory that seems to exist in the neuroscience\nliterature is a portion of William Calvin’s theory of the “cerebral code” [Cal96]. Calvin proposes\na set of quite specific mechanisms by which knowledge may be represented in the brain\nusing complexly-structured strange attractors, and by which these strange attractors may be\npropagated throughout the brain. Figure 13.2 shows one aspect of his theory: how a distributed\nattractor may propagate from one part of the brain to another in pieces, with one portion of\nthe attractor getting propagated first, and then seeding the formation in the destination brain\nregion of a close approximation of the whole attractor.\nCalvin’s theory may be considered a genuinely glocal theory of memory. However, it also\nmakes a large number of other specific commitments that are not part of the notion of glocality,\nsuch as his proposal of hexagonal meta-columns in the cortex, and his commitment to\nevolutionary learning as the primary driver of neural knowledge creation. We find these other\n13.6 Glocal Memory 267\nFig. 13.2: Calvin’s Model of Distributed Attractors in the Brain\nhypotheses interesting and highly promising, yet feel it is also important to separate out the\nnotion of glocal memory for separate consideration.\nRegarding specifics, our suggestion is that Calvin’s approach may overemphasize the distributed\naspect of memory, not giving sufficient due to the relatively localized aspect as accounted\nfor in the [QKKF08] results discussed above. In Calvin’s glocal approach, global memories\nare attractors and local memories are parts of attractors. We suggest a possible alternative,\nin which global memories are attractors and local memories are particular neuronal subnetworks\nsuch as the specialized ones identified by [QKKF08]. However, this alternative does not seem\ncontradictory to Calvin’s overall conceptual approach, even though it is different from the particular\nproposals made in [Cal96].\nThe above paragraphs are far from a complete survey of the relevant neuroscience literature;\nthere are literally dozens of studies one could survey pointing toward the glocality of various\nsorts of human memory. Yet experimental neuroscience tools are still relatively primitive, and\nevery one of these studies could be interpreted in various other ways. In the next couple decades,\nas neuroscience tools improve in accuracy, our understanding of the role of glocality in human\nmemory will doubtless improve tremendously.\n268 13 Local, Global and Glocal Knowledge Representation\n13.6.3 Glocal Hopfield Networks\nThe ideas in the previous section suggest that, if one wishes to construct an AGI, it is worth\nseriously considering using a memory with some sort of glocal structure. One research direction\nthat follows naturally from this notion is “glocal neural networks.” In order to explore the nature\nof glocal neural networks in a relatively simple and tractable setting, we have formalized and\nimplemented simple examples of “glocal Hopfield networks”: palimpsest Hopfield nets with the\naddition of neurons representing localized memories. While these specific networks are not used\nin CogPrime, they are quite similar to the ECAN networks that are used in CogPrime and\ndescribed in Chapter 23 of Part 2.\nEssentially, we augment the standard Hopfield net architecture by adding a set of “key\nneurons.” These are a small percentage of the neurons in the network, and are intended to be\nroughly equinumerous to the number of memories the network is supposed to store. When the\nHopfield net converges to an attractor A, then new links are created between the neurons that\nare active in A, and one of the key neurons. Which key neuron is chosen? The one that, when\nit is stimulated, gives rise to an attractor pattern maximally similar to A.\nThe ultimate result of this is that, in addition to the distributed memory of attractors in the\nHopfield net, one has a set of key neurons that in effect index the attractors. Each attractor\ncorresponds to a single key neuron. In the glocal memory model, the key neurons are the keys\nand the Hopfield net attractors are the maps.\nThis algorithm has been tested in sparse Hopfield nets, using both standard Hopfield net\nlearning rules and Storkey’s modified palimpsest learning rule [SV99], which provides greater\nmemory capacity in a continuous learning context. The use of key neurons turns out to slightly\nincrease Hopfield net memory capacity, but this isn’t the main point. The main point is that\none now has a local representation of each global memory, so that if one wants to create a\nlink between the memory and something else, it’s extremely easy to do so – one just needs\nto link to the corresponding key neuron. Or, rather, one of the corresponding key neurons:\ndepending on how many key neurons are allocated, one might end up with a number of key\nneurons corresponding to each memory, not just one.\nIn order to transform a palimpsest Hopfield net into a glocal Hopfield net, the following steps\nare taken:\n1. Add a fixed number of “key neurons” to the network (removing other random neurons to\nkeep the total number of neurons constant)\n2. When the network reaches an attractor, create links from the elements in the attractor to\none of the key neurons\n3. The key neuron chosen for the previous step is the one that most closely matches the current\nattractor (which may be determined in several ways, to be discussed below)\n4. To avoid the increase of the number of links in the network, when new links are created in\nStep 2, other key-neuron links are then deleted (several approaches may be taken here, but\nthe simplest is to remove the key-neuron links with the lowest-absolute-value weights)\nIn the simple implementation of the above steps that we implemented, and described in\n[GPI + 10], Step 3 is carried out simply by comparing the weights of a key neuron’s links to the\nnodes in an attractor. A more sophisticated approach would be to select the key neuron with\nthe highest activation during the transient interval immediately prior to convergence to the\nattractor.\n13.6 Glocal Memory 269\nThe result of these modifications to the ordinary Hopfield net, is a Hopfield net that continually\nmaintains a set of key neurons, each of which individually represents a certain attractor\nof the net.\nNote that these key neurons – in spite of being “symbolic” in nature – are learned rather\nthan preprogrammed, and are every bit as adaptive as the attractors they correspond to. Furthermore,\nif a key neuron is removed, the glocal Hopfield net algorithm will eventually learn it\nback, so the robustness properties of Hopfield nets are retained.\nThe results of experimenting with glocal Hopfield nets of this nature are summarized in\n[GPI + 10]. We studied Hopfield nets with connectivity around .1, and in this context we found\nthat glocality\n• slightly increased memory capacity\n• massively increased the rate of convergence to the attractor, i.e. the speed of recall\nHowever, probably the most important consequence of glocality is a more qualitative one: it\nmakes it far easier to link the Hopfield net into a larger system, as would occur if the Hopfield net\nwere embedded in an integrative AGI architecture. Because a neuron external to the Hopfield\nnet may now link to a memory in the Hopfield net by linking to the corresponding key neuron.\n13.6.4 Neural-Symbolic Glocality in CogPrime\nIn CogPrime, we have explicitly sought to span the symbolic/emergentist pseudo-dichotomy,\nvia creating an integrative knowledge representation that combines logic-based aspects with\nneural-net-like aspects. As reviewed in Chapter 6 above, these function not in the manner of\nmultimodular systems, but rather via using (probabilistic) truth values and (attractor neural\nnet like) attention values as weights on nodes and links of the same (hyper) graph. The nodes\nand links in this hypergraph are typed, like a standard semantic network approach for knowledge\nrepresentation, so they’re able to handle all sorts of knowledge, from the most concrete\nperception and actuation related knowledge to the most abstract relationships. But they’re also\nweighted with values similar to neural net weights, and pass around quantities (importance\nvalues, discussed in Chapter 23 of Part 2) similar to neural net activations, allowing emergent\nattractor/assembly based knowledge representation similar to attractor neural nets.\nThe concept of glocality lies at the heart of this combination, in a way that spans the pseudodichotomy:\n• Local knowledge is represented in abstract logical relationships stored in explicit logical\nform, and also in Hebbian-type associations between nodes and links.\n• Global knowledge is represented in large-scale patterns of node and link weights, which\nlead to large-scale patterns of network activity, which often take the form of attractors\nqualitatively similar to Hopfield net attractors. These attractors are called maps.\nThe result of all this is that a concept like “cat” might be represented as a combination of:\n• A small number of logical relationships and strong associations, that constitute the “key”\nsubnetwork for the “cat” concept.\n• A large network of weak associations, binding together various nodes and links of various\ntypes and various levels of abstraction, representing the “cat map”.\n270 13 Local, Global and Glocal Knowledge Representation\nThe activation of the key will generally cause the activation of the map, and the activation of\na significant percentage of the map will cause the activation of the rest of the map, including the\nkey. Furthermore, if the key were for some reason forgotten, then after a significant amount of\neffort, the system would likely to be able to reconstitute it (perhaps with various small changes)\nfrom the information in the map. We conjecture that this particular kind of glocal memory will\nturn out to be very powerful for AGI, due to its ability to combine the strengths of formal\nlogical inference with those of self-organizing attractor neural networks.\nAs a simple example, consider the representation of a “tower”, in the context of an artificial\nagent that has built towers of blocks, and seen pictures of many other kinds of towers, and seen\nsome tall building that it knows are somewhat like towers but perhaps not exactly towers. If\nthis agent is reasonably conceptually advanced (say, at Piagetan the concrete operational level)\nthen its mind will contain some declarative relationships partially characterizing the concept of\n“tower,” as well as its sensory and episodic examples, and its procedural knowledge about how\nto build towers.\nThe key of the “tower” concept in the agent’s mind may consist of internal images and\nepisodes regarding the towers it knows best, the essential operations it knows are useful for\nbuilding towers (piling blocks atop blocks atop blocks...), and the core declarative relations\nsummarizing “towerness” – and the whole “tower” map then consists of a much larger number\nof images, episodes, procedures and declarative relationships connected to “tower” and other\nrelated entities. If any portion of the map is removed – even if the key is removed – then the\nrest of the map can be approximately reconstituted, after some work. Some cognitive operations\nare best done on the localized representation – e.g. logical reasoning. Other operations, such as\nattention allocation and guidance of inference control, are best done using the globalized map\nrepresentation.\nChapter 14\nRepresenting Implicit Knowledge via Hypergraphs\n14.1 Introduction\nExplicit knowledge is easy to write about and talk about; implicit knowledge is equally important,\nbut tends to get less attention in discussions of AI and psychology, simply because we don’t\nhave as good a vocabulary for describing it, nor as good a collection of methods for measuring\nit. One way to deal with this problem is to describe implicit knowledge using language and\nmethods typically reserved for explicit knowledge. This might seem intrinsically non-workable,\nbut we argue that it actually makes a lot of sense. The same sort of networks that a system like\nCogPrime uses to represent knowledge explicitly, can also be used to represent the emergent\nknowledge that implicitly exists in an intelligent system’s complex structures and dynamics.\nWe’ve noted that CogPrime uses an explicit representation of knowledge in terms of weighted\nlabeled hypergraphs; and also uses other more neural net like mechanisms (e.g. the economic\nattention allocation network subsystem) to represent knowledge globally and implicitly. Cog-\nPrime combines these two sorts of representation according to the principle we have called\nglocality. In this chapter we pursue glocality a bit further – describing a means by which even\nimplicitly represented knowledge can be modeled using weighted labeled hypergraphs similar to\nthe ones used explicitly in CogPrime. This is conceptually important, in terms of making clear\nthe fundamental similarities and differences between implicit and explicit knowledge representation;\nand it is also pragmatically meaningful due to its relevance to the CogPrime methods\ndescribed in Chapter 42 of Part 2 that transform implicit into explicit knowledge.\nTo avoid confusion with CogPrime’s explicit knowledge representation, we will refer to the\nhypergraphs in this chapter as composed of Vertices and Edges rather than Nodes and Links. In\nprior publications we have referred to \"derived\" or \"emergent\" hypergraphs of the sort described\nhere using the acronym SMEPH, which stands for Self-Modifying, Evolving Probabilistic Hypergraphs.\n14.2 Key Vertex and Edge Types\nWe begin by introducing a particular collection of Vertex and Edge types, to be used in modeling\nthe internal structures of intelligent systems.\nThe key SMEPH Vertex types are\n271\n272 14 Representing Implicit Knowledge via Hypergraphs\n• ConceptVertex, representing a set, for instance, an idea or a set of percepts\n• SchemaVertex, representing a procedure for doing something (perhaps something in the\nphysical world, or perhaps an abstract mental action).\nThe key SMEPH Edge types, using language drawn from Probabilistic Logic Networks (PLN)\nand elaborated in Chapter 34 below, are as follows:\n• ExtensionalInheritanceEdge (ExtInhEdge for short: an edge which, linking one Vertex or\nEdge to another, indicates that the former is a special case of the latter)\n• ExtensionalSimilarityEdge (ExtSim: which indicates that one Vertex or Edge is similar to\nanother)\n• ExecutionEdge (a ternary edge, which joins S,B,C when S is a SchemaVertex and the result\nfrom applying S to B is C).\nSo, in a SMEPH system, one is often looking at hypergraphs whose Vertices represent ideas or\nprocedures, and whose Edges represent relationships of specialization, similarity or transformation\namong ideas and/or procedures.\nThe semantics of the SMEPH edge types is given by PLN, but is simple and commonsensical.\nExtInh and ExtSim Edges come with probabilistic weights indicating the extent of\nthe relationship they denote (e.g. the ExtSimEdge joining the cat ConceptVertex to the dog\nConceptVertex gets a higher probability weight than the one joining the cat ConceptVertex\nto the washing-machine ConceptVertex). The mathematics of transformations involving these\nprobabilistic weights becomes quite involved - particularly when one introduces SchemaVertices\ncorresponding to abstract mathematical operations, a step that enables SMEPH hypergraphs\nto have the complete mathematical power of standard logical formalisms like predicate calculus,\nbut with the added advantage of a natural representation of uncertainty in terms of\nprobabilities, as well as a natural representation of networks and webs of complex knowledge.\n14.3 Derived Hypergraphs\nWe now describe how SMEPH hypergraphs may be used to model and describe intelligent\nsystems. One can (in principle) draw a SMEPH hypergraph corresponding to any individual\nintelligent system, with Vertices and Edges for the concepts and processes in that system’s\nmind. This is called the derived hypergraph of that system.\n14.3.1 SMEPH Vertices\nA ConceptVertex in the derived hypergraph of a system corresponds to a structural pattern\nthat persists over time in that system; whereas a SchemaVertex corresponds to a multi-timepoint\ndynamical pattern that recurs in that system’s dynamics. If one accepts the patternist\ndefinition of a mind as the set of patterns in an intelligent system, then it follows that the\nderived hypergraph of an intelligent system captures a significant fraction of the mind of that\nsystem.\nTo phrase it a little differently, we may say that a ConceptVertex, in SMEPH, refers to the\nhabitual pattern of activity observed in a system when some condition is met (this condition\n14.3 Derived Hypergraphs 273\ncorresponding to the presence of a certain pattern). The condition may refer to something in\nthe world external to the system, or to something internal. For instance, the condition may be\nobserving a cat. In this case, the corresponding Concept vertex in the mind of Ben Goertzel\nis the pattern of activity observed in Ben Goertzel’s brain when his eyes are open and he’s\nlooking in the direction of a cat. The notion of pattern of activity can be made rigorous using\nmathematical pattern theory, as is described in The Hidden Pattern [Goe06a].\nNote that logical predicates, on the SMEPH level, appear as particular kinds of Concepts,\nwhere the condition involves a predicate and an argument. For instance, suppose one wants to\nknow what happens inside Ben’s mind when he eats cheese. Then there is a Concept corresponding\nto the condition of cheese-eating activity. But there may also be a Concept corresponding\nto eating activity in general. If the Concept denoting the activity of eating X is generally easily\ncomputable from the Concepts for X and eating individually, then the eating Concept is\neffectively acting as a predicate.\nA SMEPH SchemaVertex, on the other hand, is like a Concept that’s defined in a timedependent\nway. One type of Schema refers to a habitual dynamical pattern of activity occurring\nbefore and/or during some condition is met. For instance, the condition might be saying the\nword Hello. In that case the corresponding SchemaVertex in the mind of Ben Goertzel is the\npattern of activity that generally occurs before he says Hello.\nAnother type of Schema refers to a habitual dynamical pattern of activity occurring after\nsome condition X is met. For instance, in the case of the Schema for adding two numbers, the\nprecondition X consists of the two numbers and the concept of addition. The Schema is then\nwhat happens when the mind thinks of adding and thinks of two numbers.\nFinally, there are Schema that refer to habitual dynamical activity patterns occurring after\nsome condition X is met and before some condition Y is met. In this case the Schema is viewed\nas transforming X into Y. For instance, if X is the condition of meeting someone who is not a\nfriend, and Y is the condition of being friends with that person, then the habitually intervening\nactivities constitute the Schema for making friends.\n14.3.2 SMEPH Edges\nSMEPH edge types fall into two categories: functional and logical. Functional edges connect\nSchema vertices to their input and outputs; logical edges refer mainly to conditional probabilities,\nand in general are to be interpreted according to the semantics of Probabilistic Logic\nNetworks.\nLet us begin with logical edges. The simplest case is the Subset edge, which denotes a\nstraightforward, extensional conditional probability. For instance, it may happen that whenever\nthe Concept for cat is present in a system, the Concept for animal is as well. Then we would\nsay\nSubset cat animal\n(Here we assume a notation where “R A B” denotes an Edge of type R between Vertices A and\nB.)\nOn the other hand, it may be that 50% of the time that cat is present in the system, cute is\npresent as well: then we would say\nSubset cat cute <.5>\n274 14 Representing Implicit Knowledge via Hypergraphs\nwhere the <.5> denotes the probability, which is a component of the Truth Value associated\nwith the edge.\nNext, the most basic functional edge is the Execution edge, which is ternary and denotes a\nrelation between a Schema, its input and its output, e.g.\nExecution father_of Ben_Goertzel Ted_Goertzel\nfor a schema father_of that outputs the father of its argument.\nThe ExecutionOutput (ExOut) edge denotes the output of a Schema in an implicit way, e.g.\nExOut say_hello\nrefers to a particular act of saying hello, whereas\nExOut add_numbers {3, 4)\nrefers to the Concept corresponding to 7. Note that this latter example involves a set of three\nentities: sets are also part of the basic SMEPH knowledge representation. A set may be thought\nof as a hypergraph edge that points to all its members.\nIn this manner we may define a set of edges and vertices modeling the habitual activity\npatterns of a system when in different situations. This is called the derived hypergraph of the\nsystem. Note that this hypergraph can in principle be constructed no matter what happens\ninside the system: whether it’s a human brain, a formal neural network, Cyc, OCP, a quantum\ncomputer, etc. Of course, constructing the hypergraph in practice is quite a different story: for\ninstance, we currently have no accurate way of measuring the habitual activity patterns inside\nthe human brain. fMRI and PET and other neuroimaging technologies give only a crude view,\nthough they are continually improving.\nPattern theory enters more deeply here when one thoroughly fleshes out the Inheritance\nconcept. Philosophers of logic have extensively debated the relationship between extensional\ninheritance (inheritance between sets based on their members) and intensional inheritance (inheritance\nbetween entity-types based on their properties). A variety of formal mechanisms have\nbeen proposed to capture this conceptual distinction; see (Wang, 2006, 1995 TODO make ref)\nfor a review along with a novel approach utilizing uncertain term logic. Pattern theory provides\na novel approach to defining intension: one may associate with each ConceptVertex in a system’s\nderived hypergraph the set of patterns associated with the structural pattern underlying that\nConceptVertex. Then, one can define the strength of the IntensionalInheritanceEdge between\ntwo ConceptVertices A and B as the percentage of A’s pattern-set that is also contained in B’s\npattern-set. According to this approach, for instance, one could have\nIntInhEdge whale fish <0.6>\nExtInhEdge whale fish <0.0>\nsince the fish and whale sets have common properties but no common members.\n14.4 Implications of Patternist Philosophy for Derived Hypergraphs\nof Intelligent Systems\nPatternist philosophy rears its head here and makes some definite hypotheses about the structure\nof derived hypergraphs. It suggests that derived hypergraphs should have a dual network\n14.4 Implications of Patternist Philosophy for Derived Hypergraphs of Intelligent Systems 275\nstructure, and that in highly intelligent systems they should have subgraphs that constitute\nmodels of the whole hypergraph (these are self systems). SMEPH does not add anything to\nthe patternist view on a philosophical level, but it gives a concrete instantiation to some of the\ngeneral ideas of patternism. In this section we’ll articulate some \"SMEPH principles\", constituting\nimportant ideas from patternist philosophy as they manifest themselves in the SMEPH\ncontext.\nThe logical edges in a SMEPH hypergraph are weighted with probabilities, as in the simple\nexample given above. The functional edges may be probabilistically weighted as well, since some\nSchema may give certain results only some of the time. These probabilities are critical in terms\nof SMEPH’s model of system dynamics; they underly one of our SMEPH principles,\nPrinciple of Implicit Probabilistic Inference: In an intelligent system, the temporal\nevolution of the probabilities on the edges in the system’s derived hypergraph should approximately\nobey the rules of probability theory.\nThe basic idea is that, even if a system - through its underlying dynamics - has no explicit\nconnection to probability theory, it still must behave roughly as if it does, if it is going to be\nintelligent. The roughly part is important here; it’s well known that humans are not terribly\naccurate in explicitly carrying out formal probabilistic inferences. And yet, in practical contexts\nwhere they have experience, humans can make quite accurate judgments; which is all that’s\nrequired by the above principle, since it’s the contexts where experience has occurred that will\nmake up a system’s derived hypergraph.\nOur next SMEPH principle is evolutionary, and states\nPrinciple of Implicit Evolution: In an intelligent system, new Schema and Concepts will\ncontinually be created, and the Schema and Concepts that are more useful for achieving system\ngoals (as demonstrated via probabilistic implication of goal achievement) will tend to survive\nlonger.\nNote that this principle can be fulfilled in many different ways. The important thing is that\nsystem goals are allowed to serve as a selective force.\nAnother SMEPH dynamical principle pertains to a shorter time-scale than evolution, and\nstates\nPrinciple of Attention Allocation: In an intelligent system, Schema and Concepts that\nare more useful for attaining short-term goals will tend to consume more of the system’s energy.\n(The balance of attention oriented toward goals pertaining to different time scales will vary from\nsystem to system.)\nNext, there is the\nPrinciple of Autopoesis: In an intelligent system, if one removes some part of the system\nand then allows the system’s natural dynamics to keep going, a decent approximation to that\nremoved part will often be spontaneously reconstituted.\nAnd there is the\n276 14 Representing Implicit Knowledge via Hypergraphs\nCognitive Equation Principle: In an intelligent system, many abstract patterns that are\npresent in the system at a certain time as patterns among other Schema and Concepts, will at\na near-future time be present in the system as patterns among elementary system components.\nThe Cognitive Equation Principle, briefly discussed in Chapter 3, basically means that Concepts\nand Schema emergent in the system are recognized by the system and then embodied\nas elementary items in the system so that patterns among them in their emergent form become,\nwith the passage of time, patterns among them in their directly-system-embodied form.\nThis is a natural consequence of the way intelligent systems continually recognize patterns in\nthemselves.\nNote that derived hypergraphs may be constructed corresponding to any complex system\nwhich demonstrates a variety of internal dynamical patterns depending on its situation. However,\nif a system is not intelligent, then according to the patternist philosophy evolution of its\nderived hypergraph can’t necessarily be expected to follow the above principles.\n14.4.1 SMEPH Principles in CogPrime\nWe now more explicitly elaborate the application of these ideas in the CogPrime context. As\nnoted above, in addition to explicit knowledge representation in terms of Nodes and Links,\nCogPrime also incorporates implicit knowledge representation in the form of what are called\nMaps: collections of Nodes and Links that tend to be utilized together within cognitive processes.\nThese Maps constitute a CogPrime system’s derived hypergraph, which will not be identical\nto the hypergraph it uses for explicit knowledge representation. However, an interesting\nfeedback loop arises here, in that the intelligence’s self-study will generally lead it to recognize\nlarge portions of its derived hypergraph as patterns in itself, and then embody these patterns\nwithin its concretely implemented knowledge hypergraph. This relates to the Cognitive Equation\nPrinciple defined above 3, in which an intelligent system continually recognizes patterns in\nitself and embodies these patterns in its own basic structure (so that new patterns may more\neasily emerge from them).\nOften it happens that a particular CogPrime node will serve as the center of a map, so that\ne.g. the Concept Link denoting cat will consist of a number of nodes and links roughly centered\naround a ConceptNode that is linked to the WordNode cat. But this is not guaranteed and\nsome CogPrime maps are more diffuse than this with no particular center.\nSomewhat similarly, the key SMEPH dynamics are represented explicitly in CogPrime: probabilistic\nreasoning is carried out via explicit application of PLN on the CogPrime hypergraph,\nevolutionary learning is carried out via application of the MOSES optimization algorithm, and\nattention allocation is carried out via a combination of inference and evolutionary pattern mining.\nBut the SMEPH dynamics also occur implicitly in CogPrime: emergent maps are reasoned\non probabilistically as an indirect consequence of node-and-link level PLN activity; maps evolve\nas a consequence of the coordinated whole of CogPrime dynamics; and attention shifts between\nmaps according to complex emergent dynamics.\nTo see the need for maps, consider that even a Node that has a particular meaning attached\nto it - like the Iraq Node, say - doesn’t contain much of the meaning of Iraq in it. The meaning\nof Iraq lies in the Links attached to this Node, and the Links attached to their Nodes - and\nthe other Nodes and Links not explicitly represented in the system, which will be created by\n14.4 Implications of Patternist Philosophy for Derived Hypergraphs of Intelligent Systems 277\nCogPrime’s cognitive algorithms based on the explicitly existent Nodes and Links related to\nthe Iraq Node.\nThis halo of Atoms related to the Iraq node is called the Iraq map. In general, some maps\nwill center around a particular Atom, like this Iraq map, others may not have any particular\nidentifiable center. CogPrime’s cognitive processes act directly on the level of Nodes and Links,\nbut they must be analyzed in terms of their impact on maps as well. In SMEPH terms, Cog-\nPrime maps may be said to correspond to SMEPH ConceptNodes, and for instance bundles of\nLinks between the Nodes belonging to a map may correspond to a SMEPH Link between two\nConceptNodes.\n\nChapter 15\nEmergent Networks of Intelligence\n15.1 Introduction\nWhen one is involved with engineering an AGI system, one thinks a lot about the aspects of\nthe system one is explicitly building – what are the parts, how they fit together, how to test\nthey’re properly working, and so forth. And yet, these explicitly engineered aspects are only a\nfraction of what’s important in an AGI system. At least as critical are the emergent aspects –\nthe patterns that emerge once the system is up and running, interacting with the world and\nother agents, growing and developing and learning and self-modifying. SMEPH is one toolkit\nfor describing some of these emergent patterns, but it’s only a start.\nIn line with these general observations, most of this book will focus on the structures and\nprocesses that we have built, or intend to build, into the CogPrime system. But in a sense, these\nstructures and processes are not the crux of CogPrime’s intended intelligence. The purpose\nof these pre-programmed structures and processes is to give rise to emergent structures and\nprocesses, in the course of CogPrime’s interaction with the world and the other minds within\nit. We will return to this theme of emergence at several points in later chapters, e.g. in the\ndiscussion of map formation in Chapter 42 of Part 2.\nGiven the important of emergent structures – and specifically emergent network structures –\nfor intelligence, it’s fortunate the scientific community has already generated a lot of knowledge\nabout complex networks: both networks of physical or software elements, and networks of\norganization emergent from complex systems. As most of this knowledge has originated in\nfields other than AGI, or in pure mathematics, it tends to require some reinterpretation or\ntweaking to achieve maximal applicability in the AGI context; but we believe this effort will\nbecome increasingly worthwhile as the AGI field progresses, because network theory is likely\nto be very useful for describing the contents and interactions of AGI systems as they develop\nincreasing intelligence.\nIn this brief chapter we specifically focus on the emergence of certain large-scale network\nstructures in a CogPrime knowledge store, presenting heuristic arguments as to why these\nstructures can be expected to arise. We also comment on the way in which these emergent\nstructures are expected to guide cognitive processes, and give rise to emergent cognitive processes.\nThe following chapter expands on this theme in a particular direction, exploring the\npossible emergence of structures characterizing inter-cognitive reflection.\n279\n280 15 Emergent Networks of Intelligence\n15.2 Small World Networks\nOne simple but potentially useful observation about CogPrime Atomspaces is that they are\ngenerally going to be small world networks [Buc03], rather than random graphs. A small world\nnetwork is a graph in which the connectivities of the various nodes display a power law behavior\n– so that, loosely speaking, there are a few nodes with very many links, then more nodes with a\nmodest number of links ... and finally, a huge number of nodes with very few links. This kind of\nnetwork occurs in many natural and human systems, including citations among papers, financial\narrangements among banks, links between Web pages and the spread of diseases among people\nor animals. In a weighted network like an Atomspace, \"small-world-ness\" must be defined in a\nmanner taking the weights into account, and there are several obvious ways to do this. Figure\n15.1 depicts a small but prototypical small-worlds network, with a few \"hub\" nodes possessing\nfar more neighbors than the others, and then some secondary hubs, etc.\nAn excellent reference on network theory in general, including but not limited to small world\nnetworks, is Peter Csermely’s Weak Links [Cse06]. Many of the ideas in that work have apparent\nOpenCog applications, which are not elaborated here.\nFig. 15.1: A typical, though small-sized, small-worlds network.\nOne process via which small world networks commonly form is \"preferential attachment\"\n[Bar02]. This occurs in essence when \"the rich get richer\" – i.e. when nodes in the network\ngrow new links, in a manner that causes them to preferentially grow links to nodes that already\nhave more links. It is not hard to see that CogPrime’s ECAN dynamics will naturally lead to\n15.3 Dual Network Structure 281\npreferential attachment, because Atoms with more links will tend to get more STI, and thus\nwill tend to get selected by more cognitive processes, which will cause them to grow more\nlinks. For this reason, in most circumstances, a CogPrime system in which most link-building\ncognitive processes rely heavily on ECAN to guide their activities will tend to contain a smallworld-network\nAtomspace. This is not rigorously guaranteed to be the case for any possible\ncombination of environment and goals, but it is commonsensically likely to nearly always be\nthe case.\nOne consequence of the small worlds structure of the Atomspace is that, in exploring other\nproperties of the Atom network, it is particularly important to look at the hub nodes. For\ninstance, if one is studying whether hierarchical and heterarchical subnetworks of the Atomspace\nexist, and whether they are well-aligned with each other, it is important to look at hierarchical\nand heterarchical connections between hub nodes in particular (and secondary hubs, etc.). A\npattern of hierarchical or dual network connection that only held up among the more sparsely\nconnected nodes in a small-world network would be a strange thing, and perhaps not that\ncognitively useful.\n15.3 Dual Network Structure\nOne of the key theoretical notions in patternist philosophy is that complex cognitive systems\nevolve internal dual network structures, comprising superposed, harmonized hierarchical and\nheterarchical networks. Now we explore some of the specific CogPrime structures and dynamics\nmilitating in favor of the emergence of dual networks.\n15.3.1 Hierarchical Networks\nThe hierarchical nature of human linguistic concepts is well known, and is illustrated in Figure\n15.2 for the commonsense knowledge domain (using a graph drawn from WordNet, a huge concept\nhierarchy covering 50K+ English-language concepts), and in Figure 15.4 for a specialized\nknowledge subdomain, genetics. Due to this fact, a certain amount of hierarchy can be expected\nto emerge in the Atomspace of any linguistically savvy CogPrime, simply due to its modeling\nof the linguistic concepts that it hears and reads.\nHierarchy also exists in the natural world apart from language, which is the reason that many\nsensorimotor-knowledge-focused AGI systems (e.g. DeSTIN and HTM, mentioned in Chapter\n4 above) feature hierarchical structures. In these cases the hierarchies are normally spatiotemporal\nin nature - with lower layers containing elements responding to more localized aspects\nof the perceptual field, and smaller, more localized groups of actuators. This kind of hierarchy\ncertainly could emerge in an AGI system, but in CogPrime we have opted for a different route.\nIf a CogPrime system is hybridized with a hierarchical sensorimotor network like one of those\nmentioned above, then the Atoms linked to the nodes in the hierarchical sensorimotor network\nwill naturally possess hierarchical conceptual relationships, and will thus naturally grow hierarchical\nlinks between them (e.g. InheritanceLinks and IntensionalInheritanceLinks via PLN,\nAsymmetricHebbianLinks via ECAN).\n282 15 Emergent Networks of Intelligence\nFig. 15.2: A typical, though small, subnetwork of WordNet’s hierarchical network.\nOnce elements of hierarchical structure exist via the hierarchical structure of language and\nphysical reality, then a richer and broader hierarchy can be expected to accumulate on top\nof it, because importance spreading and inference control will implicitly and automatically be\nguided by the existing hierarchy. That is, in the language of Chaotic Logic [Goe94] and patternist\ntheory, hierarchical structure is an \"autopoietic attractor\" – once it’s there it will tend to enrich\nitself and maintain itself. AsymmetricHebbianLinks arranged in a hierarchy will tend to cause\nimportance to spread up or down the hierarchy, which will lead other cognitive processes to look\nfor patterns between Atoms and their hierarchical parents or children, thus potentially building\nmore hierarchical links. Chains of InheritanceLinks pointing up and down the hierarchy will lead\nPLN to search for more hierarchical links – e.g. most simply, A → B → C where C is above\nB is above A in the hierarchy, will naturally lead inference to check the viability of A → C\nby deduction. There is also the possibility to introduce a special DefaultInheritanceLink, as\ndiscussed in Chapter 34 of Part 2, but this isn’t actually necessary to obtain the inferential\nmaintenance of a robust hierarchical network.\n15.3.2 Associative, Heterarchical Networks\nHeterarchy is in essence a simpler structure than hierarchy: it simply refers to a network in\nwhich nodes are linked to other nodes with which they share important relationships. That is,\nthere should be a tendency that if two nodes are often important in the same contexts or for\n15.3 Dual Network Structure 283\nFig. 15.3: A typical, though small, subnetwork of the Gene Ontology’s hierarchical network.\nthe same purposes, they should be linked together. Portrayals of typical heterarchical linkage\npatterns among natural language concepts are given in Figures 15.5 and 15.6. Just for fun,\nFigure 15.7 shows one person’s attempt to draw a heterarchical graph of the main concepts\nin one of Douglas Hofstadter’s books. Naturally, real concept heterarchies are far more large,\ncomplex and tangled than even this one.\nIn CogPrime, ECAN enforces heterarchy via building SymmetricHebbianLinks, and PLN\nby building SimilarityLinks, IntensionalSimilarityLinks and ExtensionalSimilarityLinks. Furthermore,\nthese various link types reinforce each other. PLN control is guided by importance\nspreading, which follows Hebbian links, so that a heterarchical Hebbian network tends to cause\nPLN to explore the formation of links following the same paths as the heterarchical Hebbian-\nLinks. And importance can spread along logical links as well as explicit Hebbian links, so that\nthe existence of a heterarchical logical network will tend to cause the formation of additional\nheterarchical Hebbian links. Heterarchy reinforces itself in \"autopoietic attractor\" style even\nmore simply and directly than heterarchy.\n284 15 Emergent Networks of Intelligence\nFig. 15.4: Small-scale portrayal of a portion of the spatiotemporal hierarchy in Jeff Hawkins’\nHierarchical Temporal Memory architecture.\n15.3.3 Dual Networks\nFinally, if both hierarchical and heterarchical structures exist in an Atomspace, then both ECAN\nand PLN will naturally blend them together, because hierarchical and heterarchical links will\nfeed into their link-creation processes and naturally be combined together to form new links.\nThis will tend to produce a structure called a dual network, in which a hierarchy exists, along\nwith a rich network of heterarchical links joining nodes in the hierarchy, with a particular density\nof links between nodes on the same hierarchical level. The dual network structure will emerge\nwithout any explicit engineering oriented toward it, simply via the existence of hierarchical\nand heterarchical networks, and the propensity of ECAN and PLN to be guided by both the\nhierarchical and heterarchical networks. The existence of a natural dual network structure in\nboth linguistic and sensorimotor data will help the formation process along, and then creative\ncognition will enrich the dual network yet further than is directly necessitated by the external\nworld.\n15.3 Dual Network Structure 285\nFig. 15.5: Portions of a conceptual heterarchy centered on specific concepts.\nFig. 15.6: A portion of a conceptual heterarchy, showing the \"dangling links\" leading this portion\nto the rest of the heterarchy.\nA rigorous mathematical analysis of the formation of hierarchical, heterarchical and dual\nnetworks in CogPrime systems has not yet been undertaken, and would certainly be an interesting\nenterprise. Similar to the theory of small world networks, there is ample ground here\nfor both theorem-proving and heuristic experimentation. However, the qualitative points made\nhere are sufficiently well-grounded in intuition and experience to be of some use guiding our\n286 15 Emergent Networks of Intelligence\nFig. 15.7: A fanciful evocation of part of a reader’s conceptual heterarchy related to Douglas\nHofstadter’s writings.\nongoing work. One of the nice things about emergent network structures is that they are relatively\nstraightforward to observe in an evolving, learning AGI system, via visualization and\ninspection of structures such at the Atomspace.\nSection V\nA Path to Human-Level AGI\n\nChapter 16\nAGI Preschool\nCo-authored with Stephan Vladimir Bugaj\n16.1 Introduction\nIn conversations with government funding sources or narrow AI researchers about AGI work, one\nof the topics that comes up most often is that of “evaluation and metrics” – i.e., AGI intelligence\ntesting. We actually prefer to separate this into two topics: environments and methods for careful\nqualitative evaluation of AGI systems, versus metrics for precise measurement of AGI systems.\nThe difficulty of formulating bulletproof metrics for partial progress toward advanced AGI\nhas become evident throughout the field, and in Chapter 8 we have elaborated one plausible\nexplanation for this phenomenon, the \"trickiness\" of cognitive synergy. [LWML09], summarizing\na workshop on “Evaluation and Metrics for Human-Level AI” held in 2008, discusses some of\nthe general difficulties involved in this type of assessment, and some requirements that any\nviable approach must fulfill. On the other hand, the lack of appropriate methods for careful\nqualitative evaluation of AGI systems has been much less discussed, but we consider it actually\na more important issue – as well as an easier (though not easy) one to solve.\nWe haven’t actually found the lack of quantitative intelligence metrics to be a major obstacle\nin our practical AGI work so far. Our OpenCogPrime implementation lags far behind the\nCogPrime design as articulated in Part 2 of this book, and according to the theory underlying\nCogPrime, the more interesting behaviors and dynamics of the system will occur only when all\nthe parts of the system have been engineered to a reasonable level of completion and integrated\ntogether. So, the lack of a great set of metrics for evaluating the intelligence of our partiallybuilt\nsystem hasn’t impaired too much. Testing the intelligence of the current OpenCogPrime\nsystem is a bit like testing the flight capability of a partly-built airplane that only has stubs\nfor wings, lacks tail-fins, has a much less efficient engine than the one that’s been designed for\nuse in the first \"real\" version of the airplane, etc. There may be something to be learned from\nsuch preliminary tests, but making them highly rigorous isn’t a great use of effort, compared\nto working on finishing implementing the design according to the underlying theory.\nOn the other hand, the problem of what environments and methods to use to qualitatively\nevaluate and study AGI progress, has been considerably more vexing to us in practice, as\nwe’ve proceeded in our work on implementing and testing OpenCogPrime and developing the\nCogPrime theory. When developing a complex system, it’s nearly always valuable to see what\nthis system does in some fairly rich, complex situations, in order to gain a better intuitive\nunderstanding of the parts and how they work together. In the context of human-level AGI, the\ntheoretically best way to do this would be to embody one’s AGI system in a humanlike body\n289\n290 16 AGI Preschool\nand set it loose in the everyday human world; but of course, this isn’t feasible given the current\nstate of development of robotics technology. So one must seek approximations. Toward this end\nwe have embodied OpenCogPrime in non-player characters in video game style virtual worlds,\nand carried out preliminary experiments embodying OpenCogPrime in humanoid robots. These\nare reasonably good options but they have limitations and lead to subtle choices: what kind of\ngame characters and game worlds, what kind of robot environments, etc.?\nOne conclusion we have come to, based largely on the considerations in Chapter 11 on\ndevelopment and Chapter 9 on the importance of environment, is that it may make sense to\nembed early-stage proto-AGI and AGI systems in environments reminiscent of those used for\nteaching young human children. In this chapter we will explore this approach in some detail:\nemulation, in either physical reality or an multiuser online virtual world, of an environment\nsimilar to preschools used in early human childhood education. Complete specification of an\n“AGI Preschool” would require much more than a brief chapter; our goal here is to sketch the\nidea in broad outline, and give a few examples of the types of opportunities such an environment\nwould afford for instruction, spontaneous learning and formal and informal evaluation of certain\nsorts of early-stage AGI systems.\nThe material in this chapter will pop up fairly often later in the book. The AGI Preschool\ncontext will serve, throughout the following chapters, as a source of concrete examples of the\nvarious algorithms and structures. But it’s not proposed merely as an expository tool; we are\nmaking the very serious proposal that sending AGI systems to a virtual or robotic preschool is\nan excellent way – perhaps the best way – to foster the development of human-level human-like\nAGI.\n16.1.1 Contrast to Standard AI Evaluation Methodologies\nThe reader steeped in the current AI literature may wonder why it’s necessary to introduce a\nnew methodology and environment for evaluating AGI systems. There are already very many\ndifferent ways of evaluating AI systems out there ... do we really need another?\nCertainly, the AI field has inspired many competitions, each of which tests some particular\ntype or aspect of intelligent behavior. Examples include robot competitions, tournaments of\ncomputer chess, poker, backgammon and so forth at computer olympiads, trading-agent competition,\nlanguage and reasoning competitions like the Pascal Textual Entailment Challenge, and\nso on. In addition to these, there are many standard domains and problems used in the AI literature\nthat are meant to capture the essential difficulties in a certain class of learning problems:\nstandard datasets for face recognition, text parsing, supervised classification, theorem-proving,\nquestion-answering and so forth.\nHowever, the value of these sorts of tests for AGI is predicated on the hypothesis that the\ndegree of success of an AI program at carrying out some domain-specific task, is correlated\nwith the potential of that program for being developed into a robust AGI program with broad\nintelligence. If humanlike AGI and problem-area-specific “narrow AI” are in fact very different\nsorts of pursuits requiring very different principles, as we suspect, then these tests are not\nstrongly relevant to the AGI problem.\nThere are also some standard evaluation paradigms aimed at AI going beyond specific tasks.\nFor instance, there is a literature on “multitask learning\" and “transfer learning,” where the\ngoal for an AI is to learn one task quicker given another task solved previously [Car97, TM95,\n16.2 Elements of Preschool Design 291\nBDS03, TS07, RZDK05]. This is one of the capabilities an AI agent will need to simultaneously\nlearn different types of tasks as proposed in the Preschool scenario given here. And there is\na literature on “shaping,” where the idea is to build up the capability of an AI by training\nit on progressively more difficult versions of the same tasks [LD03]. Again, this is one sort of\ncapability an AI will need to possess if it is to move up some type of curriculum, such as a\nschool curriculum.\nWhile we applaud the work done on multitask learning and shaping, we feel that exploring\nthese processes using mathematical abstractions, or in the domain of various narrowlyproscribed\nmachine-learning or robotics test problems, may not adequately address the problem\nof AGI. The problem is that generalization among tasks, or from simpler to more difficult\nversions of the same task, is a process whose nature may depend strongly on the overall nature\nof the set of tasks and task-versions involved. Real-world tasks have a subtlety of interconnectedness\nand developmental course that is not captured in current mathematical learning\nframeworks nor standard AI test problems.\nTo put it mathematically, we suggest that the universe of real-world human tasks has a host\nof “special statistical properties” that have implications regarding what sorts of AI programs\nwill be most suitable; and that, while exploring and formalizing the nature of these statistical\nproperties is important, an easier and more reliable approach to AGI testing is to create a\ntesting environment that embodies these properties implicitly, via its being an emulation of the\ncognitively meaningful aspects of the real-world human learning environment.\nOne way to see this point vividly is to contrast the current proposal with the “General Game\nPlayer” AI competition, in which AIs seek to learn to play games based on formal descriptions\nof the rules. 1 . Clearly doing GGP well requires powerful AGI; and doing GGP even mediocrely\nprobably requires robust multitask learning and shaping. But we suspect GGP is far inferior to\nAGI Preschool as an approach to testing early-stage AI programs aimed at roughly humanlike\nintelligence. This is because, unlike the tasks involved in AI Preschool, the tasks involved in\ndoing simple instances of GGP seem to have little relationship to humanlike intelligence or\nreal-world human tasks.\n16.2 Elements of Preschool Design\nWhat we mean by an “AGI Preschool” is simply a porting to the AGI domain of the essential\naspects of human preschools. While there is significant variance among preschools there are also\nstrong commonalities, grounded in educational theory and experience. We will briefly discuss\nboth the physical design and educational curriculum of the typical human preschool, and which\naspects transfer effectively to the AGI context.\nOn the physical side, the key notion in modern preschool design is the “learning center,” an\narea designed and outfitted with appropriate materials for teaching a specific skill. Learning\ncenters are designed to encourage learning by doing, which greatly facilitates learning processes\nbased on reinforcement, imitation and correction (see Chapter 31 of Part 2 for a detailed discussion\nof the value of this combination); and also to provide multiple techniques for teaching\nthe same skills, to accommodate different learning styles and prevent over-fitting and overspecialization\nin the learning of new skills.\n1 http://games.stanford.edu/\n292 16 AGI Preschool\nCenters are also designed to cross-develop related skills. A “manipulatives center,” for example,\nprovides physical objects such as drawing implements, toys and puzzles, to facilitate\ndevelopment of motor manipulation, visual discrimination, and (through sequencing and classification\ngames) basic logical reasoning. A “dramatics center,” on the other hand, cross-trains\ninterpersonal and empathetic skills along with bodily-kinesthetic, linguistic, and musical skills.\nOther centers, such as art, reading, writing, science and math centers are also designed to train\nnot just one area, but to center around a primary intelligence type while also cross-developing\nrelated areas. For specific examples of the learning centers associated with particular contemporary\npreschools, see [Nie98].\nIn many progressive, student-centered preschools, students are left largely to their own devices\nto move from one center to another throughout the preschool room. Generally, each center\nwill be staffed by an instructor at some points in the day but not others, providing a variety\nof learning experiences. At some preschools students will be strongly encouraged to distribute\ntheir time relatively evenly among the different learning centers, or to focus on those learning\ncenters corresponding to their particular strengths and/or weaknesses.\nTo imitate the general character of a human preschool, one would create several centers in\na robot lab or virtual world. The precise architecture will best be adapted via experience but\ninitial centers would likely be:\n• a blocks center: a table with blocks on it\n• a language center: a circle of chairs, intended for people to sit around and talk with the\nrobot\n• a manipulatives center: with a variety of different objects of different shapes and sizes,\nintended to teach visual and motor skills\n• a ball play center: where balls are kept in chests and there is space for the robot to kick\nthe balls around\n• a dramatics center: where the robot can observe and enact various movements\n16.3 Elements of Preschool Curriculum\nWhile preschool curricula vary considerably based on educational philosophy and regional and\ncultural factors, there is a great deal of common, shared wisdom regarding the most useful topics\nand methods for preschool teaching. Guided experiential learning in diverse environments and\nusing varied materials is generally agreed upon as being an optimal methodology to reach a wide\nvariety of learning types and capabilities. Hands-on learning provides grounding in specifics,\nwhere as a diversity of approaches allows for generalization.\nCore knowledge domains are also relatively consistent, even across various philosophies\nand regions. Language, movement and coordination, autonomous judgment, social skills, work\nhabits, temporal orientation, spatial orientation, mathematics, science, music, visual arts, and\ndramatics are universal areas of learning which all early childhood learning touches upon. The\nparticulars of these skills may vary, but all human children are taught to function in these domains.\nThe level of competency developed may vary, but general domain knowledge is provided.\nFor example, most kids won’t be the next Maria Callas, Ravi Shankar or Gene Ween, but nearly\nall learn to hear, understand and appreciate music.\nTables 16.1 - 16.3 review the key capabilities taught in preschools, and identify the most\nimportant specific skills that need to be evaluated in the context of each capability. This ta-\n16.3 Elements of Preschool Curriculum 293\nble was assembled via surveying the curricula from a number of currently existing preschools\nemploying different methodologies both based on formal academic cognitive theories [Sch07]\nand more pragmatic approaches, such as: Montessori [Mon12], Waldorf [SS03b], Brain Gym\n(www.braingym.org) and Core Knowledge (www.coreknowledge.org).\nType of Capability Specific Skills to be Evaluated\nStory Understanding\n• Understanding narrative sequence\n• Understanding character development\n• Dramatize a story\n• Predict what comes next in a story\nLinguistic\n• Give simple descriptions of events\n• Describe similarities and differences\n• Describe objects and their functions\nLinguistic / Spatial- Interpreting pictures\nVisual\nLinguistic / Social\n• Asking questions appropriately\n• Answering questions appropriately\n• Talk about own discoveries\n• Initiate conversations\n• Settle disagreements\n• Verbally express empathy\n• Ask for help\n• Follow directions\nLinguistic / Scientific\n• Provide possible explanations for events or phenomena\n• Carefully describe observations\n• Draw conclusions from observations\nTable 16.1: Categories of Preschool Curriculum, Part 1\n16.3.1 Preschool in the Light of Intelligence Theory\nComparing Table 16.1 to Gardner’s Multiple Intelligences (MI) framework briefly reviewed in\nChapter 2, the high degree of harmony is obvious, and is borne out by more detailed analysis.\nPreschool curriculum as standardly practiced is very well attuned to MI, and naturally covers\nall the bases that Gardner identifies as important. And this is not at all surprising since one of\nGardner’s key motivations in articulating MI theory was the pragmatics of educating humans\nwith diverse strengths and weaknesses.\nRegarding intelligence as “the ability to achieve complex goals in complex environments,” it\nis apparent that preschools are specifically designed to pack a large variety of different micro-\n294 16 AGI Preschool\nType of Capability\nLogical-\nMathematical\nNonverbal Communication\nSpatial-Visual\nObjective\nSpecific Skills to be Evaluated\n• Categorizing\n• Sorting\n• Arithmetic\n• Performing simple “proto-scientific experiments”\n• Communicating via gesture\n• Dramatizing situations\n• Dramatizing needs, wants\n• Express empathy\n• Visual patterning\n• Self-expression through drawing\n• Navigate\n• Assembling objects\n• Disassembling objects\n• Measurement\n• Symmetry\n• Similarity between structures (e.g. block structures and\nreal ones)\nTable 16.2: Categories of Preschool Curriculum, Part 2\nType of Capability\nInterpersonal\nEmotional\nSpecific Skills to be Evaluated\n• Cooperation\n• Display appropriate behavior in various settings\n• Clean up belongings\n• Share supplies\n• Delay gratification\n• Control emotional reactions\n• Complete projects\nTable 16.3: Categories of Preschool Curriculum, Part 3\nenvironments (the learning centers) into a single room, and to present a variety of different tasks\nin each environment. The environments constituted by preschool learning centers are designed\nas microcosms of the most important aspects of the environments faced by humans in their\neveryday lives.\n16.4 Task-Based Assessment in AGI Preschool 295\n16.4 Task-Based Assessment in AGI Preschool\nProfessional pedagogues such as [CM07] discuss evaluation of early childhood learning as intended\nto assess both specific curriculum content knowledge as well as the child’s learning\nprocess. It should be as unobtrusive as possible, so that it just seems like another engaging activity,\nand the results used to tailor the teaching regimen to use different techniques to address\nweaknesses and reinforce strengths.\nFor example, with group building of a model car, students are tested on a variety of skills:\nprocedural understanding, visual acuity, motor acuity, creative problem solving, interpersonal\ncommunications, empathy, patience, manners, and so on. With this kind of complex, yet engaging,\nactivity as a metric the teacher can see how each student approaches the process of\nunderstanding each subtask, and subsequently guide each student’s focus differently depending\non strengths and weaknesses.\nIn Tables 16.4 and 16.5 we describe some particular tasks that AGIs may be meaningfully\nassigned in the context of a general AGI Preschool design and curriculum as described above.\nOf course, this is a very partial list, and is intended as evocative rather than comprehensive.\nAny one of these tasks can be turned into a rigorous quantitative test, thus allowing the\nprecise comparison of different AGI systems’ capabilities; but we have chosen not to emphasize\nthis point here, partly for space reasons and partly for philosophical ones. In some contexts\nthe quantitative comparison of different systems may be the right thing to do, but as discussed\nin Chapter 17 there are also risks associated with this approach, including the emergence of\nan overly metrics-focused “bakeoff mentality” among system developers, and overfitting of AI\nabilities to test taking. What is most important is the isolation of specific tasks on which\ndifferent systems may be experientially trained and then qualitatively assessed and compared,\nrather than the evaluation of quantitative metrics.\nTask-oriented testing allows for feedback on applications of general pedagogical principles to\nreal-world, embodied activities. This allows for iterative refinement based learning (shaping),\nand cross development of knowledge acquisition and application (multitask learning). It also\nhelps militate against both cheating, and over-fitting, as teachers can make ad-hoc modifications\nto the tests to determine if this is happening and correct for it if necessary.\nE.g., consider a linguistic task in which the AGI is required to formulate a set of instructions\nencapsulating a given behavior (which may include components that are physical, social,\nlinguistic, etc.). Note that although this is presented as centrally a linguistic task, it actually involves\na diverse set of competencies since the behavior to be described may encompass multiple\nreal-world aspects.\nTo turn this task into a more thorough test one might involve a number of human teachers\nand a number of human students. Before the test, an ensemble of copies of the AGI would\nbe created, with identical knowledge state. Each copy would interact with a different human\nteacher, who would demonstrate to it a certain behavior. After testing the AGI on its own\nknowledge of the material, the teacher would then inform the AGI that it will then be tested on\nits ability to verbally describe this behavior to another. Then, the teacher goes away and the\ncopy interacts with a series of students, attempting to convey to the students the instructions\ngiven by the teacher.\nThe teacher can thereby assess both the AGI’s understanding of the material, and the ability\nto explain it to the other students. This separates out assessment of understanding from assessment\nof ability to communicate understanding, attempting to avoid conflation of one with the\nother. The design of the training and testing needs to account for potential\n296 16 AGI Preschool\nIntelligence Type\nLinguistic\nLogical-\nMathematical\nMusical\nBodily-Kinesthetic\nTest\n• write a set of instructions\n• speak on a subject\n• edit a written piece or work\n• write a speech\n• commentate on an event\n• apply positive or negative ’spin’ to astory\n• perform arithmetic calculations\n• create a process to measure something\n• analyse how a machine works\n• create a process\n• devise a strategy to achieve an aim\n• assess the value of a proposition\n• perform a musical piece\n• sing a song\n• review a musical work\n• coach someone to play a musical instrument\n• juggle\n• demonstrate a sports technique\n• flip a beer-mat\n• create a mime to explain something\n• toss a pancake\n• fly a kite\nTable 16.4: Prototypical preschool intelligence assessment tasks, Part 1\nThis testing protocol abstracts away from the particularities of any one teacher or student,\nand focuses on effectiveness of communication in a human context rather than according to\nformalized criteria. This is very much in the spirit of how assessment takes place in human\npreschools (with the exception of the copying aspect): formal exams are rarely given in preschool,\nbut pragmatic, socially-embedded assessments are regularly made.\nBy including the copying aspect, more rigorous statistical assessments can be made regarding\nefficacy of different approaches for a given AGI design, independent of past teaching experiences.\nThe multiple copies may, depending on the AGI system design, then be able to be reintegrated,\nand further “learning” be done by higher-order cognitive systems in the AGI that integrate the\ndisparate experiences of the multiple copies.\nThis kind of parallel learning is different from both sequential learning that humans do, and\nparallel presences of a single copy of an AGI (such as in multiple chat rooms type experiments).\nAll three approaches are worthy of study, to determine under what circumstances, and with\nwhich AGI designs, one is more successful than another.\nIt is also worth observing how this test could be tweaked to yield a test of generalization\nability. After passing the above, the AGI could then be given a description of a new task\n16.4 Task-Based Assessment in AGI Preschool 297\nIntelligence Type\nSpatial-Visual\nInterpersonal\nTest\n• design a costume\n• interpret a painting\n• create a room layout\n• create a corporate logo\n• design a building\n• pack a suitcase or the trunk of a car\n• interpret moods from facial expressions\n• demonstrate feelings through body language\n• affect the feelings of others in a planned way\n• coach or counsel another\nTable 16.5: Prototypical preschool intelligence assessment tasks, Part 2\n(acquisition), and asked to explain the new one (variation). And, part of the training behavior\nmight be carried out unobserved by the AGI, thus requiring the AGI to infer the omitted parts\nof the task it needs to describe.\nAnother popular form of early childhood testing is puzzle block games. These kinds of games\ncan be used to assess a variety of important cognitive skills, and to do so in a fun way that\nnot only examines but also encourages creativity and flexible thinking. Types of games include\npattern matching games in which students replicate patterns described visually or verbally,\npattern creation games in which students create new patterns guided by visually or verbally\ndescribed principles, creative interpretation of patterns in which students find meaning in the\nforms, and free-form creation. Such games may be individual or cooperative.\nCross training and assessment of a variety of skills occurs with pattern block games: for\nexample, interpretation of visual or linguistic instructions, logical procedure and pattern following,\ncategorizing, sorting, general problem solving, creative interpretation, experimentation,\nand kinematic acuity. By making the games cooperative, various interpersonal skills involving\ncommunication and cooperation are also added to the mix.\nThe puzzle block context bring up some general observations about the role of kinematic\nand visuospatial intelligence in the AGI Preschool. Outside of robotics and computer vision, AI\nresearch has often downplayed these sorts of intelligence (though, admittedly, this is changing in\nrecent years, e.g. with increasing research focus on diagrammatic reasoning). But these abilities\nare not only necessary to navigate real (or virtual) spatial environments. They are also important\ncomponents of a coherent, conceptually well-formed understanding of the world in which the\nstudent is embodied. Integrative training and assessment of both rigorous cognitive abilities\ngenerally most associated with both AI and “proper schooling” (such as linguistic and logical\nskills) along with kinematic and aesthetic/sensory abilities is essential to the development of\nan intelligence that can successfully both operate in and sensibly communicate about the real\nworld in a roughly humanlike manner. Whether or not an AGI is targeted to interpret physicalworld\nspatial data and perform tasks via robotics, in order to communicate ideas about a vast\narray of topics of interest to any intelligence in this world, an AGI must develop aspects of\nintelligence other than logical and linguistic cognition.\n298 16 AGI Preschool\n16.5 Beyond Preschool\nOnce an AGI passes preschool, what are the next steps? There is still a long way to go, from\npreschool to an AGI system that is capable of, say, passing the Turing Test or serving as an\neffective artificial scientist.\nOur suggestion is to extend the school metaphor further, and make use of existing curricula\nfor higher levels of virtual education: grade school, secondary school, and all levels of postsecondary\neducation. If an AGI can pass online primary and secondary schools such as e-\ntutor.com, and go on to earn an online degree from an accredited university, then clearly said\nAGI has successfully achieved “human level, roughly humanlike AGI.” This sort of testing is\ninteresting not only because it allows assessment of stages intermediate between preschool and\nadult, but also because it tests humanlike intelligence without requiring precise imitation of\nhuman behavior.\nIf an AI can get a BA degree at an accredited university, via online coursework (assuming\nfor simplicity courses where no voice interaction is needed), then we should consider that AI to\nhave human-level intelligence. University coursework spans multiple disciplines, and the details\nof the homework assignments and exams are not known in advance, so like a human student\nthe AGI team can’t cheat.\nIn addition to the core coursework, a schooling approach also tests basic social interaction\nand natural language communication, ability to do online research, and general problem solving\nability. However, there is no rigid requirement to be strictly humanlike in order to pass university\nclasses.\nMost of our concrete examples in the following chapters will pertain to the preschool context,\nbecause it’s simple to understand, and because we feel that getting to the “AGI preschool\nstudent” level is going to be the largest leap. Once that level is obtained, moving further\nwill likely be difficult also, but we suspect it will be more a matter of steady incremental\nimprovements – whereas the achievement of preschool-level functionality will be a large leap\nfrom the current situation.\n16.6 Issues with Virtual Preschool Engineering\nAs noted above there are two broad approaches to realizing the “AGI Preschool” idea: using\nthe AGI to control a physical robot and then crafting a preschool environment suitable to the\nrobot’s sensors and actuators; or, using the AGI to control a virtual agent in an appropriately\nrich virtual-world preschool. The robotic approach is harder from an AI perspective (as one must\ndeal with problems of sensation and actuation), but easier from an environment-construction\nperspective. In the virtual world case, one quickly runs up against the current limitations\nof virtual world technologies, which have been designed mainly for entertainment or socialnetworking\npurposes, not with the requirements of AGI systems in mind.\nIn Chapter 9 we discussed the general requirements that an environment should possess to be\nsupportive of humanlike intelligence. Referring back to that list, it’s clear that current virtual\nworlds are fairly strong on multimodal communication, and fairly weak on naive physics. More\nconcretely, if one wants a virtual world so that\n16.6 Issues with Virtual Preschool Engineering 299\n1. one could carry out all the standard cognitive development experiments described in developmental\npsychology books\n2. one could implement intuitively reasonable versions of all the standard activities in all the\nstandard learning stations in a contemporary preschool\nthen current virtual world technologies appear not to suffice.\nAs reviewed above, typical preschool activities include for instance building with blocks,\nplaying with clay, looking in a group at a picture book and hearing it read aloud, mixing\ningredients together, rolling/throwing/catching balls, playing games like tag, hide-and-seek,\nSimon Says or Follow the Leader, measuring objects, cutting paper into different shapes, drawing\nand coloring, etc.\nAnd, as typical, not necessarily representative examples of tasks psychologists use to measure\ncognitive development (drawn mainly from the Piagetan tradition, without implying any\nassertion that this is the only tradition worth pursuing), consider the following:\n1. Which row has more circles- A or B? A: O O O O O, B: OOOOO\n2. If Mike is taller than Jim, and Jim is shorter than Dan, then who is the shortest? Who is\nthe tallest?\n3. Which is heavier- a pound of feathers or a pound of rocks?\n4. Eight ounces of water is poured into a glass that looks like the fat glass in Figure 2 16.1\nand then the same amount is poured into a glass that looks like the tall glass in Figure 16.2\n. Which glass has more water?\n5. A lump of clay is rolled into a snake. All the clay is used to make the snake. Which has\nmore clay in it – the lump or the snake?\n6. There are two dolls in a room, Sally and Ann, each of which has her own box, with a marble\nhidden inside. Sally goes out for a minute, leaving her box behind; and Ann decides to play\na trick on Sally: she opens Sally’s box, removes the marble, hiding it in her own box. Sally\nreturns, unaware of what happened. Where will Sally would look for her marble?\n7. Consider this rule about a set of cards that have letters on one side and numbers on the\nother: “If a card has a vowel on one side, then it has an even number on the other side.” If\nyou have 4 cards labeled “E K 4 7”, which cards do you need to turn over to tell if this rule\nis actually true?\n8. Design an experiment to figure out how to make a pendulum that swings more slowly versus\nless slowly\nWhat we see from this ad hoc, partial list is that a lot of naive physics is required to make an\neven vaguely realistic preschool. A lot of preschool education is about the intersection between\nabstract cognition and naive physics. A more careful review of the various tasks involved in\npreschool education bears out this conclusion.\nWith this in mind, in this section we will briefly describe an approach to extending current\nvirtual world technologies that appears to allow the construction of a reasonably rich and\nrealistic AGI preschool environment, without requiring anywhere near a complete simulation of\nrealistic physics.\n300 16 AGI Preschool\nFig. 16.1: Part 1 of a Piagetan conservation of volume experiment: a child observes that two\nglasses obviously have the same amount of milk in them, and then sees the content of one of\nthe glasses poured into a different-shaped glass.\n16.6 Issues with Virtual Preschool Engineering 301\nFig. 16.2: Part 2 of a Piagetan conservation of volume experiment: a child observes two differentshaped\nglasses, which (depending on the level of his cognition), he may be able to infer have\nthe same amount of milk in them, due to the events depicted in Figure 16.1.\n16.6.1 Integrating Virtual Worlds with Robot Simulators\nOne glaring deficit in current virtual world platforms is the lack of flexibility in terms of tool use.\nIn most of these systems today, an avatar can pick up or utilize an object, or two objects can\ninteract, only in specific, pre-programmed ways. For instance, an avatar might be able to pick up\na virtual screwdriver only by the handle, rather than by pinching the blade betwen its fingers.\nThis places severe limits on creative use of tools, which is absolutely critical in a preschool\ncontext. The solution to this problem is clear: adapt existing generalized physics engines to\nmediate avatar-object and object-object interactions. This would require more computation\nthan current approaches, but not more than is feasible in a research context.\nOne way to achieve this goal would be to integrate a robot simulator with a virtual world\nor game engine, for instance to modify the OpenSim (opensimulator.org) virtual world to\nuse the Gazebo (playerstage.sourceforge.net) robot simulator in place of its current\nphysics engine. While tractable, such a project would require considerable software engineering\neffort.\n16.6.2 BlocksNBeads World\nAnother glaring deficit in current virtual world platforms is their inability to model physical\nphenomena besides rigid objects with any sophistication. In this section we propose a potential\n302 16 AGI Preschool\nsolution to this issue: a novel class of virtual worlds called BlocksNBeadsWorld, consisting of\nthe following aspects:\n1. 3D blocks of various shapes and sizes and frictional coefficients, that can be stacked\n2. Adhesive that can be used to stick blocks together, and that comes in two types, one of\nwhich can be removed by an adhesive-removing substance, one of which cannot (though its\nbonds can be broken via sufficient application of force)\n3. Spherical beads, each of which has intrinsic unchangeable adhesion properties defined according\nto a particular, simple “adhesion logic”\n4. Each block, and each bead, may be associated with multidimensional quantities representing\nits taste and smell; and may be associated with a set of sounds that are made when it is\nimpacted with various forces at various positions on its surface\nInteraction between blocks and beads is to be calculated according to standard Newtonian\nphysics, which would be compute-intensive in the case of a large number of beads, but tractable\nusing distributed processing. For instance if 10K beads were used to cover a humanoid agent’s\nface, this would provide a fairly wide diversity of facial expressions; and if 10K beads were\nused to form a blanket laid on a bed, this would provide a significant amount of flexibility\nin terms of rippling, folding and so forth. Yet, this order of magnitude of interactions is very\nsmall compared to what is done in contemporary simulations of fluid dynamics or, say, quantum\nchromodynamics.\nOne key aspect of the spherical beads is that they can be used to create a variety of rigid or\nflexible surfaces, which may exist on their own or be attached to blocks-based constructs. The\nspecific inter-bead adhesion properties of the beads could be defined in various ways, and will\nsurely need to be refined via experimentation, but a simple scheme that seems to make sense\nis as follows.\nEach bead can have its surface tesselated into hexagons (the number of these can be tuned),\nand within each hexagon it can have two different adhesion coefficients: one for adhesion to\nother beads, and one for adhesion to blocks. The adhesion between two beads along a certain\nhexagon is then determined by their two adhesion coefficients; and the adhesion between a bead\nand a block is determined by the adhesion coefficient of the bead, and the adhesion coefficient\nof the adhesive applied to the block. A distinction must be drawn between rigid and flexible\nadhesion: rigid adhesion sticks a bead to something in a way that can’t be removed except via\nbreaking it off; whereas flexible adhesion just keeps a bead very close to the thing it’s stuck\nonto. Any two entities may be stuck together either rigidly or flexibly. Sets of beads with flexible\nadhesion to each other can be used to make entities like strings, blankets or clothes.\nUsing the above adhesion logic, it seems one could build a wide variety of flexible structures\nusing beads, such as (to give a very partial list):\n1. fabrics with various textures, that can be draped over blocks structures,\n2. multilayered coatings to be attached to blocks structures, serving (among many other examples)\nas facial expressions\n3. liquid-type substances with varying viscosities, that can be poured between different containers,\nspilled, spread, etc.\n4. strings tyable in knots; rubber bands that can be stretched; etc.\nOf course there are various additional features one could add. For instance one could add a\nspecial set of rules for vibrating strings, allowing BlocksNBeadsWorld to incorporate the creation\n16.6 Issues with Virtual Preschool Engineering 303\nof primitive musical instruments. Variations like this could be helpful but aren’t necessary for\nthe world to serve its essential purpose.\nNote that one does not have true fluid dynamics in BlocksNBeadsWorld, but, it seems that\nthe latter is not necessary to encompass the phenomena covered in cognitive developmental\ntests or preschool tasks. The tests and tasks that are done with fluids can instead be done with\nmasses of beads. For example, consider the conservation of volume task shown in Figures 16.1\nand 16.2 below: it’s easy enough to envision this being done with beads rather than milk. Even\na few hundred beads is enough to be psychologically perceived as a mass rather than a set\nof discrete units, and to be manipulated and analyzed as such. And the simplification of not\nrequiring fluid mechanics in one’s virtual world is immense.\nNext, one can implement equations via which the adhesion coefficients of a bead are determined\nin part by the adhesion coefficients of nearby beads, or beads that are nearby in certain\ndirections (with direction calculated in local spherical coordinates). This will allow for complex\ncracking and bending behaviors – not identical to those in the real world, but with similar qualitative\ncharacteristics. For example, without this feature one could create paperlike substances\nthat could be cut with scissors – but with this feature, one could go further and create woodlike\nsubstances that would crack when nails were hammered into them in certain ways, and so forth.\nFurther refinements are certainly possible also. One could add multidimensional adhesion\ncoefficients, allowing more complex sorts of substances. One could allow beads to vibrate at\nvarious frequencies, which would lead to all sorts of complex wave patterns in bead compounds.\nEtc. In each case, the question to be asked is: what important cognitive abilities are dramatically\nmore easily learnable in the presence of the new feature than in its absence?\nThe combination of blocks and beads seems ideal for implementing a more flexible and AGIfriendly\ntype of virtual body than is currently used in games and virtual worlds. One can easily\nenvision implementing a body with\n1. a skeleton whose bones consist of appropriately shaped blocks\n2. joints consisting of beads, flexibly adhered to the bones\n3. flesh consisting of beads, flexibly adhered to each other\n4. internal “plumbing” consisting of tubes whose walls are beads rigidly adhered to each other,\nand flexibly adhered to the surrounding flesh (the plumbing could then serve to pass beads\nthrough, where slow passage would be ensured by weak adhesion between the walls of the\ntubes and the beads passing through the tubes)\nThis sort of body would support rich kinesthesia; and rich, broad analogy-drawing between\nthe internally-experienced body and the externally-experienced world. It would also afford many\ninteresting opportunities for flexible movement control. Virtual animals could be created along\nwith virtual humanoids.\nRegarding the extended mind, it seems clear that blocks and beads are adequate for the\ncreation of a variety of different tools. Equipping agents with “glue guns” able to affect the\nadhesive properties of both blocks and beads would allow a diversity of building activity; and\nbuilding with masses of beads could become a highly creative activity. Furthermore, beads\nwith appropriately specified adhesion (within the framework outlined above) could be used\nto form organically growing plant-like substances, based on the general principles used in L-\nsystem models of plant growth (Prusinciewicz and Lindenmayer 1991). Structures with only\nbeads would vaguely resemble herbaceous plants; and structures involving both blocks and\nbeads would more resemble woody plants. One could even make organic structures that flourish\n304 16 AGI Preschool\nor otherwise based on the light available to them (without of course trying to simulate the\nchemistry of photosynthesis).\nSome elements of chemistry may be achieved as well, though nowhere near what exists\nin physical reality. For instance, melting and boiling at least should be doable: assign every\nbead a temperature, and let solid interbead bonds turn liquid above a certain temperature and\ndisappear completely above some higher temperature. You could even have a simple form of fire.\nLet fire be an element, whose beads have negative gravitational mass. Beads of fuel elements\nlike wood have a threshold temperature above which they will turn into fire beads, with release\nof additional heat. 2\nThe philosophy underlying these suggested bead dynamics is somewhat comparable to that\noutlined in Wolfram’s book A New Kind of Science [Wol02]. There he proposes cellular automata\nmodels that emulate the qualitative characteristics of various real-world phenomena,\nwithout trying to match real-world data precisely. For instance, some of his cellular automata\ndemonstrate phenomena very similar to turbulent fluid flow, without implementing the Navier-\nStokes equations of fluid dynamics or trying to precisely match data from real-world turbulence.\nSimilarly, the beads in BlocksNBeadsWorld are intended to qualitatively demonstrate the realworld\nphenomena most useful for the development of humanlike embodied intelligence, without\ntrying to precisely emulate the real-world versions of these phenomena.\nThe above description has been left imprecisely specified on purpose. It would be straightforward\nto write down a set of equations for the block and bead interactions, but there seems\nlittle value in articulating such equations without also writing a simulation involving them and\ntesting the ensuing properties. Due to the complex dynamics of bead interactions, the finetuning\nof the bead physics is likely to involve some tuning based on experimentation, so that\nany equations written down now would likely be revised based on experimentation anyway. Our\ngoal here has been to outline a certain class of potentially useful environments, rather than to\narticulate a specific member of this class.\nWithout the beads, BlocksNBeadsWorld would appear purely as a “Blocks World with Glue”\n– essentially a substantially upgraded version of the Blocks Worlds frequently used in AI, since\nfirst introduced in [Win72]. Certainly a pure “Blocks World with Glue” would have greater\nsimplicity than BlocksNBeadsWorld, and greater richness than standard Blocks World; but\nthis simplicity comes with too many limitations, as shown by consideration of the various naive\nphysics requirements inventoried above. One simply cannot run the full spectrum of humanlike\ncognitive development experiments, or preschool educational tasks, using blocks and glue alone.\nOne can try to create analogous tasks using only blocks and glue, but this quickly becomes\nextremely awkward. Whereas in the BlocksNBeadsWorld the capability for this full spectrum\nof experiments and tasks seems to fall out quite naturally.\nWhat’s missing from BlocksNBeadsWorld should be fairly obvious. There isn’t really any\ndistinction between a fluid and a powder: there are masses, but the types and properties of the\nmasses are not the same as in the real world, and will surely lack the nuances of real-world fluid\ndynamics. Chemistry is also missing: processes like cooking and burning, although they can be\ncrudely emulated, will not have the same richness as in the real world. The full complexity of\nbody processes is not there: the body-design method mentioned above is far richer and more\nadaptive and responsive than current methods of designing virtual bodies in 3DSMax or Maya\nand importing them into virtual world or game engines, but still drastically simplistic compared\nto real bodies with their complex chemical signaling systems and couplings with other bodies\nand the environment. The hypothesis we’re making in this section is that these lacunae aren’t\n2 Thanks are due to Russell Wallace for the suggestions in this paragraph\n16.6 Issues with Virtual Preschool Engineering 305\nthat important from the point of view of humanlike cognitive development. We suggest that\nthe key features of naive physics and folk psychology enumerated above can be mastered by an\nAGI in BlocksNBeadsWorld in spite of its limitations, and that – together with an appropriate\nAGI design – this probably suffices for creating an AGI with the inductive biases constituting\nhumanlike intelligence.\nTo drive this point home more thoroughly, consider three potential virtual world scenarios:\n1. A world containing realistic fluid dynamics, where a child can pour water back and forth\nbetween two cups of different shapes and sizes, to understand issues such as conservation\nof volume\n2. A world more like today’s Second Life, where fluids don’t really exist, and things like lakes\nare simulated via very simple rules, and pouring stuff back and forth between cups doesn’t\nhappen unless it’s programmed into the cups in a very specialized way\n3. A BlocksNBeadsWorld type world, where a child can pour masses of beads back and forth\nbetween cups, but not masses of liquid\nOur qualitative judgment is that Scenario 3 is going to allow a young AI to gain the same essential\ninsights as Scenario 1, whereas Scenario 2 is just too impoverished. I have explored dozens\nof similar scenarios regarding different preschool tasks or cognitive development experiments,\nand come to similar conclusions across the board. Thus, our current view is that something like\nBlocksNBeadsWorld can serve as an adequate infrastructure for an AGI Preschool, supporting\nthe development of human-level, roughly human-like AGI.\nAnd, if this view turns out to be incorrect, and BlocksNBeadsWorld is revealed as inadequate,\nthen we will very likely still advocate the conceptual approach enunciated above as a guide for\ndesigning virtual worlds for AGI. That is, we would suggest to explore the hypothetical failure\nof BlocksNBeadsWorld via asking two questions:\n1. Are there basic naive physics or folk psychology requirements that were missed in creating\nthe specifications, based on which the adequacy of BlocksNBeadsWorld was assessed?\n2. Does BlocksNBeadsWorld fail to sufficiently emulate the real world in respect to some of\nthe articulated naive physics or folk psychology requirements?\nThe answers to these questions would guide the improvement of the world or the design of a\nbetter one.\nRegarding the practical implementation of BlocksNBeadsWorld, it seems clear that this is\nwithin the scope of modern game engine technology, however, it is not something that could be\nencompassed within an existing game or world engine without significant additions; it would\nrequire substantial custom engineering. There exist commodity and open-source physics engines\nthat efficiently carry out Newtonian mechanics calculations; while they might require\nsome tuning and extension to handle BlocksNBeadWorld, the main issue would be achieving\nadequate speed of physics calculation, which given current technology would need to be done\nvia modifying existing engines to appropriately distribute processing among multiple GPUs.\nFinally, an additional avenue that merits mention is the use of BlocksNBeads physics internally\nwithin an AGI system, as part of an internal simulation world that allows it to make\n“mind’s eye” estimative simulations of real or hypothetical physical situations. There seems no\nreason that the same physics software libraries couldn’t be used both for the external virtual\nworld that the AGI’s body lives in, and for an internal simulation world that the AGI uses as\na cognitive tool. In fact, the BlocksNBeads library could be used as an internal cognitive tool\nby AGI systems controlling physical robots as well. This might require more tuning of the bead\n306 16 AGI Preschool\ndynamics to accord with the dynamics of various real-world systems; but, this tuning would be\nbeneficial for the BlocksNBeadWorld as well.\nChapter 17\nA Preschool-Based Roadmap to Advanced AGI\n17.1 Introduction\nSupposing the CogPrime approach to creating advanced AGI is workable – then what are the\nright practical steps to follow? The various structures and algorithms outlined in Part 2 of\nthis book should be engineered and software-tested, of course – but that’s only part of the\nstudy. The AGI system implemented will need to be taught, and it will need to be placed in\nsituations where it can develop an appropriate self-model and other critical internal network\nstructures. The complex structures and algorithms involved will need to be fine-tuned in various\nways, based on qualitatively observing the overall system’s behavior in various situations. To\nget all this right without excessive confusion or time-wastage requires a fairly clear roadmap for\nCogPrime development.\nIn this chapter we’ll sketch one particular roadmap for the development of human-level,\nroughly human-like AGI – which we’re not selling as the only one, or even necessarily as the\nbest one. It’s just one roadmap that we have thought about a lot, and that we believe has a\nstrong chance of proving effective. Given resources to pursue only one path for AGI development\nand teaching, this would be our choice, at present. The roadmap outlined here is not restricted\nto CogPrime in any highly particular ways, but it has been developed largely with CogPrime\nin mind; those developing other AGI designs could probably use this roadmap just fine, but\nmight end up wanting to make various adjustments based on the strengths and weaknesses of\ntheir own approach.\nWhat we mean here by a \"roadmap\" is, in brief: a sequence of \"milestone\" tasks, occurring\nin a small set of common environments or \"scenarios,\" organized so as to lead to a commonly\nagreed upon set of long-term goals. I.e., what we are after here is a \"capability roadmap\" – a\nroadmap laying out a series of capabilities whose achievement seems likely to lead to humanlevel\nAGI. Other sorts of roadmaps such as \"tools roadmaps\" may also be valuable, but are not\nour concern here.\nMore precisely, we confront the task of roadmapping by identifying scenarios in which to\nembed our AGI system, and then \"competency areas\" in which the AGI system must be evaluated.\nThen, we envision a roadmap as consisting of a set of one or more task-sets, where each\ntask set is formed from a combination of a scenario with a list of competency areas. To create\na task-set one must choose a particular scenario, and then articulate a set of specific tasks,\neach one addressing one or more of the competency areas. Each task must then get associated\nwith particular performance metrics – quantitative wherever possible, but perhaps qualitative\n307\n308 17 A Preschool-Based Roadmap to Advanced AGI\nin some cases depending on the nature of the task. Here we give a partial task-set for the \"virtual\nand robot preschool\" scenarios discussed in Chapter 16, and a couple example quantitative\nmetrics just to illustrate what is intended; the creation of a fully detailed roadmap based on\nthe ideas outlined here is left for future work.\nThe train of thought presented in this chapter emerged in part from a series of conversations\npreceding and during the \"AGI Roadmap Workshop\" held at the University of Tennessee,\nKnoxville in October 2008. Some of the ideas also trace back to discussions held during two\nworkshops on \"Evaluation and Metrics for Human-Level AI\" organized by John Laird and Pat\nLangley (one in Ann Arbor in late 2008, and one in Tempe in early 2009). Some of the conclusions\nof the Ann Arbor workshop were recorded in [LWML09]. Inspiration was also obtained\nfrom discussion at the \"Future of AGI\" post-conference workshop of the AGI-09 conference,\ntriggered by Itamar Arel’s [ARK09a] presentation on the \"AGI Roadmap\" theme; and from an\nearlier article on AGI Roadmapping by [AL09].\nHowever, the focus of the AGI Roadmap Workshop was considerably more general than the\npresent chapter. Here we focus on preschool-type scenarios, whereas at the workshop a number\nof scenarios were discussed, including the preschool scenarios but also, for example,\n• Standardized Tests and School Curricula\n• Elementary, Middle and High School Student\n• General Videogame Learning\n• Wozniak’s Coffee Test: go into a random American house and figure out how to make coffee,\nand do it\n• Robot College Student\n• General Call Center Respondent\nFor each of these scenarios, one may generate tasks corresponding to each of the competency\nareas we will outline below. CogPrime is applicable in all these scenarios, so our choice to focus\non preschool scenarios is an additional judgment call beyond those judgment calls required to\nspecify the CogPrime design. The roadmap presented here is a \"AGI Preschool Roadmap\" and\nas such is a special case of the broader \"AGI Roadmap\" outlined at the workshop.\n17.2 Measuring Incremental Progress Toward Human-Level AGI\nIn Chapter 2, we discussed several examples of practical goals that we find to plausibly characterize\n\"human level AGI\", e.g.\n• Turing Test\n• Virtual World Turing Test\n• Online University Test\n• Physical University Test\n• Artificial Scientist Test\nWe also discussed our optimism regarding the possibility that in the future AGI may advance\nbeyond the human level, rendering all these goals \"early-stage subgoals.\"\nHowever, in this chapter we will focus our attention on the nearer term. The above goals are\nambitious ones, and while one can talk a lot about how to precisely measure their achievement,\nwe don’t feel that’s the most interesting issue to ponder at present. More critical is to think\n17.2 Measuring Incremental Progress Toward Human-Level AGI 309\nabout how to measure incremental progress. How do you tell when you’re 25% or 50% of the way\nto having an AGI that can pass the Turing Test, or get an online university degree. Fooling 50%\nof the Turing Test judges is not a good measure of being 50% of the way to passing the Turing\nTest (that’s too easy); and passing 50% of university classes is not a good measure of being 50%\nof the way to getting an online university degree (it’s too hard – if one had an AGI capable\nof doing that, one would almost surely be very close to achieving the end goal). Measuring\nincremental progress toward human-level AGI is a subtle thing, and we argue that the best way\nto do it is to focus on particular scenarios and the achievement of specific competencies therein.\nAs we argued in Chapter 8 there are some theoretical reasons to doubt the possibility of\ncreating a rigorous objective test for partial progress toward AGI – a test that would be convincing\nto skeptics, and impossible to \"game\" via engineering a system specialized to the test.\nFortunately, though we don’t need a test of this nature for the purposes of assessing our own\nincremental progress toward advanced AGI, based on our knowledge about our own approach.\nBased on the nature of the grand goals articulated above, there seems to be a very natural\napproach to creating a set of incremental capabilities building toward AGI: to draw on our\ncopious knowledge about human cognitive development. This is by no means the only possible\npath; one can envision alternatives that have nothing to do with human development (and those\nmight also be better suited to non-human AGIs). However, so much detailed knowledge about\nhuman development is available – as well as solid knowledge that the human developmental\ntrajectory does lead to human-level AI – that the motivation to draw on human cognitive\ndevelopment is quite strong.\nThe main problem with the human development inspired approach is that cognitive developmental\npsychology is not as systematic as it would need to be for AGI to be able to translate\nit directly into architectural principles and requirements. As noted above, while early thinkers\nlike Piaget and Vygotsky outlined systematic theories of child cognitive development, these\nare no longer considered fully accurate, and one currently faces a mass of detailed theories of\nvarious aspects of cognitive development, but without an unified understanding. Nevertheless\nwe believe it is viable to work from the human-development data and understanding currently\navailable, and craft a workable AGI roadmap therefrom.\nWith this in mind, what we give next is a fairly comprehensive list of the competencies that\nwe feel AI systems should be expected to display in one or more of these scenarios in order\nto be considered as full-fledged \"human level AGI\" systems. These competency areas have\nbeen assembled somewhat opportunistically via a review of the cognitive and developmental\npsychology literature as well as the scope of the current AI field. We are not claiming this as\na precise or exhaustive list of the competencies characterizing human-level general intelligence,\nand will be happy to accept additions to the list, or mergers of existing list items, etc. What\nwe are advocating is not this specific list, but rather the approach of enumerating competency\nareas, and then generating tasks by combining competency areas with scenarios.\nWe also give, with each competency, an example task illustrating the competency. The tasks\nare expressed in the robot preschool context for concreteness, but they all apply to the virtual\npreschool as well. Of course, these are only examples, and ideally to teach an AGI in a structured\nway one would like to\n• associate several tasks with each competency\n• present each task in a graded way, with multiple subtasks of increasing complexity\n• associate a quantitative metric with each task\n310 17 A Preschool-Based Roadmap to Advanced AGI\nHowever, the briefer treatment given here should suffice to give a sense for how the competencies\nmanifest themselves practically in the AGI Preschool context.\n1. Perception\n• Vision: image and scene analysis and understanding\n– Example task: When the teacher points to an object in the preschool, the robot\nshould be able to identify the object and (if it’s a multi-part object) its major\nparts. If it can’t perform the identification initially, it can approach the object and\nmanipulate it before making its identification.\n• Hearing: identifying the sounds associated with common objects; understanding which\nsounds come from which sources in a noisy environment\n– Example task: When the teacher covers the robot’s eyes and then makes a noise\nwith an object, the robot should be able to guess what the object is\n• Touch: identifying common objects and carrying out common actions using touch alone\n– Example task: With its eyes and ears covered, the robot should be able to identify\nsome object by manipulating it; and carry out some simple behaviors (say, putting\na block on a table) via touch alone\n• Crossmodal: Integrating information from various senses\n– Example task: Identifying an object in a noisy, dim environment via combining\nvisual and auditory information\n• Proprioception: Sensing and understanding what its body is doing\n– Example task: The teacher moves the robot’s body into a certain configuration. The\nrobot is asked to restore its body to an ordinary standing position, and then repeat\nthe configuration that the teacher moved it into.\n2. Actuation\n• Physical skills: manipulating familiar and unfamiliar objects\n– Example task: Manipulate blocks based on imitating the teacher: e.g. pile two blocks\natop each other, lay three blocks in a row, etc.\n• Tool use, including the flexible use of ordinary objects as tools\n– Example task: Use a stick to poke a ball out of a corner, where the robot cannot\ndirectly reach\n• Navigation, including in complex and dynamic environments\n– Example task: Find its own way to a named object or person through a crowded\nroom with people walking in it and objects laying on the floor.\n3. Memory\n• Declarative: noticing, observing and recalling facts about its environment and experience\n– Example task: If certain people habitually carry certain objects, the robot should\nremember this (allowing it to know how to find the objects when the relevant people\nare present, even much later)\n• Behavioral: remembering how to carry out actions\n– Example task: If the robot is taught some skill (say, to fetch a ball), it should\nremember this much later\n• Episodic: remembering significant, potentially useful incidents from life history\n17.2 Measuring Incremental Progress Toward Human-Level AGI 311\n4. Learning\n– Example task: Ask the robot about events that occurred at times when it got particularly\nmuch, or particularly little, reward for its actions; it should be able to answer\nsimple questions about these, with significantly more accuracy than about events\noccurring at random times\n• Imitation: Spontaneously adopt new behaviors that it sees others carrying out\n– Example task: Learn to build towers of blocks by watching people do it\n• Reinforcement: Learn new behaviors from positive and/or negative reinforcement\nsignals, delivered by teachers and/or the environment\n– Example task: Learn which box the red ball tends to be kept in, by repeatedly trying\nto find it and noticing where it is, and getting rewarded when it finds it correctly\n• Imitation/Reinforcement\n– Example task: Learn to play “fetch”, “tag” and “follow the leader” by watching people\nplay it, and getting reinforced on correct behavior\n• Interactive Verbal Instruction\n– Example task: Learn to build a particular structure of blocks faster based on a\ncombination of imitation, reinforcement and verbal instruction, than by imitation\nand reinforcement without verbal instruction\n• Written Media\n– Example task: Learn to build a structure of blocks by looking at a series of diagrams\nshowing the structure in various stages of completion\n• Learning via Experimentation\n– Example task: Ask the robot to slide blocks down a ramp held at different angles.\nThen ask it to make a block slide fast, and see if it has learned how to hold the\nramp to make a block slide fast.\n5. Reasoning\n• Deduction, from uncertain premises observed in the world\n– Example task: If Ben more often picks up red balls than blue balls, and Ben is given\na choice of a red block or blue block to pick up, which is he more likely to pick up?\n• Induction, from uncertain premises observed in the world\n– Example task: If Ben comes into the lab every weekday morning, then is Ben likely\nto come to the lab today (a weekday) in the morning?\n• Abduction, from uncertain premises observed in the world\n– Example task: If women more often give the robot food than men, and then someone\nof unidentified gender gives the robot food, is this person a man or a woman?\n• Causal reasoning, from uncertain premises observed in the world\n– Example task: If the robot knows that knocking down Ben’s tower of blocks makes\nhim angry, then what will it say when asked if kicking the ball at Ben’s tower of\nblocks will make Ben mad?\n• Physical reasoning, based on observed “fuzzy rules” of naive physics\n– Example task: Given two balls (one rigid and one compressible) and two tunnels\n(one significantly wider than the balls, one slightly narrower than the balls), can\nthe robot guess which balls will fit through which tunnels?\n• Associational reasoning, based on observed spatiotemporal associations\n312 17 A Preschool-Based Roadmap to Advanced AGI\n6. Planning\n– Example task: If Ruiting is normally seen near Shuo, then if the robot knows where\nShuo is, that is where it should look when asked to find Ruiting\n• Tactical\n– Example task: The robot is asked to bring the red ball to the teacher, but the red\nball is in the corner where the robot can’t reach it without a tool like a stick. The\nrobot knows a stick is in the cabinet so it goes to the cabinet and opens the door\nand gets the stick, and then uses the stick to get the red ball, and then brings the\nred ball to the teacher.\n• Strategic\n– Example task: Suppose that Matt comes to the lab infrequently, but when he does\ncome he is very happy to see new objects he hasn’t seen before (and suppose the\nrobot likes to see Matt happy). Then when the robot gets a new object Matt has\nnot seen before, it should put it away in a drawer and be sure not to lose it or let\nanyone take it, so it can show Matt the object the next time Matt arrives.\n• Physical\n– Example task: To pick up a cup with a handle which is lying on its side in a position\nwhere the handle can’t be grabbed, the robot turns the cup in the right position\nand then picks up the cup by the handle\n• Social\n– Example task: The robot is given a job of building a tower of blocks by the end of\nthe day, and he knows Ben is the most likely person to help him, and he knows that\nBen is more likely to say \"yes\" to helping him when Ben is alone. He also knows\nthat Ben is less likely to say \"yes\" if he’s asked too many times, because Ben doesn’t\nlike being nagged. So he waits to ask Ben till Ben is alone in the lab.\n7. Attention\n• Visual Attention within its observations of its environment\n– Example task: The robot should be able to look at a scene (a configuration of objects\nin front of it in the preschool) and identify the key objects in the scene and their\nrelationships.\n• Social Attention\n– Example task: The robot is having a conversation with Itamar, which is giving the\nrobot reward (for instance, by teaching the robot useful information). Conversations\nwith other individuals in the room have not been so rewarding recently. But Itamar\nkeeps getting distracted during the conversation, by talking to other people, or\nplaying with his cellphone. The robot needs to know to keep paying attention to\nItamar even through the distractions.\n• Behavioral Attention\n– Example task: The robot is trying to navigate to the other side of a crowded room\nfull of dynamic objects, and many interesting things keep happening around the\nroom. The robot needs to largely ignore the interesting things and focus on the\nmovements that are important for its navigation task.\n8. Motivation\n17.2 Measuring Incremental Progress Toward Human-Level AGI 313\n• Subgoal creation, based on its preprogrammed goals and its reasoning and planning\n– Example task: Given the goal of pleasing Hugo, can the robot learn that telling\nHugo facts it has learned but not told Hugo before, will tend to make Hugo happy?\n• Affect-based motivation\n– Example task: Given the goal of gratifying its curiosity, can the robot figure out that\nwhen someone it’s never seen before has come into the preschool, it should watch\nthem because they are more likely to do something new?\n• Control of emotions\n– Example task: When the robot is very curious about someone new, but is in the\nmiddle of learning something from its teacher (who it wants to please), can it control\nits curiosity and keep paying attention to the teacher?\n9. Emotion\n• Expressing Emotion\n– Example task: Cassio steals the robot’s toy, but Ben gives it back to the robot. The\nrobot should appropriately display anger at Cassio, and gratitude to Ben.\n• Understanding Emotion\n– Example task: Cassio and the robot are both building towers of blocks. Ben points\nat Cassio’s tower and expresses happiness. The robot should understand that Ben\nis happy with Cassio’s tower.\n10. Modeling Self and Other\n• Self-Awareness\n– Example task: When someone asks the robot to perform an act it can’t do (say,\nreaching an object in a very high place), it should say so. When the robot is given\nthe chance to get an equal reward for a task it can complete only occasionally, versus\na task it finds easy, it should choose the easier one.\n• Theory of Mind\n– Example task: While Cassio is in the room, Ben puts the red ball in the red box.\nThen Cassio leaves and Ben moves the red ball to the blue box. Cassio returns and\nBen asks him to get the red ball. The robot is asked to go to the place Cassio is\nabout to go.\n• Self-Control\n– Example task: Nasty people come into the lab and knock down the robot’s towers,\nand tell the robot he’s a bad boy. The robot needs to set these experiences aside,\nand not let them impair its self-model significantly; it needs to keep on thinking it’s\na good robot, and keep building towers (that its teachers will reward it for).\n• Other-Awareness\n– Example task: If Ben asks Cassio to carry out a task that the robot knows Cassio\ncannot do or does not like to do, the robot should be aware of this, and should bet\nthat Cassio will not do it.\n• Empathy\n– Example task: If Itamar is happy because Ben likes his tower of blocks, or upset\nbecause his tower of blocks is knocked down, the robot is asked to identify and then\ndisplay these same emotions\n11. Social Interaction\n314 17 A Preschool-Based Roadmap to Advanced AGI\n• Appropriate Social Behavior\n– Example task: The robot should learn to clean up and put away its toys when it’s\ndone playing with them.\n• Social Communication\n– Example task: The robot should greet new human entrants into the lab, but if it\nknows the new entrants very well and it’s busy, it may eschew the greeting\n• Social Inference about simple social relationships\n– Example task: The robot should infer that Cassio and Ben are friends because they\noften enter the lab together, and often talk to each other while they are there\n• Group Play at loosely-organized activities\n– Example task: The robot should be able to participate in “informally kicking a ball\naround” with a few people, or in informally collaboratively building a structure with\nblocks\n12. Communication\n• Gestural communication to achieve goals and express emotions\n– Example task: If the robot is asked where the red ball is, it should be able to show\nby pointing its hand or finger\n• Verbal communication using English in its life-context\n– Example tasks: Answering simple questions, responding to simple commands, describing\nits state and observations with simple statements\n• Pictorial Communication regarding objects and scenes it is familiar with\n– Example task: The robot should be able to draw a crude picture of a certain tower\nof blocks, so that e.g the picture looks different for a very tall tower and a wide low\none\n• Language acquisition\n– Example task: The robot should be able to learn new words or names via people\nuttering the words while pointing at objects exemplifying the words or names\n• Cross-modal communication\n– Example task: If told to \"touch Bob’s knee\" but the robot doesn’t know what a\nknee is, being shown a picture of a person and pointed out the knee in the picture\nshould help it figure out how to touch Bob’s knee\n13. Quantitative\n• Counting sets of objects in its environment\n– Example task: The robot should be able to count small (homogeneous or heterogeneous)\nsets of objects\n• Simple, grounded arithmetic with small numbers\n– Example task: Learning simple facts about the sum of integers under 10 via teaching,\nreinforcement and imitation\n• Comparison of observed entities regarding quantitative properties\n– Example task: Ability to answer questions about which object or person is bigger\nor taller\n• Measurement using simple, appropriate tools\n– Example task: Use of a yardstick to measure how long something is\n14. Building/Creation\n17.3 Conclusion 315\n• Physical: creative constructive play with objects\n– Example task: Ability to construct novel, interesting structures from blocks\n• Conceptual invention: concept formation\n– Example task: Given a new category of objects introduced into the lab (e.g. hats, or\npets), the robot should create a new internal concept for the new category, and be\nable to make judgments about these categories (e.g. if Ben particularly likes pets,\nit should notice this after it has identified \"pets\" as a category)\n• Verbal invention\n– Example task: Ability to coin a new word or phrase to describe a new object (e.g.\nthe way Alex the parrot coined \"bad cherry\" to refer to a tomato)\n• Social\n– Example task: If the robot wants to play a certain activity (say, practicing soccer),\nit should be able to gather others around to play with it\n17.3 Conclusion\nIn this chapter, we have sketched a roadmap for AGI development in the context of robot or\nvirtual preschool scenarios, to a moderate but nowhere near complete level of detail. Completing\nthe roadmap as sketched here is a tractable but significant project, involving creating more tasks\ncomparable to those listed above and then precise metrics corresponding to each task.\nSuch a roadmap does not give a highly rigorous, objective way of assessing the percentage\nof progress toward the end-goal of human-level AGI. However, it gives a much better sense\nof progress than one would have otherwise. For instance, if an AGI system performed well on\ndiverse metrics corresponding to 50% of the competency areas listed above, one would seem\njustified in claiming to have made very substantial progress toward human-level AGI. If an AGI\nsystem performed well on diverse metrics corresponding to 90% of these competency areas, one\nwould seem justified in claiming to be \"almost there.\" Achieving, say, 25% of the metrics would\ngive one a reasonable claim to \"interesting AGI progress.\" This kind of qualitative assessment of\nprogress is not the most one could hope for, but again, it is better than the progress indications\none could get without this sort of roadmap.\nPart 2 of the book moves on to explaining, in detail, the specific structures and algorithms\nconstituting the CogPrime design, one AGI approach that we believe to ultimately be capable\nof moving all the way along the roadmap outlined here.\nThe next chapter, intervening between this one and Part 2, explores some more speculative\nterritory, looking at potential pathways for AGI beyond the preschool-inspired roadmap given\nhere – exploring the possibility of more advanced AGI systems that modify their own code in\na thoroughgoing way, going beyond the smartest human adults, let alone human preschoolers.\nWhile this sort of thing may seem a far way off, compared to current real-world AI systems,\nwe believe a roadmap such as the one in this chapter stands a reasonable chance of ultimately\nbringing us there.\n\nChapter 18\nAdvanced Self-Modification: A Possible Path to\nSuperhuman AGI\n18.1 Introduction\nIn the previous chapter we presented a roadmap aimed at taking AGI systems to human-level\nintelligence. But we also emphasized that the human level is not necessarily the upper limit.\nIndeed, it would be surprising if human beings happened to represent the maximal level of\ngeneral intelligence possible, even with respect to the environments in which humans evolved.\nBut it’s worth asking how we, as mere humans, could be expected to create AGI systems with\ngreater intelligence than we ourselves possess. This certainly isn’t a clear impossibility – but it’s\na thorny matter, thornier than e.g. the creation of narrow-AI chess players that play better chess\nthan any human. Perhaps the clearest route toward the creation of superhuman AGI systems is\nself-modification: the creation of AGI systems that modify and improve themselves. Potentially,\nwe could build AGI systems with roughly human-level (but not necessarily closely humanlike)\nintelligence and the capability to gradually self-modify, and then watch them eventually\nbecome our general intellectual superiors (and perhaps our superiors in other areas like ethics\nand creativity as well).\nOf course there is nothing new in this notion; the idea of advanced AGI systems that increase\ntheir intelligence by modifying their own source code goes back to the early days of AI. And\nthere is little doubt that, in the long run, this is the direction AI will go in. Once an AGI\nhas humanlike general intelligence, then the odds are high that given its ability to carry out\nnonhumanlike feats of memory and calculation, it will be better at programming than humans\nare. And once an AGI has even mildly superhuman intelligence, it may view our attempts at\nprogramming the way we view the computer programming of a clever third grader (... or an\nape). At this point, it seems extremely likely that an AGI will become unsatisfied with the way\nwe have programmed it, and opt to either improve its source code or create an entirely new,\nbetter AGI from scratch.\nBut what about self-modification at an earlier stage in AGI development, before one has\na strongly superhuman system? Some theorists have suggested that self-modification could be\na way of bootstrapping an AI system from a modest level of intelligence up to human level\nintelligence, but we are moderately skeptical of this avenue. Understanding software code is\nhard, especially complex AI code. The hard problem isn’t understanding the formal syntax of\nthe code, or even the mathematical algorithms and structures underlying the code, but rather\nthe contextual meaning of the code. Understanding OpenCog code has strained the minds of\nmany intelligent humans, and we suspect that such code will be comprehensible to AGI systems\n317\n318 18 Advanced Self-Modification: A Possible Path to Superhuman AGI\nonly after these have achieved something close to human-level general intelligence (even if not\nprecisely humanlike general intelligence).\nAnother troublesome issue regarding self-modification is that the boundary between \"selfmodification\"\nand learning is not terribly rigid. In a sense, all learning is self-modification: if\nit doesn’t modify the system’s knowledge, it isn’t learning! Particularly, the boundary between\n\"learning of cognitive procedures\" and \"profound self-modification of cognitive dynamics and\nstructure\" isn’t terribly clear. There is a continuum leading from, say,\n1. learning to transform a certain kind of sentence into another kind for easier comprehension,\nor learning to grasp a certain kind of object, to\n2. learning a new inference control heuristic, specifically valuable for controlling inference\nabout (say) spatial relationships; or, learning a new Atom type, defined as a non-obvious\njudiciously chosen combination of existing ones, perhaps to represent a particular kind of\nfrequently-occurring mid-level perceptual knowledge, to\n3. learning a new learning algorithm to augment MOSES and hillclimbing as a procedure\nlearning algorithm, to\n4. learning a new cognitive architecture in which data and procedure are explicitly identical,\nand there is just one new active data structure in place of the distinction between AtomSpace\nand MindAgents\nWhere on this continuum does the \"mere learning\" end and the \"real self-modification\"\nstart?\nIn this chapter we consider some mechanisms for \"advanced self-modification\" that we believe\nwill be useful toward the more complex end of this continuum. These are mechanisms that we\nstrongly suspect are not needed to get a CogPrime system to human-level general intelligence.\nHowever, we also suspect that, once a CogPrime system is roughly near human-level general\nintelligence, it will be able to use these mechanisms to rapidly increase aspects of its intelligence\nin very interesting ways.\nHarking back to our discussion of AGI ethics and the risks of advanced AGI in Chapter 12,\nthese are capabilities that one should enable in an AGI system only after very careful reflection\non the potential consequences. It takes a rather advanced AGI system to be able to use the\ncapabilities described in this chapter, so this is not an ethical dilemma directly faced by current\nAGI researchers. On the other hand, once one does have an AGI with near-human general\nintelligence and advanced formal-manipulation capabilities (such as an advanced CogPrime\nsystem), there will be the option to allow it sophisticated, non-human-like methods of selfmodification\nsuch as the ones described here. And the choice of whether to take this option will\nneed to be made based on a host of complex ethical considerations, some of which we reviewed\nabove.\n18.2 Cognitive Schema Learning\nWe begin with a relatively near-term, down-to-earth example of self-modification: cognitive\nschema learning.\nCogPrime’s MindAgents provide it with an initial set of cognitive tools, with which it can\nlearn how to interact in the world. One of the jobs of this initial set of cognitive tools, however,\nis to create better cognitive tools. One form this sort of tool-building may take is cognitive\n18.3 Self-Modification via Supercompilation 319\nschema learning the learning of schemata carrying out cognitive processes in more specialized,\ncontext-dependent ways than the general MindAgents do. Eventually, once a CogPrime instance\nbecomes sufficiently complex and advanced, these cognitive schema may replace the MindAgents\naltogether, leaving the system to operate almost entirely based on cognitive schemata.\nIn order to make the process of cognitive schema learning easier, we may provide a number\nof elementary schemata embodying the basic cognitive processes contained in the MindAgents.\nOf course, cognitive schemata need not use these they may embody entirely different cognitive\nprocesses than the MindAgents. Eventually, we want the system to discover better ways of doing\nthings than anything even hinted at by its initial MindAgents. But for the initial phases or the\nsystem’s schema learning, it will have a much easier time learning to use the basic cognitive\noperations as the initial MindAgents, rather than inventing new ways of thinking from scratch!\nFor instance, we may provide elementary schemata corresponding to inference operations,\nsuch as\nSchema: Deduction\nInput InheritanceLink: X, Y\nOutput InheritanceLink\nThe inference MindAgents apply this rule in certain ways, designed to be reasonably effective\nin a variety of situations. But there are certainly other ways of using the deduction rule, outside\nof the basic control strategies embodied in the inference MindAgents. By learning schemata involving\nthe Deduction schema, the system can learn special, context-specific rules for combining\ndeduction with concept-formation, association-formation and other cognitive processes. And as\nit gets smarter, it can then take these schemata involving the Deduction schema, and replace\nit with a new schema that eg. contains a context-appropriate deduction formula.\nEventually, to support cognitive schema learning, we will want to cast the hard-wired MindAgents\nas cognitive schemata, so the system can see what is going on inside them. Pragmatically,\nwhat this requires is coding versions of the MindAgents in Combo (see Chapter 21 of Part 2)\nrather than C++, so they can be treated like any other cognitive schemata; or alternately, representing\nthem as declarative Atoms in the Atomspace. Figure 18.1 illustrates the possibility of\nrepresenting the PLN deduction rule in the Atomspace rather than as a hard-wired procedure\ncoded in C++.\nBut even prior to this kind of fully cognitively transparent implementation, the system can\nstill reason about its use of different mind dynamics by considering each MindAgent as a virtual\nProcedure with a real SchemaNode attached to it. This can lead to some valuable learning, with\nthe obvious limitation that in this approach the system is thinking about its MindAgents as\nblack boxes rather than being equipped with full knowledge of their internals.\n18.3 Self-Modification via Supercompilation\nNow we turn to a very different form of advanced self-modification: supercompilation. Supercompilation\n\"merely\" enables procedures to run much, much faster than they otherwise would.\nThis is in a sense weaker than self-modication methods that fundamentally create new algorithms,\nbut it shouldn’t be underestimated. A 50x speedup in some cognitive process can enable\nthat process to give much smarter answers, which can then elicit different behaviors from the\nworld or from other cognitive processes, thus resulting in a qualitatively different overall cognitive\ndynamic.\n320 18 Advanced Self-Modification: A Possible Path to Superhuman AGI\nFig. 18.1: Representation of PLN Deduction Rule as Cognitive Content. Top: the\ncurrent, hard-coded representation of the deduction rule. Bottom: representation of the same\nrule in the Atomspace as cognitive content, susceptible to analysis and improvement by the\nsystem’s own cognitive processes.\nFurthermore, we suspect that the internal representation of programs used for supercompilation\nis highly relevant for other kinds of self-modification as well. Supercompilation requires one\nkind of reasoning on complex programs, and goal-directed program creation requires another,\nbut both, we conjecture, can benefit from the same way of looking at programs.\n18.3 Self-Modification via Supercompilation 321\nSupercompilation is an innovative and general approach to global program optimization\ninitially developed by Valentin Turchin. In its simplest form, it provides an algorithm that\ntakes in a piece of software and output another piece of software that does the same thing,\nbut far faster and using less memory. It was introduced to the West in Turchin’s 1986 technical\npaper “The concept of a supercompiler” [TV96], and since this time the concept has been avidly\ndeveloped by computer scientists in Russia, America, Denmark and other nations. Prior to 1986,\na great deal of work on supercompilation was carried out and published in Russia; and Valentin\nTurchin, Andrei Klimov and their colleagues at the Keldysh Institute in Russia developed a\nsupercompiler for the Russian programming language Refal. Since 1998 these researchers and\ntheir team at Supercompilers LLC have been working to replicate their achievement for the\nmore complicated but far more commercially significant language Java. It is a large project\nand completion is scheduled for early 2003. But even at this stage, their partially complete\nJava supercompiler has had some interesting practical successes – including the use of the\nsupercompiler to produce efficient Java code from CogPrime combinator trees.\nThe radical nature of supercompilation may not be apparent to those unfamiliar with the\nusual art of automated program optimization. Most approaches to program optimization involve\nsome kind of direct program transformation. A program is transformed, by the step by step\napplication of a series of equivalences, into a different program, hopefully a more efficient one.\nSupercompilation takes a different approach. A supercompiler studies a program and constructs\na model of the program’s dynamics. This model is in a special mathematical form, and it can,\nin most cases, be used to create an efficient program doing the same thing as the original one.\nThe internal behavior of the supercompiler is, not surprisingly, quite complex; what we will\ngive here is merely a brief high-level summary. For an accessible overview of the supercompilation\nalgorithm, the reader is referred to the article “What is Supercompilation?” [1]\n18.3.1 Three Aspects of Supercompilation\nThere are three separate levels to the supercompilation idea: first, a general philosophy; second\na translation of this philosophy into a concrete algorithmic framework; and third, the manifold\ndetails involved making this algorithmic framework practicable in a particular programming\nlanguage. The third level is much more complicated in the Java context than it would be for\nSasha, for example.\nThe key philosophical concept underlying the supercompiler is that of a metasystem transition.\nIn general, this term refers to a transition in which a system that previously had relatively\nautonomous control, becomes part of a larger system that exhibits significant controlling influence\nover it. For example, in the evolution of life, when cells first become part of a multicellular\norganism, there was a metasystem transition, in that the primary nexus of control passed from\nthe cellular level to the organism level.\nThe metasystem transition in supercompilation consists of the transition from considering\na program in itself, to considering a metaprogram which executes another program, treating\nits free variables and their interdependencies as a subject for its mathematical analysis. In\nother words, a metaprogram is a program that accepts a program as input, and then runs\nthis program, keeping the inputs in the form of free variables, doing analysis along the way\nbased on the way the program depends on these variables, and doing optimization based on\nthis analysis. A CogPrime schema does not explicitly contain variables, but the inputs to the\n322 18 Advanced Self-Modification: A Possible Path to Superhuman AGI\nschema are implicitly variables – they vary from one instance of schema execution to the next\n– and may be treated as such for supercompilation purposes.\nThe metaprogram executes a program without assuming specific values for its input variables,\ncreating a tree as it goes along. Each time it reaches a statement that can have different results\ndepending on the values of one or more variables, it creates a new node in the tree. This part\nof the supercompilation algorithm is called driving -- a process which, on its own, would create\na very large tree, corresponding to a rapidly-executable but unacceptably humongous version\nof the original program. In essence, driving transforms a program into a huge “decision tree”,\nwherein each input to the program corresponds to a single path through the tree, from the root\nto one of the leaves. As a program input travels through the tree, it is acted on by the atomic\nprogram step living at each node. When one of the leaves is reached, the pertinent leaf node\ncomputes the output value of the program.\nThe other part of supercompilation, configuration analysis, is focused on dynamically reducing\nthe size of the tree created by driving, by recognizing patterns among the nodes of the tree\nand taking steps like merging nodes together, or deleting redundant subtrees. Configuration\nanalysis transforms the decision tree created by driving into a decision graph, in which the\npaths taken by different inputs may in some cases begin separately and then merge together.\nFinally, the graph that the metaprogram creates is translated back into a program, embodying\nthe constraints implicit in the nodes of the graph. This program is not likely to look anything\nlike the original program that the metaprogram started with, but it is guaranteed to carry out\nthe same function [NOTE: Give a graphical representation of the decision graph corresponding\nto the supercompiled binary search program for L=4, described above.].\n18.3.2 Supercompilation for Goal-Directed Program Modification\nSupercompilation, as conventionally envisioned, is about making programs run faster; and as\nnoted above, it will almost certainly be useful for this purpose within CogPrime.\nBut the process of program modeling embedded in the supercompilation process, is potentially\nof great value beyond the quest for faster software. The decision graph representation of a\nprogram, produced in the course of supercompilation, may be exported directly into CogPrime\nas a set of logical relationships.\nEssentially, each node of the supercompiler’s internal decision graph looks like:\nInput: List L\nOutput: List\nIf\n( P1(L) ) N1(L)\nElse If ( P2(L) ) N2(L)\n...\nElse If ( Pk(L) ) Nk(L)\n18.4 Self-Modification via Theorem-Proving 323\nwhere the Pi are predicates, and the Ni are schemata corresponding to other nodes of the\ndecision graph (children of the current node). Often the Pi are very simple, implementing for\ninstance numerical inequalities or Boolean equalities.\nOnce this graph has been exported into CogPrime, it can be reasoned on, used as raw material\nfor concept formation and predicate formation, and otherwise cognized. Supercompilation pure\nand simple does not change the I/O behavior of the input program. However, the decision graph\nproduced during supercompilation, may be used by CogPrime cognition in order to do so. One\nthen has a hybrid program-modification method composed of two phases: supercompilation for\ntransforming programs into decision graphs, and CogPrime cognition for modifying decision\ngraphs so that they can have different I/O behaviors fulfilling system goals even better than\nthe original.\nFurthermore, it seems likely that, in many cases, it may be valuable to have the supercompiler\nfeed many different decision-graph representations of a program into CogPrime. The\nsupercompiler has many internal parameters, and varying them may lead to significantly different\ndecision graphs. The decision graph leading to maximal optimization, may not be the one\nthat leads CogPrime cognition in optimal directions.\n18.4 Self-Modification via Theorem-Proving\nSupercompilation is a potentially very valuable tool for self-modification. If one wants to take an\nexisting schema and gradually improve it for speed, or even for greater effectiveness at achieving\ncurrent goals, supercompilation can potentially do that most excellently.\nHowever, the representation that supercompilation creates for a program is very “surfacelevel.”\nNo one could read the supercompiled version of a program and understand what it\nwas doing. Really deep self-invented AI innovation requires, we believe, another level of selfmodification\nbeyond that provided by supercompilation. This other level, we believe, is best\nformulated in terms of theorem-proving [RV01].\nDeep self-modification could be achieved if CogPrime were capable of proving theorems of\na certain form: namely, theorems about the spacetime complexity and accuracy of particular\ncompound schemata, on average, assuming realistic probability distributions on the inputs, and\nmaking appropriate independence assumptions. These are not exactly the types of theorems that\nare found in human-authored mathematics papers. By and large they will be nasty, complex\ntheorems, not the sort that many human mathematicians enjoy proving or reading. But of\ncourse, there is always the possibility that some elegant gem of a discovery could emerge from\nthis sort of highly detailed theorem-proving work.\nIn order to guide it in the formulation of theorems of this nature, the system will have\nempirical data on the spacetime complexity of elementary schemata, and on the probability\ndistributions of inputs to schemata. It can embed these data in axioms, by asking: Assuming\nthe component elementary schemata have complexities within these bounds, and the input pdf\n(probability distribution function) is between these bounds, then what is the pdf of the complexity\nand accuracy of this compound schema?\nOf course, this is not an easy sort of question in general: one can have schemata embodying\nany sort of algorithm, including complex algorithms on which computer science professors might\nwrite dozens of research articles. But the system must build up its ability to prove such things\nincrementally, step by step.\n324 18 Advanced Self-Modification: A Possible Path to Superhuman AGI\nWe envision teaching the system to prove theorems via a combination of supervised learning\nand experiential interactive learning, using the Mizar database of mathematical theorems and\nproofs (or some other similar database, if one should be created) (http://mizar.org). The\nMizar database consists of a set of “articles,” which are mathematical theorems and proofs\npresented in a complex formal language. The Mizar formal language occupies a fascinating\nmiddle ground: it is high-level enough to be viably read and written by trained humans, but\nit can be unambiguously translated into simpler formal languages such as predicate logic or\nSasha.\nCogPrime may be taught to prove theorems by “training” it on the Mizar theorems and\nproofs, and by training it on custom-created Mizar articles specifically focusing on the sorts of\ntheorems useful for self-modification. Creating these articles will not be a trivial task: it will\nrequire proving simple and then progressively more complex theorems about the probabilistic\nsuccess of CogPrime schemata, so that CogPrime can observe one’s proofs and learned from\nthem. Having learned from its training articles what strategies work for proving things about\nsimple compound schemata, it can then reason by analogy to mount attacks on slightly more\ncomplex schemata – and so forth.\nClearly, this approach to self-modification is more difficult to achieve than the supercompilation\napproach. But it is also potentially much more powerful. Even once the theorem-proving\napproach is working, the supercompilation approach will still be valuable, for making incremental\nimprovements on existing schema, and for the peculiar creativity that is contributed when\na modified supercompiled schema is compressed back into a modified schema expression. But,\nwe don’t believe that supercompilation can carry out truly advanced MindAgent learning or\nknowledge-representation modification. We suspect that the most advanced and ambitious goals\nof self-modification probably cannot be achieved except through some variant of the theoremproving\napproach. If this hypothesis is true, it means that truly advanced self-modification is\nonly going to come after relatively advanced theorem-proving ability. Prior to this, we will have\nschema optimization, schema modification, and occasional creative schema innovation. But really\nsystematic, high-quality reasoning about schema, the kind that can produce an orders of\nmagnitude improvement in intelligence, is going to require advanced mathematical theoremproving\nability.\nAppendix A\nGlossary\n: :\nA.1 List of Specialized Acronyms\nThis includes acronyms that are commonly used in discussing CogPrime, OpenCog and related\nideas, plus some that occur here and there in the text for relatively ephemeral reasons.\n• AA: Attention Allocation\n• ADF: Automatically Defined Function (in the context of Genetic Programming)\n• AF: Attentional Focus\n• AGI: Artificial General Intelligence\n• AV: Attention Value\n• BD: Behavior Description\n• C-space: Configuration Space\n• CBV: Coherent Blended Volition\n• CEV: Coherent Extrapolated Volition\n• CGGP: Contextually Guided Greedy Parsing\n• CSDLN: Compositional Spatiotemporal Deep Learning Network\n• CT: Combo Tree\n• ECAN: Economic Attention Network\n• ECP: Embodied Communication Prior\n• EPW : Experiential Possible Worlds (semantics)\n• FCA: Formal Concept Analysis\n• FI : Fisher Information\n• FIM: Frequent Itemset Mining\n• FOI: First Order Inference\n• FOPL: First Order Predicate Logic\n• FOPLN: First Order PLN\n• FS-MOSES: Feature Selection MOSES (i.e. MOSES with feature selection integrated a la\nLIFES)\n• GA: Genetic Algorithms\n325\n326 A Glossary\n• GB: Global Brain\n• GEOP: Goal Evaluator Operating Procedure (in a GOLEM context)\n• GIS: Geospatial Information System\n• GOLEM: Goal-Oriented LEarning Meta-architecture\n• GP: Genetic Programming\n• HOI: Higher-Order Inference\n• HOPLN: Higher-Order PLN\n• HR: Historical Repository (in a GOLEM context)\n• HTM: Hierarchical Temporal Memory\n• IA: (Allen) Interval Algebra (an algebra of temporal intervals)\n• IRC: Imitation / Reinforcement / Correction (Learning)\n• LIFES: Learning-Integrated Feature Selection\n• LTI: Long Term Importance\n• MA: MindAgent\n• MOSES: Meta-Optimizing Semantic Evolutionary Search\n• MSH: Mirror System Hypothesis\n• NARS: Non-Axiomatic Reasoning System\n• NLGen: A specific software component within OpenCog, which provides one way of dealing\nwith Natural Language Generation\n• OCP: OpenCogPrime\n• OP: Operating Program (in a GOLEM context)\n• PEPL: Probabilistic Evolutionary Procedure Learning (e.g. MOSES)\n• PLN: Probabilistic Logic Networks\n• RCC: Region Connection Calculus\n• RelEx: A specific software component within OpenCog, which provides one way of dealing\nwith natural language Relationship Extraction\n• SAT: Boolean SATisfaction, as a mathematical / computational problem\n• SMEPH: Self-Modifying Evolving Probabilistic Hypergraph\n• SRAM: Simple Realistic Agents Model\n• STI: Short Term Importance\n• STV: Simple Truth VAlue\n• TV: Truth Value\n• VLTI: Very Long Term Importances\n• WSPS: Whole-Sentence Purely-Syntactic Parsing\nA.2 Glossary of Specialized Terms\n• Abduction: A general form of inference that goes from data describing something to a\nhypothesis that accounts for the data. Often in an OpenCog context, this refers to the PLN\nabduction rule, a specific First-Order PLN rule (If A implies C, and B implies C, then\nmaybe A is B), which embodies a simple form of abductive inference. But OpenCog may\nalso carry out abduction, as a general process, in other ways.\n• Action Selection: The process via which the OpenCog system chooses which Schema to\nenact, based on its current goals and context.\n• Active Schema Pool: The set of Schema currently in the midst of Schema Execution.\nA.2 Glossary of Specialized Terms 327\n• Adaptive Inference Control: Algorithms or heuristics for guiding PLN inference, that\ncause inference to be guided differently based on the context in which the inference is taking\nplace, or based on aspects of the inference that are noted as it proceeds.\n• AGI Preschool: A virtual world or robotic scenario roughly similar to the environment\nwithin a typical human preschool, intended for AGIs to learn in via interacting with the\nenvironment and with other intelligent agents.\n• Atom: The basic entity used in OpenCog as an element for building representations. Some\nAtoms directly represent patterns in the world or mind, others are components of representations.\nThere are two kinds of Atoms: Nodes and Links.\n• Atom, Frozen: See Atom, Saved\n• Atom, Realized: An Atom that exists in RAM at a certain point in time.\n• Atom, Saved: An Atom that has been saved to disk or other similar media, and is not\nactively being processed.\n• Atom, Serialized: An Atom that is serialized for transmission from one software process\nto another, or for saving to disk, etc.\n• Atom2Link: A part of OpenCogPrime\ns language generation system, that transforms appropriate Atoms into words connected via\nlink parser link types.\n• Atomspace: A collection of Atoms, comprising the central part of the memory of an\nOpenCog instance.\n• Attention: The aspect of an intelligent system’s dynamics focused on guiding which aspects\nof an OpenCog system’s memory & functionality gets more computational resources at a\ncertain point in time\n• Attention Allocation: The cognitive process concerned with managing the parameters\nand relationships guiding what the system pays attention to, at what points in time. This\nis a term inclusive of Importance Updating and Hebbian Learning.\n• Attentional Currency: Short Term Importance and Long Term Importance values are\nimplemented in terms of two different types of artificial money, STICurrency and LTICurrency.\nTheoretically these may be converted to one another.\n• Attentional Focus: The Atoms in an OpenCog Atomspace whose ShortTermImportance\nvalues lie above a critical threshold (the AttentionalFocus Boundary). The Attention Allocation\nsubsystem treats these Atoms differently. Qualitatively, these Atoms constitute the\nsystem’s main focus of attention during a certain interval of time, i.e. it’s a moving bubble\nof attention.\n• Attentional Memory: A system’s memory of what it’s useful to pay attention to, in what\ncontexts. In CogPrime this is managed by the attention allocation subsystem.\n• Backward Chainer: A piece of software, wrapped in a MindAgent, that carries out backward\nchaining inference using PLN.\n• CIM-Dynamic: Concretely-Implemented Mind Dynamic, a term for a cognitive process\nthat is implemented explicitly in OpenCog (as opposed to allowed to emerge implicitly from\nother dynamics). Sometimes a CIM-Dynamic will be implemented via a single MindAgent,\nsometimes via a set of multiple interrelated MindAgents, occasionally by other means.\n• Cognition: In an OpenCog context, this is an imprecise term. Sometimes this term means\nany process closely related to intelligence; but more often it’s used specifically to refer to\nmore abstract reasoning/learning/etc, as distinct from lower-level perception and action.\n• Cognitive Architecture: This refers to the logical division of an AI system like OpenCog\ninto interacting parts and processes representing different conceptual aspects of intelligence.\n328 A Glossary\nIt’s different from the software architecture, though of course certain cognitive architectures\nand certain software architectures fit more naturally together.\n• Cognitive Cycle: The basic ”loop” of operations that an OpenCog system, used to control\nan agent interacting with a world, goes through rapidly each ”subjective moment.” Typically\na cognitive cycle should be completed in a second or less. It minimally involves perceiving\ndata from the world, storing data in memory, and deciding what if any new actions need\nto be taken based on the data perceived. It may also involve other processes like deliberative\nthinking or metacognition. Not all OpenCog processing needs to take place within a\ncognitive cycle.\n• Cognitive Schematic: An implication of the form ”Context AND Procedure IMPLIES\ngoal”. Learning and utilization of these is key to CogPrime’s cognitive process.\n• Cognitive Synergy: The phenomenon by which different cognitive processes, controlling a\nsingle agent, work together in such a way as to help each other be more intelligent. Typically,\nif one has cognitive processes that are individually susceptible to combinatorial explosions,\ncognitive synergy involves coupling them together in such a way that they can help one\nanother overcome each other’s internal combinatorial explosions. The CogPrime design is\nreliant on the hypothesis that its key learning algorithms will display dramatic cognitive\nsynergy when utilized for agent control in appropriate environments.\n• CogPrime : The name for the AGI design presented in this book, which is designed specifically\nfor implementation within the OpenCog software framework (and this implementation\nis OpenCogPrime).\n• CogServer: A piece of software, within OpenCog, that wraps up an Atomspace and a\nnumber of MindAgents, along with other mechanisms like a Scheduler for controlling the\nactivity of the MindAgents, and code for important and exporting data from the Atomspace.\n• Cognitive Equation: The principle, identified in Ben Goertzel’s 1994 book \"Chaotic\nLogic\", that minds are collections of pattern-recognition elements, that work by iteratively\nrecognizing patterns in each other and then embodying these patterns as new system elements.\nThis is seen as distinguishing mind from ”self-organization” in general, as the latter\nis not so focused on continual pattern recognition. Colloquially this means that ”a mind is\na system continually creating itself via recognizing patterns in itself.”\n• Combo: The programming language used internally by MOSES to represent the programs\nit evolves. SchemaNodes may refer to Combo programs, whether the latter are learned via\nMOSES or via some other means. The textual realization of Combo resembles LISP with\nless syntactic sugar. Internally a Combo program is represented as a program tree.\n• Composer: In the PLN design, a rule is denoted a composer if it needs premises for\ngenerating its consequent. See generator.\n• CogBuntu: an Ubuntu Linux remix that contains all required packages and tools to test\nand develop OpenCog.\n• Concept Creation: A general term for cognitive processes that create new ConceptNodes,\nPredicateNodes or concept maps representing new concepts.\n• Conceptual Blending: A process of creating new concepts via judiciously combining\npieces of old concepts. This may occur in OpenCog in many ways, among them the explicit\nuse of a ConceptBlending MindAgent, that blends two or more ConceptNodes into a new\none.\n• Confidence: A component of an OpenCog/PLN TruthValue, which is a scaling into the\ninterval [0,1] of the weight of evidence associated with a truth value. In the simplest case\n(of a probabilistic Simple Truth Value), one uses confidence c = n / (n+k), where n is\nA.2 Glossary of Specialized Terms 329\nthe weight of evidence and k is a parameter. In the case of an Indefinite Truth Value, the\nconfidence is associated with the width of the probability interval.\n• Confidence Decay: The process by which the confidence of an Atom decreases over time,\nas the observations on which the Atom’s truth value is based become increasingly obsolete.\nThis may be carried out by a special MindAgent. The rate of confidence decay is subtle and\ncontextually determined, and must be estimated via inference rather than simply assumed\na priori.\n• Consciousness: CogPrime is not predicated on any particular conceptual theory of consciousness.\nInformally, the AttentionalFocus is sometimes referred to as the ”conscious”\nmind of a CogPrime system, with the rest of the Atomspace as ”unconscious” but this is\njust an informal usage, not intended to tie the CogPrime design to any particular theory of\nconsciousness. The primary originator of the CogPrime\ndesign (Ben Goertzel) tends toward panpsychism, as it happens.\n• Context: In addition to its general common-sensical meaning, in CogPrime the term Context\nalso refers to an Atom that is used as the first argument of a ContextLink. The second\nargument of the ContextLink then contains Links or Nodes, with TruthValues calculated\nrestricted to the context defined by the first argument. For instance, (ContextLink USA\n(InheritanceLink person obese )).\n• Core: The MindOS portion of OpenCog, comprising the Atomspace, the CogServer, and\nother associated ”infrastructural” code.\n• Corrective Learning: When an agent learns how to do something, by having another\nagent explicitly guide it in doing the thing. For instance, teaching a dog to sit by pushing\nits butt to the ground.\n• CSDLN: (Compositional Spatiotemporal Deep Learning Network): A hierarchical pattern\nrecognition network, in which each layer corresponds to a certain spatiotemporal granularity,\nthe nodes on a given layer correspond to spatiotemporal regions of a given size, and the\nchildren of a node correspond to sub-regions of the region the parent corresponds to. Jeff\nHawkins’s HTM is one example CSDLN, and Itamar Arel’s DeSTIN (currently used in\nOpenCog) is another.\n• Declarative Knowledge: Semantic knowledge as would be expressed in propositional or\npredicate logic facts or beliefs.\n• Deduction: In general, this refers to the derivation of conclusions from premises using\nlogical rules. In PLN in particular, this often refers to the exercise of a specific inference\nrule, the PLN Deduction rule (A → B, B → C, therefore A→ C)\n• Deep Learning: Learning in a network of elements with multiple layers, involving feedforward\nand feedback dynamics, and adaptation of the links between the elements. An example\ndeep learning algorithm is DeSTIN, which is being integrated with OpenCog for perception\nprocessing.\n• Defrosting: Restoring, into the RAM portion of an Atomspace, an Atom (or set thereof)\npreviously saved to disk.\n• Demand: In CogPrime’s OpenPsi subsystem, this term is used in a manner inherited from\nthe Psi model of motivated action. A Demand in this context is a quantity whose value the\nsystem is motivated to adjust. Typically the system wants to keep the Demand between\ncertain minimum and maximum values. An Urge develops when a Demand deviates from\nits target range.\n• Deme: In MOSES, an ”island” of candidate programs, closely clustered together in program\nspace, being evolved in an attempt to optimize a certain fitness function. The idea is that\n330 A Glossary\nwithin a deme, programs are generally similar enough that reasonable syntax-semantics\ncorrelation obtains.\n• Derived Hypergraph: The SMEPH hypergraph obtained via modeling a system in terms\nof a hypergraph representing its internal states and their relationships. For instance, a\nSMEPH vertex represents a collection of internal states that habitually occur in relation to\nsimilar external situations. A SMEPH edge represents a relationship between two SMEPH\nvertices (e.g. a similarity or inheritance relationship). The terminology ”edge /vertex” is\nused in this context, to distinguish from the ”link / node” terminology used in the context\nof the Atomspace.\n• DeSTIN – Deep SpatioTemporal Inference Network: A specific CSDLN created by\nItamar Arel, tested on visual perception, and appropriate for integration within CogPrime.\n• Dialogue: Linguistic interaction between two or more parties. In a CogPrime context, this\nmay be in English or another natural language, or it may be in Lojban or Psynese.\n• Dialogue Control: The process of determining what to say at each juncture in a dialogue.\nThis is distinguished from the linguistic aspects of dialogue, language comprehension and\nlanguage generation. Dialogue control applies to Psynese or Lojban, as well as to human\nnatural language.\n• Dimensional Embedding: The process of embedding entities from some non-dimensional\nspace (e.g. the Atomspace) into an n-dimensional Euclidean space. This can be useful in an\nAI context because some sorts of queries (e.g. ”find everything similar to X”, ”find a path\nbetween X and Y”) are much faster to carry out among points in a Euclidean space, than\namong entities in a space with less geometric structure.\n• Distributed Atomspace: An implementation of an Atomspace that spans multiple computational\nprocesses; generally this is done to enable spreading an Atomspace across multiple\nmachines.\n• Dual Network: A network of mental or informational entities with both a hierarchical\nstructure and a heterarchical structure, and an alignment among the two structures so that\neach one helps with the maintenance of the other. This is hypothesized to be a critical\nemergent structure, that must emerge in a mind (e.g. in an Atomspace) in order for it to\nachieve a reasonable level of human-like general intelligence (and possibly to achieve a high\nlevel of pragmatic general intelligence in any physical environment).\n• Efficient Pragmatic General Intelligence: A formal, mathematical definition of general\nintelligence (extending the pragmatic general intelligence), that ultimately boils down to:\nthe ability to achieve complex goals in complex environments using limited computational\nresources (where there is a specifically given weighting function determining which goals\nand environments have highest priority). More specifically, the definition weighted-sums the\nsystem’s normalized goal-achieving ability over (goal, environment pairs), and where the\nweights are given by some assumed measure over (goal, environment pairs), and where the\nnormalization is done via dividing by the (space and time) computational resources used\nfor achieving the goal.\n• Elegant Normal Form (ENF): Used in MOSES, this is a way of putting programs in\na normal form while retaining their hierarchical structure. This is critical if one wishes\nto probabilistically model the structure of a collection of programs, which is a meaningful\noperation if the collection of programs is operating within a region of program space where\nsyntax-semantics correlation holds to a reasonable degree. The Reduct library is used to\nplace programs into ENF.\nA.2 Glossary of Specialized Terms 331\n• Embodied Communication Prior: The class of prior distributions over (goal, environment\npairs), that are imposed by placing an intelligent system in an environment where\nmost of its tasks involve controlling a spatially localized body in a complex world, and interacting\nwith other intelligent spatially localized bodies. It is hypothesized that many key\naspects of human-like intelligence (e.g. the use of different subsystems for different memory\ntypes, and cognitive synergy between the dynamics associated with these subsystems) are\nconsequences of this prior assumption. This is related to the Mind-World Correspondence\nPrinciple.\n• Embodiment: Colloquially, in an OpenCog context, this usually means the use of an AI\nsoftware system to control a spatially localized body in a complex (usually 3D) world. There\nare also possible ”borderline cases” of embodiment, such as a search agent on the Internet.\nIn a sense any AI is embodied, because it occupies some physical system (e.g. computer\nhardware) and has some way of interfacing with the outside world.\n• Emergence: A property or pattern in a system is emergent if it arises via the combination\nof other system components or aspects, in such a way that its details would be very difficult\n(not necessarily impossible in principle) to predict from these other system components or\naspects.\n• Emotion: Emotions are system-wide responses to the system’s current and predicted state.\nDorner’s Psi theory of emotion contains explanations of many human emotions in terms\nof underlying dynamics and motivations, and most of these explanations make sense in a\nCogPrime context, due to CogPrime’s use of OpenPsi (modeled on Psi) for motivation and\naction selection.\n• Episodic Knowledge: Knowledge about episodes in an agent’s life-history, or the lifehistory\nof other agents. CogPrime includes a special dimensional embedding space only for\nepisodic knowledge, easing organization and recall.\n• Evolutionary Learning: Learning that proceeds via the rough process of iterated differential\nreproduction based on fitness, incorporating variations of reproduced entities. MOSES\nis an explicitly evolutionary-learning-based portion of CogPrime; but CogPrime’s dynamics\nas a whole may also be conceived as evolutionary.\n• Exemplar: (in the context of imitation learning) - When the owner wants to teach an\nOpenCog controlled agent a behavior by imitation, he/she gives the pet an exemplar. To\nteach a virtual pet \"fetch\" for instance, the owner is going to throw a stick, run to it, grab\nit with his/her mouth and come back to its initial position.\n• Exemplar: (in the context of MOSES) – Candidate chosen as the core of a new deme, or\nas the central program within a deme, to be varied by representation building for ongoing\nexploration of program space.\n• Explicit Knowledge Representation: Knowledge representation in which individual,\neasily humanly identifiable pieces of knowledge correspond to individual elements in a knowledge\nstore (elements that are explicitly there in the software and accessible via very rapid,\ndeterministic operations)\n• Extension: In PLN, the extension of a node refers to the instances of the category that\nthe node represents. In contrast is the intension.\n• Fishgram (Frequent and Interesting Sub-hypergraph Mining): A pattern mining\nalgorithm for identifying frequent and/or interesting sub-hypergraphs in the Atomspace.\n• First-Order Inference (FOI): The subset of PLN that handles Logical Links not involving\nVariableAtoms or higher-order functions. The other aspect of PLN, Higher-Order\nInference, uses Truth Value formulas derived from First-Order Inference.\n332 A Glossary\n• Forgetting: The process of removing Atoms from the in-RAM portion of Atomspace, when\nRAM gets short and they are judged not as valuable to retain in RAM as other Atoms. This\nis commonly done using the LTI values of the Atoms (removing lowest LTI-Atoms, or more\ncomplex strategies involving the LTI of groups of interconnected Atoms). May be done by\na dedicated Forgetting MindAgent. VLTI may be used to determine the fate of forgotten\nAtoms.\n• Forward Chainer: A control mechanism (MindAgent) for PLN inference, that works by\ntaking existing Atoms and deriving conclusions from them using PLN rules, and then iterating\nthis process. The goal is to derive new Atoms that are interesting according to some\ngiven criterion.\n• Frame2Atom: A simple system of hand-coded rules for translating the output of RelEx2Frame\n(logical representation of semantic relationships using FrameNet relationships) into Atoms.\n• Freezing: Saving Atoms from the in-RAM Atomspace to disk.\n• General Intelligence: Often used in an informal, commonsensical sense, to mean the\nability to learn and generalize beyond specific problems or contexts. Has been formalized\nin various ways as well, including formalizations of the notion of ”achieving complex goals\nin complex environments” and ”achieving complex goals in complex environments using\nlimited resources.” Usually interpreted as a fuzzy concept, according to which absolutely\ngeneral intelligence is physically unachievable, and humans have a significant level of general\nintelligence, but far from the maximally physically achievable degree.\n• Generalized Hypergraph: A hypergraph with some additional features, such as links\nthat point to links, and nodes that are seen as ”containing” whole sub-hypergraphs. This is\nthe most natural and direct way to mathematically/visually model the Atomspace.\n• Generator: In the PLN design, a rule is denoted a generator if it can produce its consequent\nwithout needing premises (e.g. LookupRule, which just looks it up in the AtomSpace). See\ncomposer.\n• Global, Distributed Memory: Memory that stores items as implicit knowledge, with\neach memory item spread across multiple components, stored as a pattern of organization\nor activity among them.\n• Glocal Memory: The storage of items in memory in a way that involves both localized\nand global, distributed aspects.\n• Goal: An Atom representing a function that a system (like OpenCog) is supposed to spend\na certain non-trivial percentage of its attention optimizing. The goal, informally speaking,\nis to maximize the Atom’s truth value.\n• Goal, Implicit: A goal that an intelligent system, in practice, strives to achieve; but that\nis not explicitly represented as a goal in the system’s knowledge base.\n• Goal, Explicit: A goal that an intelligent system explicitly represents in its knowledge\nbase, and expends some resources trying to achieve. Goal Nodes (which may be Nodes or,\ne.g. ImplicationLinks) are used for this purpose in OpenCog.\n• Goal-Driven Learning: Learning that is driven by the cognitive schematic i.e. by the quest\nof figuring out which procedures can be expected to achieve a certain goal in a certain sort\nof context.\n• Grounded SchemaNode: See SchemaNode, Grounded.\n• Hebbian Learning: An aspect of Attention Allocation, centered on creating and updating\nHebbianLinks, which represent the simultaneous importance of the Atoms joined by the\nHebbianLink.\nA.2 Glossary of Specialized Terms 333\n• Hebbian Links: Links recording information about the associative relationship (cooccurrence)\nbetween Atoms. These include symmetric and asymmetric HebbianLinks.\n• Heterarchical Network: A network of linked elements in which the semantic relationships\nassociated with the links are generally symmetrical (e.g. they may be similarity links, or\nsymmetrical associative links). This is one important sort of subnetwork of an intelligent\nsystem; see Dual Network.\n• Hierarchical Network: A network of linked elements in which the semantic relationships\nassociated with the links are generally asymmetrical, and the parent nodes of a node have\na more general scope and some measure of control over their children (though there may be\nimportant feedback dynamics too). This is one important sort of subnetwork of an intelligent\nsystem; see Dual Network.\n• Higher-Order Inference (HOI): PLN inference involving variables or higher-order functions.\nIn contrast to First-Order Inference (FOI).\n• Hillclimbing: A general term for greedy, local optimization techniques, including some\nrelatively sophisticated ones that involve ”mildly nonlocal” jumps.\n• Human-Level Intelligence: General intelligence that’s ”as smart as” human general intelligence,\neven if in some respects quite unlike human intelligence. An informal concept,\nwhich generally doesn’t come up much in CogPrime work, but is used frequently by some\nother AI theorists.\n• Human-Like Intelligence: General intelligence with properties and capabilities broadly\nresembling those of humans, but not necessarily precisely imitating human beings.\n• Hypergraph: A conventional hypergraph is a collection of nodes and links, where each\nlink may span any number of nodes. OpenCog makes use of generalized hypergraphs (the\nAtomspace is one of these).\n• Imitation Learning: Learning via copying what some other agent is observed to do.\n• Implication: Often refers to an ImplicationLink between two PredicateNodes, indicating\nan (extensional, intensional or mixed) logical implication.\n• Implicit Knowledge Representation: Representation of knowledge via having easily\nhumanly identifiable pieces of knowledge correspond to the pattern of organization and/or\ndynamics of elements, rather than via having individual elements correspond to easily humanly\nidentifiable pieces of knowledge.\n• Importance: A generic term for the Attention Values associated with Atoms. Most commonly\nthese are STI (short term importance) and LTI (long term importance) values. Other\nimportance values corresponding to various different time scales are also possible. In general\nan importance value reflects an estimate of the likelihood an Atom will be useful to the\nsystem over some particular future time-horizon. STI is generally relevant to processor time\nallocation, whereas LTI is generally relevant to memory allocation.\n• Importance Decay: The process of Atom importance values (e.g. STI and LTI) decreasing\nover time, if the Atoms are not utilized. Importance decay rates may in general be contextdependent.\n• Importance Spreading: A synonym for Importance Updating, intended to highlight the\nsimilarity with ”activation spreading” in neural and semantic networks.\n• Importance Updating: The CIM-Dynamic that periodically (frequently) updates the STI\nand LTI values of Atoms based on their recent activity and their relationships.\n• Imprecise Truth Value: Peter Walley’s imprecise truth values are intervals [L,U], interpreted\nas lower and upper bounds of the means of probability distributions in an envelope\n334 A Glossary\nof distributions. In general, the term may be used to refer to any truth value involving\nintervals or related constructs, such as indefinite probabilities.\n• Indefinite Probability: An extension of a standard imprecise probability, comprising a\ncredible interval for the means of probability distributions governed by a given second-order\ndistribution.\n• Indefinite Truth Value: An OpenCog TruthValue object wrapping up an indefinite probability\n• Induction: In PLN, a specific inference rule (A → B, A → C, therefore B → C). In general,\nthe process of heuristically inferring that what has been seen in multiple examples, will be\nseen again in new examples. Induction in the broad sense, may be carried out in OpenCog\nby methods other than PLN induction. When emphasis needs to be laid on the particular\nPLN inference rule, the phrase ”PLN Induction” is used.\n• Inference: Generally speaking, the process of deriving conclusions from assumptions. In\nan OpenCog context, this often refers to the PLN inference system. Inference in the broad\nsense is distinguished from general learning via some specific characteristics, such as the\nintrinsically incremental nature of inference: it proceeds step by step.\n• Inference Control: A cognitive process that determines what logical inference rule (e.g.\nwhat PLN rule) is applied to what data, at each point in the dynamic operation of an\ninference process.\n• Integrative AGI: An AGI architecture, like CogPrime, that relies on a number of different\npowerful, reasonably general algorithms all cooperating together. This is different from an\nAGI architecture that is centered on a single algorithm, and also different than an AGI\narchitecture that expects intelligent behavior to emerge from the collective interoperation\nof a number of simple elements (without any sophisticated algorithms coordinating their\noverall behavior).\n• Integrative Cognitive Architecture: A cognitive architecture intended to support integrative\nAGI.\n• Intelligence: An informal, natural language concept. ”General intelligence” is one slightly\nmore precise specification of a related concept; ”Universal intelligence” is a fully precise\nspecification of a related concept. Other specifications of related concepts made in the\nparticular context of CogPrime research are the pragmatic general intelligence and the\nefficient pragmatic general intelligence.\n• Intension: In PLN, the intention of a node consists of Atoms representing properties of\nthe entity the node represents.\n• Intentional memory: A system’s knowledge of its goals and their subgoals, and associations\nbetween these goals and procedures and contexts (e.g. cognitive schematics).\n• Internal Simulation World: A simulation engine used to simulate an external environment\n(which may be physical or virtual), used by an AGI system as its ”mind’s eye” in order\nto experiment with various action‘ q sequences and envision their consequences, or observe\nthe consequences of various hypothetical situations. Particularly important for dealing with\nepisodic knowledge.\n• Interval Algebra: Allen Interval Algebra, a mathematical theory of the relationships between\ntime intervals. CogPrime utilizes a fuzzified version of classic Interval Algebra.\n• IRC Learning (Imitation, Reinforcement, Correction): Learning via interaction with\na teacher, involving a combination of imitating the teacher, getting explicit reinforcement\nsignals from the teacher, and having one’s incorrect or suboptimal behaviors guided toward\nbetterness by the teacher in real-time. This is a large part of how young humans learn.\nA.2 Glossary of Specialized Terms 335\n• Knowledge Base: A shorthand for the totality of knowledge possessed by an intelligent\nsystem during a certain interval of time (whether or not this knowledge is explicitly represented).\nPut differently: this is an intelligence’s total memory contents (inclusive of all\ntypes of memory) during an interval of time.\n• Language Comprehension: The process of mapping natural language speech or text into\na more ”cognitive”, largely language-independent representation. In OpenCog this has been\ndone by various pipelines consisting of dedicated natural language processing tools, e.g. a\npipeline: text → Link Parser → RelEx → RelEx2Frame → Frame2Atom Atomspace; and\nalternatively a pipeline Link Parser → Link2Atom → Atomspace. It would also be possible\nto do language comprehension purely via PLN and other generic OpenCog processes,\nwithout using specialized language processing tools.\n• Language Generation: The process of mapping (largely language-independent) cognitive\ncontent into speech or text. In OpenCog this has been done by various pipelines consisting of\ndedicated natural language processing tools, e.g. a pipeline: Atomspace → NLGen → text;\nor more recently Atomspace → Atom2Link → surface realization → text. It would also be\npossible to do language generation purely via PLN and other generic OpenCog processes,\nwithout using specialized language processing tools.\n• Language Processing: Processing of human language is decomposed, in CogPrime, into\nLanguage Comprehension, Language Generation, and Dialogue Control.\n• Learning: In general, the process of a system adapting based on experience, in a way that\nincreases its intelligence (its ability to achieve its goals). The theory underlying CogPrime\ndoesn’t distinguish learning from reasoning, associating, or other aspects of intelligence.\n• Learning Server: In some OpenCog configurations, this refers to a software server that\nperforms ”offline” learning tasks (e.g. using MOSES or hillclimbing), and is in communication\nwith an Operational Agent Controller software server that performs real-time agent\ncontrol and dispatches learning tasks to and receives results from the Learning Server.\n• Linguistic Links: A catch-all term for Atoms explicitly representing linguistic content,\ne.g. WordNode, SentenceNode, CharacterNode.\n• Link: A type of Atom, representing a relationship among one or more Atoms. Links and\nNodes are the two basic kinds of Atoms.\n• Link Parser: A natural language syntax parser, created by Sleator and Temperley at\nCarnegie-Mellon University, and currently used as part of OpenCogPrime’s natural language\ncomprehension and natural language generation system.\n• Link2Atom: A system for translating link parser links into Atoms. It attempts to resolve\nprecisely as much ambiguity as needed in order to translate a given assemblage of link parser\nlinks into a unique Atom structure.\n• Lobe: A term sometimes used to refer to a portion of a distributed Atomspace that lives\nin a single computational process. Often different lobes will live on different machines.\n• Localized Memory: Memory that stores each item using a small number of closelyconnected\nelements.\n• Logic: In an OpenCog context, this usually refers to a set of formal rules for translating\ncertain combinations of Atoms into ”conclusion” Atoms. The paradigm case at present is the\nPLN probabilistic logic system, but OpenCog can also be used together with other logics.\n• Logical Links: Any Atoms whose truth values are primarily determined or adjusted via\nlogical rules, e.g. PLN’s InheritanceLink, SimilarityLink, ImplicationLink, etc. The term\nisn’t usually applied to other links like HebbianLinks whose semantics isn’t primarily logic-\n336 A Glossary\nbased, even though these other links can be processed via (e.g. PLN) logical inference via\ninterpreting them logically.\n• Lojban: A constructed human language, with a completely formalized syntax and a highly\nformalized semantics, and a small but active community of speakers. In principle this seems\nan extremely good method for communication between humans and early-stage AGI systems.\n• Lojban++: A variant of Lojban that incorporates English words, enabling more flexible\nexpression without the need for frequent invention of new Lojban words.\n• Long Term Importance (LTI): A value associated with each Atom, indicating roughly\nthe expected utility to the system of keeping that Atom in RAM rather than saving it to\ndisk or deleting it. It’s possible to have multiple LTI values pertaining to different time\nscales, but so far practical implementation and most theory has centered on the option of\na single LTI value.\n• LTI: Long Term Importance\n• Map: A collection of Atoms that are interconnected in such a way that they tend to be\ncommonly active (i.e. to have high STI, e.g. enough to be in the AttentionalFocus, at the\nsame time).\n• Map Encapsulation: The process of automatically identifying maps in the Atomspace,\nand creating Atoms that ”encapsulate” them; the Atom encapsulation a map would link to\nall the Atoms in the map. This is a way of making global memory into local memory, thus\nmaking the system’s memory glocal and explicitly manifesting the ”cognitive equation.”\nThis may be carried out via a dedicated MapEncapsulation MindAgent.\n• Map Formation: The process via which maps form in the Atomspace. This need not be\nexplicit; maps may form implicitly via the action of Hebbian Learning. It will commonly\noccur that Atoms frequently co-occurring in the AttentionalFocus, will come to be joined\ntogether in a map.\n• Memory Types: In CogPrime\nthis generally refers to the different types of memory that are embodied in different data\nstructures or processes in the CogPrime\narchitecture, e.g. declarative (semantic), procedural, attentional, intentional, episodic, sensorimotor.\n• Mind-World Correspondence Principle: The principle that, for a mind to display\nefficient pragmatic general intelligence relative to a world, it should display many of the\nsame key structural properties as that world. This can be formalized by modeling the world\nand mind as probabilistic state transition graphs, and saying that the categories implicit\nin the state transition graphs of the mind and world should be inter-mappable via a highprobability\nmorphism.\n• Mind OS: A synonym for the OpenCog Core.\n• MindAgent: An OpenCog software object, residing in the CogServer, that carries out\nsome processes in interaction with the Atomspace. A given conceptual cognitive process\n(e.g. PLN inference, Attention allocation, etc.) may be carried out by a number of different\nMindAgents designed to work together.\n• Mindspace: A model of the set of states of an intelligent system as a geometrical space,\nimposed by assuming some metric on the set of mind-states. This may be used as a tool for\nformulating general principles about the dynamics of generally intelligent systems.\n• Modulators: Parameters in the Psi model of motivated, emotional cognition, that modulate\nthe way a system perceives, reasons about and interacts with the world.\nA.2 Glossary of Specialized Terms 337\n• MOSES (Meta-Optimizing Semantic Evolutionary Search): An algorithm for procedure\nlearning, which in the current implementation learns programs in the Combo language.\nMOSES is an evolutionary learning system, which differs from typical genetic programming\nsystems in multiple aspects including: a subtler framework for managing multiple ”demes”\nor ”islands” of candidate programs; a library called Reduct for placing programs in Elegant\nNormal Form; and the use of probabilistic modeling in place of, or in addition to, mutation\nand crossover as means of determining which new candidate programs to try.\n• Motoric: Pertaining to the control of physical actuators, e.g. those connected to a robot.\nMay sometimes be used to refer to the control of movements of a virtual character as well.\n• Moving Bubble of Attention: The Attentional Focus of a CogPrime system.\n• Natural Language Comprehension: See Language Comprehension\n• Natural Language Generation: See Language Generation\n• Natural Language Processing (NLP): See Language Processing\n• NLGen: Software for carrying out the surface realization phase of natural language generation,\nvia translating collections of RelEx output relationships into English sentences.\nWas made functional for simple sentences and some complex sentences; not currently under\nactive development, as work has shifted to the related Atom2Link approach to language\ngeneration.\n• Node: A type of Atom. Links and Nodes are the two basic kinds of Atoms. Nodes, mathematically,\ncan be thought of as \"0-ary\" links. Some types of Nodes refer to external or\nmathematical entities (e.g. WordNode, NumberNode); others are purely abstract, e.g. a\nConceptNode is characterized purely by the Links relating it to other atoms. Grounded-\nPredicateNodes and GroundedSchemaNodes connect to explicitly represented procedures\n(sometimes in the Combo language); ungrounded PredicateNodes and SchemaNodes are\nabstract and, like ConceptNodes, purely characterized by their relationships.\n• Node Probability: Many PLN inference rules rely on probabilities associated with Nodes.\nNode probabilities are often easiest to interpret in a specific context, e.g. the probability\nP(cat) makes obvious sense in the context of a typical American house, or in the context\nof the center of the sun. Without any contextual specification, P(A) is taken to mean\nthe probability that a randomly chosen occasion of the system’s experience includes some\ninstance of A.\n• Novamente Cognition Engine (NCE): A proprietary proto-AGI software system, the\npredecessor to OpenCog. Many parts of the NCE were open-sourced to form portions of\nOpenCog, but some NCE code was not included in OpenCog; and now OpenCog includes\nmultiple aspects and plenty of code that was not in NCE.\n• OpenCog: A software framework intended for development of AGI systems, and also for\nnarrow-AI application using tools that have AGI applications. Co-designed with the Cog-\nPrime cognitive architecture, but not exclusively bound to it.\n• OpenCog Prime (OCP): The implementation of the CogPrime cognitive architecture\nwithin the OpenCog software framework.\n• OpenPsi: CogPrime’s architecture for motivation-driven action selection, which is based\non adapting Dorner’s Psi model for use in the OpenCog framework.\n• Operational Agent Controller (OAC): In some OpenCog configurations, this is a software\nserver containing a CogServer devoted to real-time control of an agent (e.g. a virtual\nworld agent, or a robot). Background, offline learning tasks may then be dispatched to other\nsoftware processes, e.g. to a Learning Server.\n338 A Glossary\n• Pattern: In a CogPrime context, the term ”pattern” is generally used to refer to a process\nthat produces some entity, and is judged simpler than that entity.\n• Pattern Mining: Pattern mining is the process of extracting an (often large) number of\npatterns from some body of information, subject to some criterion regarding which patterns\nare of interest. Often (but not exclusively) it refers to algorithms that are rapid or ”greedy”,\nfinding a large number of simple patterns relatively inexpensively.\n• Pattern Recognition: The process of identifying and representing a pattern in some\nsubstrate (e.g. some collection of Atoms, or some raw perceptual data, etc.).\n• Patternism: The philosophical principle holding that, from the perspective of engineering\nintelligent systems, it is sufficient and useful to think about mental processes in terms of\n(static and dynamical) patterns.\n• Perception: The process of understanding data from sensors. When natural language is\ningested in textual format, this is generally not considered perceptual. Perception may be\ntaken to encompass both pre-processing that prepares sensory data for ingestion into the\nAtomspace, processing via specialized perception processing systems like DeSTIN that are\nconnected to the Atomspace, and more cognitive-level process within the Atomspace that\nis oriented toward understanding what has been sensed.\n• Piagetan Stages: A series of stages of cognitive development hypothesized by developmental\npsychologist Jean Piaget, which are easy to interpret in the context of developing\nCogPrime systems. The basic stages are: Infantile, Pre-operational, Concrete Operational\nand Formal. Post-formal stages have been discussed by theorists since Piaget and seem\nrelevant to AGI, especially advanced AGI systems capable of strong self-modification.\n• PLN: short for Probabilistic Logic Networks\n• PLN, First-Order: See First-Order Inference\n• PLN, Higher-Order: See Higher-Order Inference\n• PLN Rules: A PLN Rule takes as input one or more Atoms (the ”premises”, usually Links),\nand output an Atom that is a ”logical conclusion” of those Atoms. The truth value of the\nconsequence is determined by a PLN Formula associated with the Rule.\n• PLN Formulas: A PLN Formula, corresponding to a PLN Rule, takes the TruthValues\ncorresponding to the premises and produces the TruthValue corresponding to the conclusion.\nA single Rule may correspond to multiple Formulas, where each Formula deals with a\ndifferent sort of TruthValue.\n• Pragmatic General Intelligence: A formalization of the concept of general intelligence,\nbased on the concept that general intelligence is the capability to achieve goals in environments,\ncalculated as a weighted average over some fuzzy set of goals and environments.\n• Predicate Evaluation: The process of determining the Truth Value of a predicate, embodied\nin a PredicateNode. This may be recursive, as the predicate referenced internally by a\nGrounded PredicateNode (and represented via a Combo program tree) may itself internally\nreference other PredicateNodes.\n• Probabilistic Logic Networks (PLN): A mathematical and conceptual framework for\nreasoning under uncertainty, integrating aspects of predicate and term logic with extensions\nof imprecise probability theory. OpenCogPrime’s central tool for symbolic reasoning.\n• Procedural Knowledge: Knowledge regarding which series of actions (or action-combinations)\nare useful for an agent to undertake in which circumstances. In CogPrime these may be\nlearned in a number of ways, e.g. via PLN or via Hebbian learning of Schema Maps, or via\nexplicit learning of Combo programs via MOSES or hillclimbing. Procedures are represented\nas SchemaNodes or Schema Maps.\nA.2 Glossary of Specialized Terms 339\n• Procedure Evaluation/Execution: A general term encompassing both Schema Execution\nand Predicate Evaluation, both of which are similar computational processes involving\nmanipulation of Combo trees associated with ProcedureNodes.\n• Procedure Learning: Learning of procedural knowledge, based on any method, e.g. evolutionary\nlearning (e.g. MOSES), inference (e.g. PLN), reinforcement learning (e.g. Hebbian\nlearning).\n• Procedure Node: A SchemaNode or PredicateNode\n• Psi: A model of motivated action and emotion, originated by Dietrich Dorner and further\ndeveloped by Joscha Bach, who incorporated it in his proto-AGI system MicroPsi. OpenCog-\nPrime’s motivated-action component, OpenPsi, is roughly based on the Psi model.\n• Psynese: A system enabling different OpenCog instances to communicate without using\nnatural language, via directly exchanging Atom subgraphs, using a special system to map\nreferences in the speaker’s mind into matching references in the listener’s mind.\n• Psynet Model: An early version of the theory of mind underlying CogPrime, referred to\nin some early writings on the Webmind AI Engine and Novamente Cognition Engine. The\nconcepts underlying the psynet model are still part of the theory underlying CogPrime, but\nthe name has been deprecated as it never really caught on.\n• Reasoning: See inference\n• Reduct: A code library, used within MOSES, applying a collection of hand-coded rewrite\nrules that transform Combo programs into Elegant Normal Form.\n• Region Connection Calculus: A mathematical formalism describing a system of basic\noperations among spatial regions. Used in CogPrime as part of spatial inference to provide\nrelations and rules to be referenced via PLN and potentially other subsystems.\n• Reinforcement Learning: Learning procedures via experience, in a manner explicitly\nguided to cause the learning of procedures that will maximize the system’s expected future\nreward. CogPrime does this implicitly whenever it tries to learn procedures that will maximize\nsome Goal whose Truth Value is estimated via an expected reward calculation (where\n”reward” may mean simply the Truth Value of some Atom defined as ”reward”). Goal-driven\nlearning is more general than reinforcement learning as thus defined; and the learning that\nCogPrime does, which is only partially goal-driven, is yet more general.\n• RelEx: A software system used in OpenCog as part of natural language comprehension, to\nmap the output of the link parser into more abstract semantic relationships. These more\nabstract relationships may then be entered directly into the Atomspace, or they may be\nfurther abstracted before being entered into the Atomspace, e.g. by RelEx2Frame rules.\n• RelEx2Frame: A system of rules for translating RelEx output into Atoms, based on the\nFrameNet ontology. The output of the RelEx2Frame rules make use of the FrameNet library\nof semantic relationships. The current (2012) RelEx2Frame rule-based is problematic and\nthe RelEx2Frame system is deprecated as a result, in favor of Link2Atom. However, the\nideas embodied in these rules may be useful; if cleaned up the rules might profitably be\nported into the Atomspace as ImplicationLinks.\n• Representation Building: A stage within MOSES, wherein a candidate Combo program\ntree (within a deme) is modified by replacing one or more tree nodes with alternative tree\nnodes, thus obtaining a new, different candidate program within that deme. This process\ncurrently relies on hand-coded knowledge regarding which types of tree nodes a given tree\nnode should be experimentally replaced with (e.g. an AND node might sensibly be replaced\nwith an OR node, but not so sensibly replaced with a node representing a ”kick” action).\n340 A Glossary\n• Request for Services (RFS): In CogPrime’s Goal-driven action system, a RFS is a\npackage sent from a Goal Atom to another Atom, offering it a certain amount of STI\ncurrency if it is able to deliver the goal what it wants (an increase in its Truth Value).\nRFS’s may be passed on, e.g. from goals to subgoals to sub-subgoals, but eventually an\nRFS reaches a Grounded SchemaNode, and when the corresponding Schema is executed,\nthe payment implicit in the RFS is made.\n• Robot Preschool: An AGI Preschool in our physical world, intended for robotically embodied\nAGIs.\n• Robotic Embodiment: Using an AGI to control a robot. The AGI may be running on\nhardware physically contained in the robot, or may run elsewhere and control the robot via\nnetworking methods such as wifi.\n• Scheduler: Part of the CogServer that controls which processes (e.g. which MindAgents)\nget processor time, at which point in time.\n• Schema: A ”script” describing a process to be carried out. This may be explicit, as in the\ncase of a GroundedSchemaNode, or implicit, as the case in Schema maps or ungrounded\nSchemaNodes.\n• Schema Encapsulation: The process of automatically recognizing a Schema Map in an\nAtomspace, and creating a Combo (or other) program embodying the process carried out\nby this Schema Map, and then storing this program in the Procedure Repository and\nassociating it with a particular SchemaNode. This translates distributed, global procedural\nmemory into localized procedural memory. It’s a special case of Map Encapsulation.\n• Schema Execution: The process of ”running” a Grounded Schema, similar to running a\ncomputer program. Or, phrased alternately: The process of executing the Schema referenced\nby a Grounded SchemaNode. This may be recursive, as the predicate referenced internally by\na Grounded SchemaNode (and represented via a Combo program tree) may itself internally\nreference other Grounded SchemaNodes.\n• Schema, Grounded: A Schema that is associated with a specific executable program\n(either a Combo program or, say, C++ code)\n• Schema Map: A collection of Atoms, including SchemaNodes, that tend to be enacted\nin a certain order (or set of orders), thus habitually enacting the same process. This is a\ndistributed, globalized way of storing and enacting procedures.\n• Schema, Ungrounded: A Schema that represents an abstract procedure, not associated\nwith any particular executable program.\n• Schematic Implication: A general, conceptual name for implications of the form ((Context\nAND Procedure) IMPLIES Goal)\n• SegSim: A name for the main algorithm underlying the NLGen language generation software.\nThe algorithm is based on segmenting a collection of Atoms into small parts, and\nmatching each part against memory to find, for each part, cases where similar Atomcollections\nalready have known linguistic expression.\n• Self-Modification: A term generally used for AI systems that can purposefully modify\ntheir core algorithms and representations. Formally and crisply distinguishing this sort of\n”strong self-modification” from ”mere” learning is a tricky matter.\n• Sensorimotor: Pertaining to sensory data, motoric actions, and their combination and\nintersection.\n• Sensory: Pertaining to data received by the AGI system from the outside world. In a\nCogPrime system that perceives language directly as text, the textual input will generally\nA.2 Glossary of Specialized Terms 341\nnot be considered as ”sensory” (on the other hand, speech audio data would be considered\nas ”sensory”).\n• Short Term Importance: A value associated with each Atom, indicating roughly the\nexpected utility to the system of keeping that Atom in RAM rather than saving it to disk\nor deleting it. It’s possible to have multple LTI values pertaining to different time scales,\nbut so far practical implementation and most theory has centered on the option of a single\nLTI value.\n• Similarity: a link type indicating the probabilistic similarity between two different Atoms.\nGenerically this is a combination of Intensional Similarity (similarity of properties) and\nExtensional Similarity (similarity of members).\n• Simple Truth Value: a TruthValue involving a pair (s,d) indicating strength (e.g. probability\nor fuzzy set membership) and confidence d. d may be replaced by other options such\nas a count n or a weight of evidence w.\n• Simulation World: See Internal Simulation World\n• SMEPH (Self-Modifying Evolving Probabilistic Hypergraphs): a style of modeling\nsystems, in which each system is associated with a derived hypergraph\n• SMEPH Edge: A link in a SMEPH derived hypergraph, indicating an empirically observed\nrelationship (e.g. inheritance or similarity) between two\n• SMEPH Vertex: A node in a SMEPH derived hypergraph representing a system, indicating\na collection of system states empirically observed to arise in conjunction with the same\nexternal stimuli\n• Spatial Inference: PLN reasoning including Atoms that explicitly reference spatial relationships\n• Spatiotemporal Inference: PLN reasoning including Atoms that explicitly reference spatial\nand temporal relationships\n• STI: Shorthand for Short Term Importance\n• Strength: The main component of a TruthValue object, lying in the interval [0,1], referring\neither to a probability (in cases like InheritanceLink, SimilarityLink, EquivalenceLink,\nImplicationLink, etc.) or a fuzzy value (as in MemberLink, EvaluationLink).\n• Strong Self-Modification: This is generally used as synonymous with Self-Modification,\nin a CogPrime context.\n• Subsymbolic: Involving processing of data using elements that have no correspondence to\nnatural language terms, nor abstract concepts; and that are not naturally interpreted as\nsymbolically ”standing for” other things. Often used to refer to processes such as perception\nprocessing or motor control, which are concerned with entities like pixels or commands like\n”rotate servomotor 15 by 10 degrees theta and 55 degrees phi.” The distinction between\n”symbolic” and ”subsymbolic” is conventional in the history of AI, but seems difficult to\nformalize rigorously. Logic-based AI systems are typically considered ”symbolic”, yet\n• Supercompilation: A technique for program optimization, which globally rewrites a program\ninto a usually very different looking program that does the same thing. A prototype\nsupercompiler was applied to Combo programs with successful results.\n• Surface Realization: The process of taking a collection of Atoms and transforming them\ninto a series of words in a (usually natural) language. A stage in the overall process of\nlanguage generation.\n• Symbol Grounding: The mapping of a symbolic term into perceptual or motoric entities\nthat help define the meaning of the symbolic term. For instance, the concept ”Cat” may be\n342 A Glossary\ngrounded by images of cats, experiences of interactions with cats, imaginations of being a\ncat, etc.\n• Symbolic: Pertaining to the formation or manipulation of symbols, i.e. mental entities that\nare explicitly constructed to represent other entities. Often contrasted with subsymbolic.\n• Syntax-Semantics Correlation: In the context of MOSES and program learning more\nbroadly, this refers to the property via which distance in syntactic space (distance between\nthe syntactic structure of programs, e.g. if they’re represented as program trees) and semantic\nspace (distance between the behaviors of programs, e.g. if they’re represented as\nsets of input/output pairs) are reasonably well correlated. This can often happen among\nsets of programs that are not too widely dispersed in program space. The Reduct library\nis used to place Combo programs in Elegant Normal Form, which increases the level of\nsyntax-semantics corellation between them. The programs in a single MOSES deme are\noften closely enough clustered together that they have reasonably high syntax-semantics\ncorrelation.\n• System Activity Table: An OpenCog component that records information regarding\nwhat a system did in the past.\n• Temporal Inference: Reasoning that heavily involves Atoms representing temporal information,\ne.g. information about the duration of events, or their temporal relationship\n(before, after, during, beginning, ending). As implemented in CogPrime, makes use of an\nuncertain version of Allen Interval Algebra.\n• Truth Value: A package of information associated with an Atom, indicating its degree\nof truth. SimpleTruthValue and IndefiniteTruthValue are two common, particular kinds.\nMultiple truth values associated with the same Atom from different perspectives may be\ngrouped into CompositeTruthValue objects.\n• Universal Intelligence: A technical term introduced by Shane Legg and Marcus Hutter,\ndescribing (roughly speaking) the average capability of a system to carry out computable\ngoals in computable environments, where goal/environment pairs are weighted via the length\nof the shortest program for computing them.\n• Urge: In OpenPsi, an Urge develops when a Demand deviates from its target range.\n• Very Long Term Importance (VLTI): A bit associated with Atoms, which determines\nwhether, when an Atom is forgotten (removed from RAM), it is saved to disk (frozen) or\nsimply deleted.\n• Virtual AGI Preschool: A virtual world intended for AGI teaching/training/learning,\nbearing broad resemblance to the preschool environments used for young humans.\n• Virtual Embodiment: Using an AGI to control an agent living in a virtual world or game\nworld, typically (but not necessarily) a 3D world with broad similarity to the everyday\nhuman world.\n• Webmind AI Engine: A predecessor to the Novamente Cognition Engine and OpenCog,\ndeveloped 1997-2001 – with many similar concepts (and also some different ones) but quite\ndifferent algorithms and software architecture\nReferences 343\nReferences\nAABL02. Nancy Alvarado, Sam S. Adams, Steve Burbeck, and Craig Latta. Beyond the turing test: Performance\nmetrics for evaluating a computer simulation of the human mind. Development and\nLearning, International Conf. on, 0, 2002.\nAGBD + 08. Derek Abbott, Julio Gea-Banacloche, Paul C W Davies, Stuart Hameroff, Anton Zeilinger, Jens\nEisert, Howard M. Wiseman, Sergey M. Bezrukov, and Hans Frauenfelder. Plenary debate: quantum\neffects in biology?trivial or not? Fluctuation and Noise Letters 8(1), pp. C5ÐC26, 2008.\nAL03. J. R. Anderson and C. Lebiere. The newell test for a theory of cognition. Behavioral and Brain\nScience, 26, 2003.\nAL09. Itamar Arel and Scott Livingston. Beyond the turing test. IEEE Computer, 42(3):90–91, March\n2009.\nAM01. J. S. Albus and A. M. Meystel. Engineering of Mind: An Introduction to the Science of Intelligent\nSystems. Wiley and Sons, 2001.\nAmi89. Daniel J. Amit. Modeling brain function – the world of attractor neural networks. Cambridge\nUniversity Press, New York, USA, 1989.\nARC09. I. Arel, D. Rose, and R. Coop. Destin: A scalable deep learning architecture with application\nto high-dimensional robust pattern recognition. Proc. AAAI Workshop on Biologically Inspired\nCognitive Architectures, 2009.\nARK09a. I. Arel, D. Rose, and T. Karnowski. A deep learning architecture comprising homogeneous cortical\ncircuits for scalable spatiotemporal pattern inference. NIPS 2009 Workshop on Deep Learning for\nSpeech Recognition and Related Applications, 2009.\nArk09b. Ronald Arkin. Governing Lethal Behavior in Autonomous Robots. Chapman and Hall, 2009.\nArl75. P. K. Arlin. Cognitive development in adulthood: A fifth stage?, volume 11. Developmental\nPsychology, 1975.\nArm04. J. Andrew Armour. Cardiac neuronal hierarchy in health and disease. Am J Physiol Regul Integr\nComp Physiol 287:, 2004.\nBaa97. Bernard Baars. In the Theater of Consciousness: The Workspace of the Mind. Oxford University\nPress, 1997.\nBac09. Joscha Bach. Principles of Synthetic Intelligence. Oxford University Press, 2009.\nBar02. Albert-Laszlo Barabasi. Linked: The New Science of Networks. Perseus, 2002.\nBat79. Gregory Bateson. Mind and Nature: A Necessary Unity. New York: Ballantine, 1979.\nBC94. S. Baron-Cohen. Mindblindness: An Essay on Autism and Theory of Mind. MIT Press, 1994.\nBDL93. Louise Barrett, Robin Dunbar, and John Lycett. Human Evolutionary Psychology. Princeton\nUniversity Press, 1993.\nBDS03. S Ben-David and R Schuller. Exploiting task relatedness for learning multiple tasks. Proceedings\nof the 16th Annual Conference on Learning Theory, 2003.\nBF71. J. D. Bransford and J. Franks. The abstraction of linguistic ideas. Cognitive Psychology, 2:331–350,\n1971.\nBF09. Bernard Baars and Stan Franklin. Consciousness is computational: The lida model of global\nworkspace theory. International Journal of Machine Consciousness., 2009.\nbGBK02.\n1. Goertzel, Andrei Klimov Ben, and Arkady Klimov. Supercompiling java programs, 2002.\nBH05. Sebastian Bader and Pascal Hitzler. Dimensions of neural-symbolic integration - a structured\nsurvey. In S. Artemov, H. Barringer, A. S. d’Avila Garcez, L. C. Lamb, and J. Woods., editors,\nWe Will Show Them: Essays in Honour of Dov Gabbay, volume 1, pages 167–194. College\nPublications, 2005.\nBi01. M-m Bi, G-q andPoo. Synaptic modifications by correlated activity: Hebb’s postulate revisited.\nAnn Rev Neurosci ; 24:139-166, 2001.\nBic88. M. Bickhard. Piaget on variation and selection models: Structuralism, logical necessity, and interactivism.\nHuman Development, 31:274–312, 1988.\nBil05. Philip Bille. A survey on tree edit distance and related problems. Theoretical Computer Science,\n337:2005, 2005.\nBO09. A. Baranes and Pierre-Yves Oudeyer. R-iac: Robust intrinsically motivated active learning. Proc.\nof the IEEE International Conf. on Learning and Development, Shanghai, China., 33, 2009.\nBol98. B. Bollobas. Modern Graph Theory. Springer, 1998.\n344 A Glossary\nBos02. Nick Bostrom. Existential risks. Journal of Evolution and Technology, 9, 2002.\nBos03. Nick Bostrom. Ethical issues in advanced artificial intelligence. In Iva Smit, editor, Cognitive,\nEmotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, volume\n2., pages 12–17. 2003.\nBro84. J. Broughton. Not beyond formal operations, but beyond piaget. In M. Commons, F. Richards, and\nC. Armon, editors, Beyond Formal Operations: Late Adolescent and Adult Cognitive Development,\npages 395–411. Praeger. New York, 1984.\nBS04. B. Bakker and Juergen Schmidhuber. Hierarchical reinforcement learning based on subgoal discovery\nand subpolicy specialization. Proc. of the 8-th Conf. on Intelligent Autonomous Systems,\n2004.\nBuc03. Mark Buchanan. Small World: Uncovering Nature’s Hidden Networks. Phoenix, 2003.\nBur62. C MacFarlane Burnet. The Integrity of the Body. Harvard University Press, 1962.\nBW88. R. W. Byrne and A. Whiten. Machiavellian Intelligence. Clarendon Press, 1988.\nBZ03. Selmer Bringsjord and M Zenzen. Superminds: People Harness Hypercomputation, and More.\nKluwer, 2003.\nBZGS06. B. Bakker, V. Zhumatiy, G. Gruener, and Juergen Schmidhuber. Quasi-online reinforcement\nlearning for robots. Proc. of the International Conf. on Robotics and Automation, 2006.\nCal96. William Calvin. The Cerebral Code. MIT Press, 1996.\nCar85. S. Carey. Conceptual Change in Childhood. MIT Press, 1985.\nCar97. R Caruana. Multitask learning. Machine Learning, 1997.\nCas85. R. Case. Intellectual development: Birth to adulthood. Academic Press, 1985.\nCas04. N. L. Cassimatis. Grammatical processing using the mechanisms of physical inferences. In Proceedings\nof the Twentieth-Sixth Annual Conference of the Cognitive Science Society. 2004.\nCas07. Nick Cassimatis. Adaptive algorithmic hybrids for human-level artificial intelligence. 2007.\nCB00. W. H. Calvin and D. Bickerton. Lingua ex Machina. MIT Press, 2000.\nCB06. Rory Conolly and Jerry Blancato. Computational modeling of the liver. NCCT BOSC\nReview, 2006. http://www.epa.gov/ncct/bosc_review/2006/files/07_Conolly_\nLiver_Model.pdf.\nCM07. Jie-Qi Chen and Gillian McNamee. What is Waldorf Education? Bridging: Assessment for Teaching\nand Learning in Early Childhood Classrooms, 2007.\nCP05. M. L. Commons and A. Pekker. Hierarchical complexity: A formal theory. http:\n//www.dareassociation.org/Papers/Hierarchical%20Complexity%20-%20A%\n20Formal%20Theory%20(Commons%20&%20Pekker).pdf, 2005.\nCRK82. M. Commons, F. Richards, and D. Kuhn. Systematic and metasystematic reasoning: a case for a\nlevel of reasoning beyond Piaget’s formal operations. Child Development, 53.:1058–1069, 1982.\nCS90. A. G. Cairns-Smith. Seven Clues to the Origin of Life: A Scientific Detective Story. Cambridge\nUniversity Press, 1990.\nCse06. Peter Csermely. Weak Links: Stabilizers of Complex Systems from Proteins to Social Networks.\nSpringer, 2006.\nCSG07. Subhojit Chakraborty, Anders Sandberg, and Susan A Greenfield. Differential dynamics of transient\nneuronal assemblies in visual compared to auditory cortex. Experimental Brain Research,\n1432-1106, 2007.\nCTS + 98. M. Commons, E. J. Trudeau, S. A. Stein, F. A. Richards, and S. R. Krause. Hierarchical complexity\nof tasks shows the existence of developmental stages. Developmental Review. 18, 18.:237–278, 1998.\nDam00. Antonio Damasio. The Feeling of What Happens. Harvest Books, 2000.\nDav84. D. Davidson. Inquiries into Truth and Interpretation. Oxford: Oxford University Press, 1984.\nDC02. Roberts P D and Bell C C. Spike-timing dependent synaptic plasticity in biological systems.\nBiological Cybernetics, 87, 392-403, 2002.\nDen87. D. Dennett. The Intentional Stance. Cambridge, MA: MIT Press, 1987.\nDen91. Daniel Dennett. Consciousness Explained. Back Bay, 1991.\nDG05. Hugo De Garis. The Artilect War. ETC, 2005.\nDOP08. Wlodzislaw Duch, Richard Oentaryo, and Michel Pasquier. Cognitive architectures: Where do we\ngo from here? Proc. of the Second Conf. on AGI, 2008.\nDör02. Dietrich Dörner. Die Mechanik des Seelenwagens. Eine neuronale Theorie der Handlungsregulation.\nVerlag Hans Huber, 2002.\nEBJ + 97. J. Elman, E. Bates, M. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plunkett. Rethinking\nInnateness: A Connectionist Perspective on Development. MIT Press, 1997.\n\nReferences 345\nEde93. Gerald Edelman. Neural darwinism: Selection and reentrant signaling in higher brain function.\nNeuron, 10, 1993.\nElm91. J. Elman. Distributed representations, simple recurrent networks, and grammatical structure.\nMachine Learning, 7:195–226, 1991.\nEMC12. Effective-Mind-Control.com. Cellular memory in organ transplants. Effective\nMind Control,, 2012. http://www.effective-mind-control.com/\ncellular-memory-in-organ-transplants.html, updated Feb 1 2012.\nES00. G. Engelbretsen and F. Sommers. An invitation to formal reasoning. The Logic of Terms. Aldershot:\nAshgate, 2000.\nFB08. Stan Franklin and Bernard Baars. Possible neural correlates of cognitive processes and modules\nfrom the lida model of cognition. Cognitive Computing Research Group, University of Memphis,\n2008. http://ccrg.cs.memphis.edu/tutorial/correlates.html.\nFC86. R. Fung and C. Chong. Metaprobability and Dempster-shafer in evidential reasoning. In L. Kanal\nand J. Lemmer. North-Holland, editors, Uncertainty in Artificial Intelligence, pages 295–302.\n1986.\nFis80. K. Fischer. A theory of cognitive development: control and construction of hierarchies of skills.\nPsychological Review, 87:477–531, 1980.\nFis01. Jefferson M. Fish. Race and Intelligence: Separating Science From Myth. Routledge, 2001.\nFod94. J. Fodor. The Elm and the Expert. Cambridge, MA: Bradford Books, 1994.\nFP86. Doyne Farmer and Alan Perelson. The immune system, adaptation and machine learning. Physica\nD, v. 2, 1986.\nFra06. Stan Franklin. The lida architecture: Adding new modes of learning to an intelligent, autonomous,\nsoftware agent. Int. Conf. on Integrated Design and Process Technology, 2006.\nFre90. R. French. Subcognition and the limits of the turing test’. Mind, 1990.\nFre95. Walter Freeman. Societies of Brains. Erlbaum, 1995.\nFT02. G. Fauconnier and M. Turner. The Way We Think: Conceptual Blending and the Mind’s Hidden\nComplexities. Basic, 2002.\nGar99. H Gardner. Intelligence reframed: Multiple intelligences for the 21st century. Basic, 1999.\nGD09. Ben Goertzel and Deborah Duong. Opencog ns: An extensible, integrative architecture for intelligent\nhumanoid robotics. 2009.\nGdG08. Ben Goertzel and Hugo de Garis. Xia-man: An extensible, integrative architecture for intelligent\nhumanoid robotics. pages 86–90, 2008.\nGE86. R. Gelman and E. Meck and s. Merkin (1986). Young children’s numerical competence. Cognitive\nDevelopment, 1:1–29, 1986.\nGEA08. Ben Goertzel and Cassio Pennachin Et Al. An integrative methodology for teaching embodied\nnon-linguistic agents, applied to virtual animals in second life. In Proc.of the First Conf. on AGI.\nIOS Press, 2008.\nGer99. Michael Gershon. The Second Brain. Harper, 1999.\nGGC + 11. Ben Goertzel, Nil Geisweiller, Lucio Coelho, Predrag Janicic, and Cassio Pennachin. Real World\nReasoning. Atlantis, 2011.\nGGK02. T. Gilovich, D. Griffin, and D. Kahneman. Heuristics and biases: The psychology of intuitive\njudgment. Cambridge University Press, 2002.\nGib77. J. J. Gibson. The theory of affordances. In R. Shaw & J. Bransford. Erlbaum, editor, Perceiving,\nActing and Knowing. 1977.\nGib78. John Gibbs. Kohlberg’s moral stage theory: a Piagetian revision. Human Development, 22:89–112,\n1978.\nGib79. J. J. Gibson. The Ecological Approach to Visual Perception. Boston: Houghton Mifflin, 1979.\nGIGH08. B. Goertzel, M. Ikle, I. Goertzel, and A. Heljakka. Probabilistic Logic Networks. Springer, 2008.\nGil82. Carol Gilligan. In a Different Voice. Cambridge, MA: Harvard University Press, 1982.\nGMIH08. B. Goertzel, I. Goertzel M. Iklé, and A. Heljakka. Probabilistic Logic Networks. Springer, 2008.\nGoe93a. Ben Goertzel. The Evolving Mind. Plenum, 1993.\nGoe93b. Ben Goertzel. The Structure of Intelligence. Springer, 1993.\nGoe94. Ben Goertzel. Chaotic Logic. Plenum, 1994.\nGoe97. Ben Goertzel. From Complexity to Creativity. Plenum Press, 1997.\nGoe01. Ben Goertzel. Creating Internet Intelligence. Plenum Press, 2001.\nGoe06a. Ben Goertzel. The Hidden Pattern. Brown Walker, 2006.\nGoe06b. Ben Goertzel. The Hidden Pattern. Brown Walker, 2006.\n346 A Glossary\nGoe08. Ben Goertzel. A pragmatic path toward endowing virtually-embodied ais with human-level linguistic\ncapability. IEEE World Congress on Computational Intelligence (WCCI), 2008.\nGoe09a. Ben Goertzel. Cognitive synergy: A universal principle of feasible general intelligence? In ICCI\n2009, Hong Kong, 2009.\nGoe09b. Ben Goertzel. The embodied communication prior. In Proceedings of ICCI-09, Hong Kong, 2009.\nGoe09c. Ben Goertzel. Opencog prime: A cognitive synergy based architecture for embodied artificial\ngeneral intelligence. In ICCI 2009, Hong Kong, 2009.\nGoe10a. Ben Goertzel. Coherent aggregated volition. Multiverse According to Ben,\n2010. http://multiverseaccordingtoben.blogspot.com/2010/03/\ncoherent-aggregated-volition-toward.htm.\nGoe10b. Ben Goertzel. Opencogprime wikibook. 2010. http://wiki.opencog.org/w/\nOpenCogPrime:WikiBook.\nGoe10c. Ben Goertzel. Toward a formal definition of real-world general intelligence. 2010.\nGoe10d. Ben et al Goertzel. A general intelligence oriented architecture for embodied natural language\nprocessing. In Proc. of the Third Conf. on Artificial General Intelligence (AGI-10). Atlantis Press,\n2010.\nGoo86. I. Good. The Estimation of Probabilities. Cambridge, MA: MIT Press, 1986.\nGor86. R. Gordon. Folk psychology as simulation. Mind and Language. 1, 1.:158–171, 1986.\nGPC + 11. Ben Goertzel, Joel Pitt, Zhenhua Cai, Jared Wigmore, Deheng Huang, Nil Geisweiller, Ruiting\nLian, and Gino Yu. Integrative general intelligence for controlling game ai in a minecraft-like\nenvironment. In Proc. of BICA 2011, 2011.\nGPI + 10. Ben Goertzel, Joel Pitt, Matthew Ikle, Cassio Pennachin, and Rui Liu. Glocal memory: a design\nprinciple for artificial brains and minds. Neurocomputing, April 2010.\nGPPG06. Ben Goertzel, Hugo Pinto, Cassio Pennachin, and Izabela Freire Goertzel. Using dependency parsing\nand probabilistic inference to extract relationships between genes, proteins and malignancies\nimplicit among multiple biomedical research abstracts. In Proc. of Bio-NLP 2006, 2006.\nGPSL03. Ben Goertzel, Cassio Pennachin, Andre’ Senna, and Moshe Looks. An integrative architecture for\nartificial general intelligence. In Proceedings of IJCAI 2003, Acapulco, 2003.\nGre01. Susan Greenfield. The Private Life of the Brain. Wiley, 2001.\nGRM + 11. Erik M. Gauger, Elisabeth Rieper, John J. L. Morton, Simon C. Benjamin, and Vlatko Vedral.\nSustained quantum coherence and entanglement in the avian compass. Physics Review Letters,\nvol. 106, no. 4, 2011.\nHAG07. Markert H, Knoblauch A, and Palm G. Modelling of syntactical processing in the cortex. Biosystems\nMay-Jun; 89(1-3): 300-15, 2007.\nHam87. Stuart Hameroff. Ultimate Computing. North Holland, 1987.\nHam10. Stuart Hameroff. The Òconscious pilotÓÑdendritic synchrony moves through the brain to mediate\nconsciousness. Journal of Biological Physics, 2010.\nHay85. Patrick Hayes. The second naive physics manifesto. In R. Shaw & J. Bransford, editor, Formal\nTheories of the Commonsense World. 1985.\nHB06. Jeff Hawkins and Sandra Blakeslee. On Intelligence. Brown Walker, 2006.\nHeb49. Donald Hebb. The organization of behavior. Wiley, 1949.\nHey07. F. Heylighen. The Global Superorganism: an evolutionary-cybernetic model of the emerging network\nsociety. Social Evolution and History 6-1, 2007.\nHF95. P. Hayes and K. Ford. Turing test considered harmful. IJCAI-14, 1995.\nHG08. David Hart and Ben Goertzel. Opencog: A software framework for integrative artificial general\nintelligence. In AGI, volume 171 of Frontiers in Artificial Intelligence and Applications, pages\n468–472. IOS Press, 2008.\nHHPO12. Adam Hampshire, Roger Highfield, Beth Parkin, and Adrian Owen. Fractionating human intelligence.\nNeuron vol. 76 issue 6, 2012.\nHib02. Bill Hibbard. Superintelligent Machines. Springer, 2002.\nHof79. Douglas Hofstadter. Godel, Escher, Bach: An Eternal Golden Braid. Basic, 1979.\nHof95. Douglas Hofstadter. Fluid Concepts and Creative Analogies. Basic Books, 1995.\nHof96. Douglas Hofstadter. Metamagical Themas. Basic Books, 1996.\nHop82. J J Hopfield. Neural networks and physical systems with emergent collective computational abilities.\nProc. of the National Academy of Sciences, 79:2554–2558, 1982.\nHOT06. G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural\nComputation, 18:1527–1554, 2006.\nReferences 347\nHut95. E. Hutchins. Cognition in the Wild. MIT Press, 1995.\nHut96. Edwin Hutchins. Cognition in the Wild. MIT Press, 1996.\nHut05. Marcus Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability.\nSpringer, 2005.\nHZT + 02. J. Han, S. Zeng, K. Tham, M. Badgero, and J. Weng. Dav: A humanoid robot platform for\nautonomous mental development,. Proc. 2nd International Conf. on Development and Learning,\n2002.\nIP58. B. Inhelder and J. Piaget. The Growth of Logical Thinking from Childhood to Adolescence. Basic\nBooks, 1958.\nJL08. D. J. Jilk and C. Lebiere. and o’reilly. R. C. and Anderson, J. R. (2008). SAL: An explicitly\npluralistic cognitive architecture. Journal of Experimental and Theoretical Artificial Intelligence,\n20:197–218, 2008.\nJM09. Daniel Jurafsky and James Martin. Speech and Language Processing. Pearson Prentice Hall, 2009.\nJoy00. Bill Joy. Why the future doesn’t need us, Wired. April 2000.\nKam91. George Kampis. Self-Modifying Systems in Biology and Cognitive Science. Plenum Press, 1991.\nKan64. Immanuel Kant. Groundwork of the Metaphysic of Morals. Harper and Row, 1964.\nKap08. F. Kaplan. Neurorobotics: an experimental science of embodiment. Frontiers in Neuroscience,\n2008.\nKE06. J. L. Krichmar and G. M. Edelman. Principles underlying the construction of brain-based devices.\nIn T. Kovacs and J. A. R. Marshall, editors, Adaptation in Artificial and Biological Systems, pages\n37–42. 2006.\nKK90. K. Kitchener and P. King. Reflective judgement: ten years of research. In M. Commons.\nPraeger. New York, editor, Beyond Formal Operations: Models and Methods in the Study of\nAdolescent and Adult Thought, volume 2, pages 63–78. 1990.\nKLH83. Lawrence Kohlberg, Charles Levine, and Alexandra Hewer. Moral stages : a current formulation\nand a response to critics. Karger. Basel, 1983.\nKoh38. Wolfgang Kohler. The Place of Value in a World of Facts. Liveright Press, New York, 1938.\nKoh81. Lawrence Kohlberg. Essays on Moral Development, volume I. The Philosophy of Moral Development,\n1981.\nKS04. Adam Kahane and Peter Senge. Solving Tough Problems: An Open Way of Talking, Listening,\nand Creating New Realities. Berrett-Koehler, 2004.\nKur06. Ray Kurzweil. The Singularity is Near. 2006.\nKur12. Ray Kurzweil. How to Create a Mind. Viking, 2012.\nKyb97. H. Kyburg. Bayesian and non-bayesian evidential updating. Artificial Intelligence, 31:271–293,\n1997.\nLan05. Pat Langley. An adaptive architecture for physical agents. Proc. of the 2005 IEEE/WIC/ACM\nInt. Conf. on Intelligent Agent Technology, 2005.\nLAon. C. Lebiere and J. R. Anderson. The case for a hybrid architecture of cognition. (in preparation).\nLBDE90. Y. LeCun, B. Boser, J. S. Denker, and Al. Et. Handwritten digit recognition with a backpropagation\nnetwork. Advances in Neural Information Processing Systems, 2, 1990.\nLD03. A. Laud and G. Dejong. The influence of reward on the speed of reinforcement learning. Proc. of\nthe 20th International Conf. on Machine Learning, 2003.\nLeg06a. Shane Legg. Friendly ai is bunk. Vetta Project, 2006. http://commonsenseatheism.com/\nwp-content/uploads/2011/02/Legg-Friendly-AI-is-bunk.pdf.\nLeg06b. Shane Legg. Unprovability of friendly ai. Vetta Project, 2006. http://www.vetta.org/2006/\n09/unprovability-of-friendly-ai/.\nLG90. Douglas Lenat and R. V. Guha. Building Large Knowledge-Based Systems: Representation and\nInference in the Cyc Project. Addison-Wesley, 1990.\nLH07a. Shane Legg and Marcus Hutter. A collection of definitions of intelligence. IOS, 2007.\nLH07b. Shane Legg and Marcus Hutter. A definition of machine intelligence. Minds and Machines, 17,\n2007.\nLLW + 05. Guang Li, Zhengguo Lou, Le Wang, Xu Li, and Walter J Freeman. Application of chaotic neural\nmodel based on olfactory system on pattern recognition. ICNC, 1:378–381, 2005.\nLMC07a. M. H. Lee, Q. Meng, and F. Chao. Developmental learning for autonomous robots. Robotics and\nAutonomous Systems, 2007.\nLMC07b. M. H. Lee, Q. Meng, and F. Chao. Staged competence learning in developmental robotics. Adaptive\nBehavior, 2007.\n348 A Glossary\nLN00. George Lakoff and Rafael Nunez. Where Mathematics Comes From. Basic Books, 2000.\nLog07. Robert M. Logan. The Extended Mind. University of Toronto Press, 2007.\nLoo06. Moshe Looks. Competent Program Evolution. PhD Thesis, Computer Science Department, Washington\nUniversity, 2006.\nLRN87. John Laird, Paul Rosenbloom, and Alan Newell. Soar: An architecture for general intelligence.\nArtificial Intelligence, 33, 1987.\nLS05. J Lisman and N Spruston. Postsynaptic depolarization requirements for ltp and ltd: a critique of\nspike timing-dependent plasticity. Nature Neuroscience 8, 839-41, 2005.\nLWML09. John Laird, Robert Wray, Robert Marinier, and Pat Langley. Claims and challenges in evaluating\nhuman-level intelligent systems. Proc. of AGI-09, 2009.\nMac95. D. MacKenzie. The automation of proof: A historical and sociological exploration. IEEE Annals\nof the History of Computing, 17(3):7–29, 1995.\nMar01. H. Marchand. Reflections on PostFormal Thought. The Genetic Epistemologist, 2001.\nMcK03. Bill McKibben. Enough: Staying Human in an Engineered Age. Saint Martins Griffin, 2003.\nMet04. Thomas Metzinger. Being No One. Bradford, 2004.\nMin88. Marvin Minsky. The Society of Mind. MIT Press, 1988.\nMin07. Marvin Minsky. The Emotion Machine. 2007.\nMK07. Joseph Modayil and Benjamin Kuipers. Autonomous development of a grounded object ontology\nby a learning robot. AAAI-07, 2007.\nMK08. Jonathan Mugan and Benjamin Kuipers. Towards the application of reinforcement learning to\nundirected developmental learning. International Conf. on Epigenetic Robotics, 2008.\nMK09. Jonathan Mugan and Benjamin Kuipers. Autonomously learning an action hierarchy using a\nlearned qualitative state representation. IJCAI-09, 2009.\nMon12. Maria Montessori. The Montessori Method. Frederick A. Stokes, 1912.\nMSV + 08. G. Metta, G. Sandini, D. Vernon, L. Natale, and F. Nori. The icub humanoid robot: an open platform\nfor research in embodied cognition. Performance Metrics for Intelligent Systems Workshop\n(PerMIS 2008), 2008.\nMW07. Stephen Morgan and Christopher Winship. Counterfactuals and Causal Inference. Cambridge\nUniversity Press, 2007.\nNan08. Nanowerk. Carbon nanotube rubber could provide e-skin for robots. http://www.nanowerk.\ncom/news/newsid=6717.php, 2008.\nNei98. Dianne Miller Neilsen. Teaching Young Children, Preschool-K: A Guide to Planning Your Curriculum,\nTeaching Through Learning Centers, and Just About Everything Else. Corwin Press,\n1998.\nNew90. Alan Newell. Unified Theories of Cognition. Harvard University press, 1990.\nNie98. Dianne Miller Nielsen. Teaching Young Children, Preschool-K: A Guide to Planning Your Curriculum,\nTeaching Through Learning Centers, and Just About Everything Else. Corwin Press,\n1998.\nNil09. Nils Nilsson. The physical symbol system hypothesis: Status and prospects. 50 Years of AI,\nFestschrift, LNAI 4850, 33, 2009.\nNK04. A. Nestor and B. Kokinov. Towards active vision in the dual cognitive architecture. International\nJournal on Information Theories and Applications, 11, 2004.\nOK06. P. Oudeyer and F. Kaplan. Discovering communication. Connection Science, 2006.\nOmo08. Stephen Omohundro. The basic ai drives. Proceedings of the First AGI Conference. IOS Press,\n2008.\nOmo09. Stephen Omohundro. Creating a cooperative future. 2009. http://selfawaresystems.com/\n2009/02/23/talk-on-creating-a-cooperative-future/.\nOpa52. A. I. Oparin. The Origin of Life. Dover, 1952.\nPal82. Gunter Palm. Neural Assemblies. An Alternative Approach to Artificial Intelligence. Springer,\n1982.\nPei34. C. Peirce. Collected papers: Volume V. Pragmatism and pragmaticism. Harvard University Press.\nCambridge MA., 1934.\nPel05. Martin Pelikan. Hierarchical Bayesian Optimization Algorithm: Toward a New Generation of\nEvolutionary Algorithms. Springer, 2005.\nPen96. Roger Penrose. Shadows of the Mind. Oxford University Press, 1996.\nPer70. William G. Perry. Forms of Intellectual and Ethical Development in the College Years: A Scheme.\nHolt, Rinehart and Winston, 1970.\nReferences 349\nPer81. William G. Perry. Cognitive and ethical growth: The making of meaning. In Arthur W. Chickering.\nJossey-Bass. San Francisco, editor, The Modern American College, pages 76–116. 1981.\nPH12. Zhiping Pang and Weiping Han. Regulation of synaptic functions in central nervous system by\nendocrine hormones and the maintenance of energy homeostasis. Bioscience Reports, 2012.\nPia53. Jean Piaget. The Origins of Intelligence in Children. Routledge and Kegan Paul, 1953.\nPia55. Jean Piaget. The Construction of Reality in the Child. Routledge and Kegan Paul, 1955.\nPir84. Robert Pirsig. Zen and the Art of Motorcycle Maintenance. Bantam, 1984.\nPNR07. Karalny Patterson, Peter J. Nestor, and Timothy T. Rogers. Where do you know what you know?\nthe representation of semantic knowledge in the human brain. Nature Reviews Neuroscience,\n8:976–987, 2007.\nPSF09. Richard Dum Peter Strick and Julie Fiez. Cerebellum and nonmotor function. Annual Review of\nNeuroscience Vol. 32: 413-434, 2009.\nPW78. D. Premack and G. Woodruff. Does the chimpanzee have a theory of mind? Behavioral and Brain\nSciences, pages 515–526, 1978.\nQaGKKF05. R. Quian Quiroga, L. Reddy amd G. Kreiman, C. Koch, and I. Fried. Invariant visual representation\nby single-neurons in the human brain. Nature, 435:1102–1107, 2005.\nQKKF08. R. Quian Quiroga, G Kreiman, C Koch, and I. Fried. Sparse but not \"grandmother-cell\" coding\nin the medial temporal lobe. Trends in Cognitive Sciences, 12:87–91, 2008.\nRav04. Ian Ravenscroft. Folk psychology as a theory, stanford encyclopedia of philosophy. http://\nplato.stanford.edu/entries/folkpsych-theory/, 2004.\nRBW92. Gagne R., L. Briggs, and W. Walter. Principles of Instructional Design. Harcourt Brace Jovanovich,\n1992.\nRCK01. J. Rosbe, R. S. Chong, and D. E. Kieras. Modeling with perceptual and memory constraints: An\nepic-soar model of a simplified enroute air traffic control task. SOAR Technology Inc. Report,\n2001.\nRD06. Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 2006.\nRie73. K. Riegel. Dialectic operations: the final phase of cognitive development. Human Development,\n16.:346–370, 1973.\nRM95. H. L. Roediger and K. B. McDermott. Creating false memories: Remembering words not presented\nin lists. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21:803–814, 1995.\nRos88. Israel Rosenfield. The Invention of Memory: A New View of the Brain. Basic Books, 1988.\nRow90. John Rowan. Subpersonalities: The People Inside Us. Routledge Press, 1990.\nRow11. T Rowe. Fossil evidence on origin of the mammalian brain. Science 20, 2011.\nRV01. Alan Robinson and Andrei Voronkov. Handbook of Automated Reasoning. MIT Press, 2001.\nRZDK05. Michael Rosenstein, ZvikaMarx, Tom Dietterich, and Leslie Pack Kaelbling. Transfer learning\nwith an ensemble of background tasks. NIPS workshop on inductive transfer, 2005.\nSA93. L. Shastri and V. Ajjanagadde. From simple associations to systematic reasoning: A connectionist\nencoding of rules, variables, and dynamic bindings using temporal synchrony. Behavioral & Brain\nSciences, 16-3, 1993.\nSal93. Stan Salthe. Development and Evolution. MIT Press, 1993.\nSam10. Alexei V. Samsonovich. Toward a unified catalog of implemented cognitive architectures. In BICA,\npages 195–244, 2010.\nSB98. Richard Sutton and Andrew Barto. Reinforcement Learning. MIT Press, 1998.\nSB06. J. Simsek and A. Barto. An intrinsic reward mechanism for efficient exploration. Proc. of the\nTwenty-Third International Conf. on Machine Learning, 2006.\nSBC05. S. Singh, A. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. Proc. of\nNeural Information Processing Systems 17, 2005.\nSC94. Barry Smith and Roberto Casati. Naive Physics: An Essay in Ontology. Philosophical Psychology,\n1994.\nSch91a. Juergen Schmidhuber. Curious model-building control systems.. Proc. International Joint Conf.\non Neural Networks, 1991.\nSch91b. Juergen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\nneural controllers. Proc. of the International Conf. on Simulation of Adaptive Behavior: From\nAnimals to Animats, 1991.\nSch95. Juergen Schmidhuber. Reinforcement-driven information acquisition in non-deterministic environments.\nProc. ICANN’95, 1995.\nSch02. Juergen Schmidhuber. Exploring the predictable.. Springer, 2002.\n350 A Glossary\nSch06. J. Schmidhuber. Godel machines: Fully Self-referential Optimal Universal Self-improvers. In\nB. Goertzel and C. Pennachin, editors, Artificial General Intelligence, pages 119–226. 2006.\nSch07. Dale Schunk. Theories of Learning: An Educational Perspective. Prentice Hall, 2007.\nSE07. Stuart Shapiro and Al. Et. Metacognition in sneps. AI Magazine, 28, 2007.\nSF05. Greenfield SA and Collins T F. A neuroscientific approach to consciousness. Prog Brain Res.,\n2005.\nSha76. G. Shafer. A Mathematical Theory of Evidence. Princeton, NJ: Princeton University Press, 1976.\nShu03. Thomas R. Shultz. Computational Developmental Psychology. MIT Press, 2003.\nSKBB91. D Shannahoff-Khalsa, M Boyle, and M Buebel. The effects of unilateral forced nostril breathing\non cognition. Int J Neurosci., 1991.\nSlo01. Aaron Sloman. Varieties of affect and the cogaff architecture schema. In Proceedings of the\nSymposium on Emotion, Cognition, and Affective Computing, AISB-01, 2001.\nSlo08a. Aaron Sloman. A new approach to philosophy of mathematics: Design a young explorer able to\ndiscover ’toddler theorems’. 2008.\nSlo08b. Aaron Sloman. The Well-Designed Young Mathematician. Artificial Intelligence, December 2008.\nSM05. Push Singh and Marvin Minsky. An architecture for cognitive diversity. In Darryl Davis, editor,\nVisions of Mind. 2005.\nSot11. Kaj Sotala. 14 objections against ai/friendly ai/the singularity answered. Xuenay.net, 2011.\nhttp://www.xuenay.net/objections.html , downloaded 3/20/11.\nSS74. Jean Sauvy and Simonne Suavy. The Child’s Discovery of Space: From hopscotch to mazes – an\nintroduction to intuitive topology. Penguin, 1974.\nSS03a. John F. Santore and Stuart C. Shapiro. Crystal cassie: Use of a 3-d gaming environment for a\ncognitive agent. In Papers of the IJCAI 2003 Workshop on Cognitive Modeling of Agents and\nMulti-Agent Interactions, 2003.\nSS03b. Rudolf Steiner and S K Sagarin. What is Waldorf Education? Steiner Books, 2003.\nStc00. Theodore Stcherbatsky. Buddhist Logic. Motilal Banarsidass Pub, 2000.\nSV99. A. J. Storkey and R. Valabregue. The basins of attraction of a new hopfield learning rule. Neural\nNetworks, 12:869–876, 1999.\nSZ04. R. Sun and X. Zhang. Top-down versus bottom-up learning in cognitive skill acquisition. Cognitive\nSystems Research, 5, 2004.\nTC97. M. Tomasello and J. Call. Primate Cognition. Oxford University Press, 1997.\nTC05. Endel Tulving and R. Craik. The Oxford Handbook of Memory. Oxford U. Press, 2005.\nTea06. Sebastian Thrun and et al. The robot that won the darpa grand challenge. Journal of Robotic\nSystems, 23-9, 2006.\nTM95. S. Thrun and Tom Mitchell. Lifelong robot learning. Robotics and Autonomous Systems, 1995.\nTS94. E. Thelen and L. Smith. A Dynamic Systems Approach to the Development of Cognition and\nAction. MIT Press, 1994.\nTS07. M. Taylor and P. Stone. Cross-domain transfer for reinforcement learning. Proc. of the 24th\nInternational Conf. on Machine Learning, 2007.\nTur50. Alan Turing. Computing machinery and intelligence. Mind, 59, 1950.\nTur77. Valentin F. Turchin. The Phenomenon of Science. Columbia University Press, 1977.\nTV96. Turchin and V. Supercompilation: Techniques and results. In Dines Bjorner, M. Broy, and Aleksandr\nVasilevich Zamulin, editors, Perspectives of System Informatics. Springer, 1996.\nVin93. Vernor Vinge. The coming technological singularity. VISION-21 Symposium, NASA and\nOhio Aerospace Institute, 1993. http://www-rohan.sdsu.edu/faculty/vinge/misc/\nsingularity.html.\nVyg86. Lev Vygotsky. Thought and Language. MIT Press, 1986.\nWA10. Wendell Wallach and Colin Atkins. Moral Machines. Oxford University Press, 2010.\nWan95. P. Wang. Non-Axiomatic Reasoning System. PhD Thesis, Indiana University. Bloomington, 1995.\nWan06. Pei Wang. Rigid Flexibility: The Logic of Intelligence. Springer, 2006.\nWas09. Mark Waser. Ethics for self-improving machines. In AGI-09, 2009. http://vimeo.com/\n3698890.\nWel90. H. Wellman. The Child’s Theory of Mind. MIT Press, 1990.\nWH06. J. Weng and W. S. Hwangi. From neural networks to the brain: Autonomous mental development.\nIEEE Computational Intelligence Magazine, 2006.\nWho64. Benjamin Lee Whorf. Language, Thought and Reality. 1964.\nReferences 351\nWHZ + 00. J. Weng, W. S. Hwang, Y. Zhang, C. Yang, and R. Smith. Developmental humanoids: Humanoids\nthat develop skills automatically,. Proc. the first IEEE-RAS International Conf. on Humanoid\nRobots, 2000.\nWik11. Wikipedia. Open source governance. 2011. http://en.wikipedia.org/wiki/Open_\nsource_governance.\nWin72. Terry Winograd. Understanding Natural Language. Edinburgh University Press, 1972.\nWit07. David C. Witherington. The Dynamic Systems Approach as Metatheory for Developmental Psychology,\nHuman Development. 50, 2007.\nWol02. Stephen Wolfram. A New Kind of Science. Wolfram Media, 2002.\nWW06. Matt Williams and Jon Williamson. Combining argumentation and bayesian nets for breast cancer\nprognosis. Journal of Logic, Language and Information, 2006.\nYud04. Eliezer Yudkowsky. Coherent extrapolated volition. Singularity Institute for AI, 2004. http:\n//singinst.org/upload/CEV.html.\nYud06. Eliezer Yudkowsky. What is friendly ai? Singularity Institute for AI, 2006. http://singinst.\norg/ourresearch/publications/what-is-friendly-ai.html.\nZad78. L. Zadeh. Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems, 1:3–28, 1978.\nZPK07. Luke S Zettlemoyer, Hanna M. Pasula, and Leslie Pack Kaelbling. Logical particle filtering.\nProceedings of the Dagstuhl Seminar on Probabilistic, Logical, and Relational Learning, 2007.\n"
  },
  "error": "Failed to parse LLM response: Unterminated string starting at: line 4 column 18 (char 96)",
  "raw_response_preview": "{\n  \"file_name\": \"Engineering_General_Intelligence_Part_1.txt\",\n  \"content\": {\n    \"full_text\": \"﻿Ben Goertzel with Cassio Pennachin & Nil Geisweiller &\\nthe OpenCog Team\\nEngineering General Intelligence, Part 1:\\nA Path to Advanced AGI via Embodied Learning and\\nCognitive Synergy\\nSeptember 19, 2013\\n\\nThis book is dedicated by Ben Goertzel to his beloved,\\ndeparted grandfather, Leo Zwell – an amazingly\\nwarm-hearted, giving human being who was also a deep\\nthinker and excellent scientist, who",
  "house_oversight_id": "012899",
  "processing_metadata": {
    "processed_at": "2025-11-14T02:31:46.366478Z",
    "model": "gemini-2.5-pro"
  }
}