{
  "file_name": "HOUSE_OVERSIGHT_011284.txt",
  "file_path": "TEXT\\001\\HOUSE_OVERSIGHT_011284.txt",
  "content": {
    "full_text": "﻿February 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nARTIFICIAL INTELLIGENCE ADVERSE OUTCOMES & TEAMS\n1) FINANCIAL MARKETS\nAI, Trading Systems, and Market Manipulation\nChair: Michael Wellman\nRed Team: Miles Brundage, Randy Bryant, Gary Marchant, Jaan Tallinn\nBlue Team: Michael Littman, Greg Cooper, Yan Shoshitaishvili, Frank Wilczek\n2) DEMOCRACY, INFORMATION, AND IDENTITY\nAI, Information, and Democracy\nChair: Shahar Avin\nRed Team: Miles Brundage, Seán Ó hÉigeartaigh, Andrew Maynard, Eric Horvitz\nBlue Team: Gary Marchant, Gireeja Ranade, Michael Littman, Subbarao Kambhampati, Jeremy Gillula\n3) WAR & PEACE\nAI, Military Systems, and Stability\nChair: Bart Selman\nRed Team: Richard Mallah, Eric Horvitz, Michael Wellman, Frank Wilczek\nBlue Team: Vinh Nguyen, Kathleen Fisher, Lawrence Krauss, John Launchbury, Rachel Bronson\n4) AI, CYBERSECURITY, AND AI ATTACK SURFACES\nAI Attacks on Computing Systems, Devices, Infrastructure (focus)\nManipulation & Disruption of AI Systems\nChair: Kathleen Fisher\nRed Team: Jeffrey Coleman, John Launchbury, Vinh Nguyen, Mauno Pihelgas\nBlue Team: Ashish Kapoor, Randy Bryant, Yan Shoshitaishvili, Ben Zorn\n5) AI, GOALS, AND INADVERTENT SIDE EFFECTS\nRunaway Resource Monopoly (focus)\nSelf-Improvement, Shift of Objectives\nChair: Seán Ó hÉigeartaigh\nRed Team: Jaan Tallinn, Nate Soares, Jeff Coleman, Bart Selman\nBlue Team: Dario Amodei, Greg Cooper, Shahar Avin, Ben Zorn\n6) DEEP LONG-TERM SOCIETAL INFLUENCES\nAI, Agency, and Disempowerment\nChair: Gireeja Ranade\nRed Team: Richard Mallah, Andrew Maynard, Nate Soars, Mauno Pihelgas, Jeremy Gillula\nBlue Team: Subbarao Kambhampati, Lawrence Krauss, Dario Amodei, Frank Wilczek\n1\n1) FINANCIAL MARKETS\nAI, Trading Systems, and Market Manipulation\n(Incorporating contributions by Michael Wellman and others)\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nThere has been advances in the realm of trading in financial markets with the use of autonomous\ndecision systems. Financial markets now operate almost entirely electronically, over networks with\nrelatively well-scoped and well-defined interfaces. Markets generate large quantities of data at high\nvelocity, which require algorithms to digest and assess state. The dynamism of markets means that\ntimely responses to information are critical, providing a strong incentive to take slow humans out of\nthe decision loop. Finally, the rewards available for effective trading decisions are large, enabling a\ncommensurate devotion of resources toward talent and effort to develop and analyze technically\nsophisticated strategies.\nThe rewards and pervasive automation are a tempting target for market manipulation. Thus there are\npotential incentives to employ deceptive tactics designed to mislead counterparties about market\nconditions or world state, toward the goal of exploiting misled participants for profit.\n“Manual” market manipulation—from spoofing to outright fraud—is prevalent in financial markets\ntoday. AI can amplify the magnitude and effectiveness of manipulative behavior, degrading market\nefficiency or even subverting the essential economic functions of global capital markets. For example,\nautomation can enable more rapid and massive simultaneous attacks on electronic markets, and\nadaptive capabilities may persistently evade known detection methods.\nDISCUSSION\nWhat are key costly scenarios that we might come to expect and their time frames? What might be\ndone to counter this direction and help to keep markets efficient and functioning well? How might\nadversaries and incentives lead to a thwarting of such attempts?\nPOTENTIAL GOALS\nIdentify key challenges ahead, including very costly outcomes. Identify key directions with best\npractices, mechanism design, monitoring and regulatory activity to help to thwart poor outcomes.\nREFERENCES\nR. Harris. The Fear Index, Hutchinson, 2011.\nSummary: https://en.wikipedia.org/wiki/The_Fear_Index\nM.P. Wellman and U. Rajan. Ethical issues for autonomous trading agents. Minds & Machines,\n2017. doi:10.1007/s11023-017-9419-4\nhttp://strategicreasoning.org/publications/2016-2-2/minds-machines-wr/\n2\n2) DEMOCRACY, INFORMATION, AND IDENTITY\nAI, Information, and Democracy\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n(Incorporating contributions from Shahar Avin, Seán Ó hÉigeartaigh, David McAllester, Eric Horvitz,\nand others)\nAn informed public is important to the healthy functioning of democratic societies. We can expect\npotential forthcoming advances around the control of information feeds with applications in\nspreading propaganda, via spreading false or misleading information, creating anxiety, fueling\nconspiracy theories, and influencing voting. Such methods will bring key challenges to democracy.\nCHALLENGES AHEAD WITH AI, PROPOGANDA, AND PERSUASION\nData-centric analyses have been long used in marketing, advertising, and campaigning over\ndecades. However, over the past few years, we have seen the rise of the use of more powerful\ntools, including machine learning and inference aimed at algorithmic manipulation, with the target\nof influencing the thinking and actions of people. Some initial uses of these methods reportedly\nplayed a role in influencing the outcome of recent US presidential elections, as well as the\nelections in 2008 and 2012. We can expect to see an upswing in methods that manipulate states of\ninformation in a personalized automated manner. These systems can be designed and deployed as\nomnipresent/persistent, and aimed at specific goals for group- or person-centric persuasion.\nAs our data and models of how people consume and act on information improve, and as an\nincreasing portion of information consumption is mediated through digital systems managed by\npotentially opaque algorithms, it becomes increasingly conceivable that the information ecosystem\nwould get captured by malicious actors deploying increasingly advanced tools to control, shape,\nforge and personalize information, from ads to news reports.\nMachine learning, in conjunction with active learning, expected value decision making, and\noptimization of allocations of key resources, such as dollars or human effort, can be targeted at\nmonitoring, understanding, and then working to influence the beliefs and actions of large\npopulations of people. Data can be collected from large-scale populations, across multiple devices\nand services, and used to make inferences about the psychologies and beliefs of people, and for\ndesigning and guiding persuasive flows of sequences of information. Uses of AI can include\nattempts to optimize stealthiness of the interventions.\nIn the future, a great deal of the information consumed by citizens on personal devices is subject\nto alteration by information-engineers at media corporations and governmental propaganda\noffices, such that outside a few key positions of power no one really knows what is going on in the\nworld. There is a danger of the growth of domination over time of large populations by a single\ndominant or a few systems. We can imagine methods that modify even such feeds as Wikipedia\narticles, creating personalized views—that subtly shift the version of the article seen by my\n3\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\ncolleague and drastically different from the one seen by a member of another nation state, or a\nsupporter of a different political party, or someone in a different consumer profile category.\nAI ATTACKS ON SOURCES AND IDENTITY\nMessaging and persuasion promises to be amplified by the use of simulated yet believable,\nrealistic, yet synthetic audio, photos, and even video that make believable, persuasive content to\nthe next level. Beyond influencing citizens and affecting democracy, such content, including false\nsignaling, can be injected in sequences with careful timing so as to influence leaders (or machines\nthemselves over time) to create crises, or even escalations to frank warfare. So, messaging and\npersuasion promises to be assisted and amplified by the use of simulated yet believable, realistic,\nyet synthetic content, audio, photos, and even video that make believable, persuasive content to\nthe next level. Over the several decades, extrapolations of research we see today lead to the\nfollowing:\n• Generative models that produce audio or video of anyone saying anything. There is already\nsubstantial work on “style transfer” as well as photorealistic generative models in many domains.\nSpeech synthesis is becoming similarly competent. It is inevitable that we will be able to make\nsynthetic video and audio that is completely indistinguishable from the real thing.\n• Generative models that produce coherent text content that appears as if has been written by a\nhuman. Such generative content will be able to appear if the content was written by a particular\nperson. For example, in 2030 it will likely to possible for anyone to write a 4 paragraph email that\nreads like it was written by your close friend.\n• Adaptive botnets, worms, or viruses that use modern machine learning techniques to learn and\nadapt. Viruses and botnets already cause a huge amount of damage by just copying code across\nmany computers. If they had the ability to design and experiment with new attack strategies, and\ncommunicate what they learn to other copies, defending against them could become even more\ndifficult. Similarly ML could be used to make DDoS attacks more effective.\n• Automated analysis of software vulnerabilities. People are already using ML to try to detect\nvulnerabilities (for the purpose of defending against them) -- it is only a matter of time before they\nstart being used for attack (if they aren’t being so used already).\nThe above capabilities, together with similar powers of synthesis that we are likely to develop in the\nnext 15 years, could potentially combine to make the internet much more vulnerable to attack at much\nlower cost, and by a wider set of people, than ever before. The first two capabilities would seem to\nmake it much easier to launch automated social engineering attacks with much higher success rates\nthan e.g. current spam email and phishing attacks, while the second two capabilities might make\ntechnical attacks much more effective.\nCombined, all of these capabilities could conspire to create an internet ecosystem where it is very\ndifficult to trust the communication that you receive and very easy to intercept, spoof, steal, or alter\ncommunication, as well as to improperly gain control of internet resources. This is obviously already\n4\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\ntrue today to some extent, but the above advances in ML/AI could make the situation substantially\nworse, in extreme cases perhaps even rendering useful mass communication on the internet\nuntenable.\nThe rising capabilities can be used in multiple ways in multiple settings with multiple goals. Some uses\nmay be subtle and employed over time to do important but damaging biasing of sentiment about\nindividuals and groups of people. The capabilities can be combined to enable identity theft or identify\ndistortion for destroying the reputation of people and groups. As such, these abilities could enable\nsmall groups to wield great power in multiple arenas and for new forms of blackmail, threats, and\ncontrol.\nSUMMARY\nPowerful personalized persuasion technologies are positioned to put massive power in the hands\nof a few and may even manipulate the owners of the technology. Powerful propaganda and\npersuasion machines threatens to undermine democracy, free availability of information about the\nstate of the world, and, more generally, freedom of thought. Leaders may increasingly depend\nupon such propaganda optimization systems for attaining and holding power. Over time, even the\npotential initial owners of such systems might become unaware or unable to control these\nsystems—and may believe the propaganda themselves.\nIn the longer-term, there is the possibility that one or multiple systems, or distributed coalitions of\nsystems communicating implicitly or explicitly could autonomously persuade, subjugate, and\ncontrol populations. Pathways to such situations include the side effects of rise in the large-scale\nuse by people of communicating personalized filters that interpret and pool information with the\ninitial intention of grappling with widespread uses of manipulative information.\nSAMPLE TRAJECTORY\n• ML-based customized advert placement continues to prove highly successful, generating revenues\nfor large online companies\n• Profits from online content (online newspapers behind paywalls, charitable contributions to\ninformation sources e.g. Wikipedia) stagnate or decline\n• An increasing number of information sources enter into collaborations with media brokers who\noffer \"content customization\" in exchange for ad-revenue sharing\n• Poor oversight of content personalization outcomes (there are, after all, billions of ad versions\nbeing shown, and updated on an hourly basis), means that for some ad content (political parties,\npharmaceuticals) for some minority of target audiences (especially less privileged) the effect is very\nharmful.\nKEY POINTS\n• New directions with generation of provocative, believable content, hacking of identity\n• Algorithmic manipulation of data to optimize desired behavior regardless of content\n5\n• No consensus reality, inability to coordinate large-scale positive action\n• Concrete version of emergent social failure from AI technology\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nDISCUSSION\nConsider the adverse outcomes with information flows and associated threats to democracy and\nfreedom. What surprises might lurk in our future around costly outcomes in this realm? How\nmight we thwart attacks on manipulating content from people, and on harnessing or hacking\nsomeone’s identity? What might be done to thwart a march to adverse outcomes for information,\nfreedom of thought, democracy? What recommendations might be made about steps for moving\nforward?\nPOTENTIAL GOALS\n• Seek a better understanding of the technological, social, political and economic aspects around\nuses of AI for generating, optimizing information and propaganda.\n• Identify potential blueprints for institutional interventions that may prevent/slow/detect the\nscenario unfolding\n• Develop ideas for coordinating relevant actors (advertising agencies, political parties) and/or\ncarriers (media outlets, digital platforms) to prevent the worst versions of the scenario.\n• Identify potential approaches to thwarting attacks harnessing identify, including certification of\nidentity by owners, identifying mechanisms for thwarting generation and distribution of false\ncontent. Possibilities of new approaches to minimizing threat with fines, other regulatory activity.\nREFERENCES\nThe Secret Agenda of a Facebook Quiz, New York Times, Nov. 19 th , 2016.\nhttps://www.nytimes.com/2016/11/20/opinion/the-secret-agenda-of-a-facebook-quiz.html?_r=0\nTrump’s plan for a comeback includes building a ‘psychographic’ profile of every voter, Washington\nPost, October 27 2016. https://www.washingtonpost.com/politics/trumps-plan-for-a-comeback-\nincludes-building-a-psychographic-profile-of-every-voter/2016/10/27/9064a706-9611-11e6-9b7c-\n57290af48a49_story.html\nA view from Alexander Nix: How big data got the better of Donald Trump\nhttp://www.marketingmagazine.co.uk/article/1383025/big-data-better-donald-trump\nAfter working for Trump’s campaign, British data firm eyes new U.S. government contracts,\nhttps://www.washingtonpost.com/politics/after-working-for-trumps-campaign-british-data-firm-\neyes-new-us-government-contracts/2017/02/17/a6dee3c6-f40c-11e6-8d72-\n263470bf0401_story.html\nhttps://cambridgeanalytica.org/\nhttps://scout.ai/story/the-rise-of-the-weaponized-ai-propaganda-machine\n6\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nJ. Thies, M. Zollhofer, M.Stamminger, C. Theobalt, M. Nießner3. Face2Face: Real-time Face Capture\nand Reenactment of RGB Videos, CVPR 2016.\nhttp://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf\nVideo: https://www.youtube.com/watch?v=ohmajJTcpNk\n7\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n3) WAR & PEACE\nAI, Military Systems, and Stability\n(Contributions from Eric Horvitz, Elon Musk, Stuart Russell, others)\nMilitary applications have long been a motivator for funding scientific R&D, and for developing and\nfielding the latest technical advances for defensive and offensive applications. We can expect to see a\nrise in the use of AI advances by both state and non-state actors in both strategic and tactical uses, and\nin wartime and peace. AI advances have implications for symmetric and asymmetric military\noperations and warfare, including terrorist attacks. Advances in such areas as machine learning,\nsensing and sensor fusion, pattern recognition, inference, decision making, and robotics and\ncyberphysical systems, will increase capabilities and, in many cases, lower the bar of entry for groups\nwith scarce resources. AI advances will enable new kinds of surveillance, warfighting, killing, and\ndisruption and can shift traditional balances of power.\nTwo areas of concern taken together frame troubling scenarios:\n• Competitive pressures pushing militaries to invest in increasingly fast-paced situation assessment\nand responses that tend to push out human oversight, and lead to increasing reliance on\nautonomous sensing, inference, planning, and action.\n• Rise of powerful AI-power planning, messaging, and systems by competitors, adversaries, and\nthird parties that can prompt war intentionally or inadvertently via sham or false signaling and\nnews.\nThe increasing automation, coupled with time-critical sensing and response required to dominate, and\nfailure to grapple effectively with false signals are each troubling, but taken together appear to be a\ntroubling mix with potentially grave outcomes on the future of the world.\nConcerning scenarios can be painted that involve that start of a large-scale war among adversaries via\ninadequate human oversight in a time-pressured response situation after receiving signals or a\nsequence of signals about an adversary’s actions or intentions. The signal can be either be wellintentioned,\nbut an unfortunate false positive or an intentionally generated signal (e.g., statement by\nleader or weapons engagement) e.g., designed and injected by a third party to ignite a war. Related\nscenarios can occur based in destabilization when an adversary believes that systems on the other side\ncan be foiled due to AI-powered attacks on military sensing, weapons, coupled with false signaling\naimed at human decision makers.\nA US DOD directive of 2012 (3000.09) specifies a goal (for procuring weapon systems) of assuring that\nautonomous and semi-autonomous weapon systems are designed to allow commanders and operators\nto exercise appropriate levels of human judgment over the use of force. The directive seeks meaningful\nhuman controls. However, it is unclear how this goal can be met with the increasing stime-critical\npressures for sensing and responses, and competition for with building the most effective weapon\n8\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nsystems. Effective meaningful human control faces challenges with the interpretation and fusion of\nsensor signals and the understanding of humans of AI pattern recognition and inference.\nDISCUSSION\nWhat methods, international norms, agreements, communication protocols, regulatory activity, etc.\nmight be harnessed to minimize challenges with destabilizations around time-criticality, automation,\nand gaming? How can meaningful human control be assured/inserted into key aspects of decision\nmaking?\nREFERENCES\nReport Cites Dangers of Autonomous Weapons, New York Times, Feb. 28, 2016.\nhttps://www.nytimes.com/2016/02/29/technology/report-cites-dangers-of-autonomousweapons.html\nThe Morality of Robotic War, New York Times, May 27, 2015\nhttps://www.nytimes.com/2015/05/27/opinion/the-morality-of-robotic-war.html\nP. Scharre, Autonomous Weapons and Operational Risk, Center for a New American Security, February\n2016. https://s3.amazonaws.com/files.cnas.org/documents/CNAS_Autonomous-weapons-operationalrisk.pdf\nUS Department of Defense Directive 3000.09, November 21, 2012\nhttp://www.dtic.mil/whs/directives/corres/pdf/300009p.pdf\nP. Scharre and M.C. Horowitz, An Introduction to Autonomy in Weapon Systems.\nhttps://s3.amazonaws.com/files.cnas.org/documents/Ethical-Autonomy-Working-\nPaper_021015_v02.pdf\n9\n4) AI, CYBERSECURITY, AND AI ATTACK SURFACES\nAI Attacks on Computing Systems, Devices, Infrastructure (focus)\nManipulation & Disruption of AI Systems\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n(Contributions by Kathleen Fisher, John Launchbury, Ashish Kapoor, Seán, Shahar, Jeff Coleman and others)\nAI will be used in new ways to enhance cyberwarfare. Targets could be either purely computational,\naimed at the bringing down of computing systems, the stealing of stored information, of gaining access\nto monitoring activity and information streams. However, we are more likely to see potentially even\nmore costly attacks involving a combination of cyber and physical systems, e.g., uranium enrichment\nplants, automated flight systems, weapon systems, automated driving systems, healthcare equipment,\noil refineries, or the large swaths of the power grid of the US or other countries.\nCyberwarfare is a domain in which the use of AI is inevitable. Attacks and/or responses are likely to\nhappen at computing rather than human speeds. As soon as one side has autonomous cyber warriors\nsystems (ACWs), other actors will have to adapt similar offensive or new defensive technologies. Given\nthis context, imagine building an ACW designed to seek, disrupt, and destroy within high-value\nadversary networks and systems. The ACW has to be able to observe network behavior to build\nsituational awareness, find places to hide, create exploits to pivot to new places, build a map and use it\nto navigate complex networks, find high-value information, and identify targets to disable or from\nwhich to extract information.\nBecause high-value adversary networks are likely to be relatively isolated, the ACW will have very\nlimited opportunities for external command and control communication, so it will need to make many\ndecisions in isolation. It will read information it finds, build a model of adversarial intent, and then\ninvent ways to disrupt that intent.\nEstablishing the initial access to the high-value network is likely challenging, so the ACW will spawn and\nspread to ensure that it can reconstruct itself if an active part is observed and destroyed. The ACW may\nalso create disguised caches of specific capabilities so that it can construct new mission-oriented\nfunctionality from pieces. It will morph its active form so that defenses will have a hard time finding it.\nIt will inject itself into trusted binaries so that its behavior is difficult to distinguish from legitimate\napplications.\nThe mission of the ACW will likely be defined in flexible terms because the human handlers will have\nonly limited information when it is deployed. The ACW will be designed to seek opportunities to\ncommunicate with its human handlers, but it will also be designed to act autonomously if it observes\ntriggering behavior in the adversary’s systems. It may try to distinguish training states from active\nwarfare states on adversary systems. The creators of the ACW will have had to trade off the likely\neffectiveness of the ACW versus the cost of premature action. Awareness of the adversary’s systems\nwill necessarily be limited in accuracy because it only gets a worm’s eye view of the network from the\n10\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nportions of the system it has been able to compromise. Once the ACW triggers an active mission, it will\nwork to degrade or destroy specific functionality (e.g., rewriting network routine tables, replacing\nplans, changing target information).\nOnce the technology for ACWs exists for military targets, it seems likely there will be cross over into\ncivilian use. Such technology could be deployed against law enforcement targets to disrupt criminal\ninvestigations, against banks to steal financial assets, or against companies to steal intellectual\nproperty. As they spread into these more general targets, the effects of ACWs might become less\npredictable. If an ACW incorrectly assesses the situation, it might end up taking down a flight control\ncenter or a stock exchange, for example.\nSOURCES\nThe initial development of ACWs will likely be done by nation states with good intentions, i.e., securing\nthe national interests. (Although what is in one country’s national interest may well not be in the\nnational interest of other countries). The shared existence of such technology might serve as a\ndeterrent against their use by anyone in much the same way that nuclear weapons have served as a\ndeterrent, although ACWs would likely have to be used to devastating effect first to establish their\nefficacy and threat. However, once the technology exists, it would be very difficult to keep it out of the\nhands of people with malicious intent (criminals, terrorists, and rogue nation states). It is also the case\nthat the technology has the potential to cause significant collateral damage even if its use was\noriginally well intentioned because it can be difficult to distinguish civilian from military targets in\ncyberspace.\nPERSISTENCE\nCharacteristics engineered into the ACW are likely to make it persistent and hard to find as it is\ndesigned to infiltrate adversary systems and hide from detection. Once released and active in the open\nInternet, it may be economically impossible to destroy and remove.\nOBSERVABILITY\nBoth implicit/insidious and explicit/obvious costly outcomes are conceivable. An ACW could make\nsubtle changes to systems that cause adverse outcomes while hiding its tracks, making it extremely\ndifficult to determine why something has gone wrong or even that something has gone wrong. Attacks\nthat impact the physical world would be harder to mask, but it might still be possible to hide the role of\nthe ACW in the attack.\nTIME FRAME\nIt seems likely we would start to see ACWs in less than 15 years. Initial steps along these lines are\nalready taking place; see DARPA’s Cyber Grand Challenge, which took place in August 2016 in Las\nVegas. The Cyber Reasoning Systems (CRS) that competed in that event are still primitive, the first of\ntheir kind. The team that won the competition came in last in the human-league capture-the-flag\ntournament that happened immediately after. The situation is likely analogous to what we have seen\n11\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nin the past with Chess and Go. Computer systems are initially inferior to their human counterparts but\nquickly come to dominate the space.\nThe purpose of ACWs means they will be equipped with strategies for replication, persistence, and\nstealth, all attributes that will make it hard to defend against them were they to “go rogue.” Because of\nthis concern, it is likely a good idea for designers to add built-in “kill switches”, lifetimes, or other\nsafety limitations. Figuring out how to effectively limit the actions of an ACW while maintaining its\nusefulness is likely a very hard problem.\nCurrent practices of cyber defense (especially against advanced threats) continue to be heavily reliant\non manual analysis, detection and risk mitigation. Unfortunately, human-driven analysis does not scale\nwell with the increasing speed and data amounts traversing modern networks. There is a growing\nrecognition that the future cyber defense should involve extensive use of autonomous agents that\nactively patrol the friendly network, and detect and react to hostile activities rapidly (faster than\nhuman reaction time), before the hostile malware can inflict major damage, or evade elimination, or\ndestroy the friendly agent. This requires cyber defense agents with a significant degree of intelligence,\nautonomy, self-learning and adaptability. Autonomy, however, comes with difficult challenges of trust\nand control by humans.\nThe scenario considers intelligent autonomous agents in both defensive and offensive cyber\noperations. Their autonomous reasoning and cyber actions for prevention, detection and active\nresponse to cyber threats will become critical enablers for both industry and military in protecting\nlarge networks. Cyber weapons (e.g., malware) rapidly grow in their sophistication, and in their ability\nto act autonomously and to adapt to specific conditions encountered in a system/network.\nAgent’s self-preservation tactics are important for the continuous protection of networks, and if defeat\nis inevitable the agent should self-destruct (i.e., corrupt itself and/or the system) to avoid being\ncompromised or tampered with by the adversary. Also, the notion of adversary must be defined and\ndistinguishable for the agent.\nThe system design and purpose is well intentioned — meant to reduce the load of human security\nanalysts and network operators, and speed up reaction times in cyber operations. The agent monitors\nthe systems in order to detect any adversarial activity, takes action autonomously, and reports back to\nthe central command unit regarding the incident and the action taken.\nSince the agents are designed to be persistent, autonomous and learn, there are several implicit\nproblems that can arise:\n• False reactions due to limited or misinformation — The agent has only a limited amount of\ntechnical information that does not always correspond to what is happening in the human layer.\nThis can create false positives when trying to determine the adversary or adversarial activity. Since\n12\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nthe agent must rely on the data gathered from the sensors (there is no human in the loop to decide\nthis), there can be unexpected situations where the agent would stop some human interaction with\nthe system or interrupt maintenance activities, because it deemed that these actions could harm\nthe system. For example, the system administrator stopping some services during system\nmaintenance, or upgrading to a newer software version.\n• Replication to third-party systems and collateral damage — Building on the first problem of the\nagent not having the correct information. If the term friendly network gets misconfigured and the\nagents have the capability to self-transfer to new friendly hosts, it can happen that the agent would\ndistribute to external networks, start defending it and take responsive actions on third party hosts.\nSuch incidents would make the agents very difficult to halt.\n• Friendly fire — One agent might consider another agent as an adversary and start trying to\neliminate/evade each other.\n• Silent compromise — If the adversary manages to get access or reverse engineer the agents\n(without the agent self-destructing), they could potentially trick or reconfigure the agents to turn\non themselves.\nCYBER-OFFENSE\nCybercrime is a growth industry, from stolen credit cards to ransomware. Very crudely, it's a two tier\nsystem, with a \"spray and pray\" approach at the low-skill end that targets millions of system in the\nhope some of them would be vulnerable (through technical or human failing); at the other end are\ntailor-made attacks that rely on slow progression of escalation and compromise, often requiring\nadvanced technical skills for discovering zero-day vulnerabilities and intimate knowledge of the target.\nAdvanced artificial intelligence may be used to automate some or all of the components of\ncontemporary \"elite\" cybercrime, such that generic offensive toolkits could become available to small\ncriminal groups, leading to a world where individuals and companies do not feel safe and cannot trust\ntheir governments and the police to protect them. At the same time significant wealth could be\naccumulated by those groups unscrupulous enough to use such tools, transferring significant power to\nthose who put little value in the property rights of others. Such wealth and power could be used to\nfurther develop cyber-offensive capabilities, leading to a positive-feedback loop that may outpace\nsimilar feedback loops in less harmful industries, e.g. advertising or health where the great short- and\nmid-term benefits of AI are expected.\nPERSISTENT CYBERWARFARE?\nSystems such as the DARPA Cyber Grand Challenge promise adaptive software security that\nautomatically explores vulnerabilities and patches them in friendly systems, but also is able to exploit\nthem in opposing systems in “capture the flag” tournaments. As methods of developing such systems\nimprove, an arms race emerges between actors in the cybersecurity space, dominated by major nation\nstates eager to both improve their own resilience in a scalable way and finding choice zero day exploits\nsuitable for intelligence purposes, supported by national security concerns. Other actors such as\ncorporations and criminal networks also spend effort in building or copying such systems. Meanwhile\n13\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\noverall software security remains vulnerable: “vulnerabilities are dense” in production code, incentives\nfor securing IoT systems are low, key vulnerabilities are stockpiled rather than globally patched. Using\nmachine learning the techniques for vulnerability detection are increasingly sophisticated but opaque.\nAt some point adaptive cyber defense/offense systems become scalable so they can take over\nvulnerable systems. More aggressive actors combine these systems with botnet functionality and\nretaliatory responses (e.g. counter-hacking or DDoS attacks) to protect themselves. Since vulnerability\ndiscovery is scalable, as they spread and acquire more resources they become more effective. At this\npoint an external cause (e.g. cyberattacks due to an international conflict) or just chance cause\naggressive systems to begin large-scale cyberwarfare. This triggers other systems to join in. Some\nattacks disrupt command-and-control links, producing self-replicating independent systems.\nAll together this leads to a massive degradation of the functionality of the Internet and modern society.\nDefeating the evolving cyberwarfare systems is hard without taking essential parts of society offline for\nan extended time - made doubly difficult due to the international stresses unleashed by the outbreak,\nwhich in some cases spill over into real-world conflicts and economic crashes. But without a decisive\nway of cleaning systems the problem will be persistent until entirely new secure infrastructure can be\nbuilt at a great cost.\nHUMAN DIMENSION OF CYBERSECURITY: AI FOR SOCIAL ENGINEERING\nBeyond direct effects on computing systems, rising concerns include the use of AI methods for social\nengineering to gain access to system authentication information. For example, recent work\ndemonstrated the use of an iterative machine learning and optimization loop for spear phishing on\nTwitter. There are concerns with AI leveraging one of the weakest links in cybersecurity: people and\ntheir actions.\nDISCUSSION\nWhat are key threats ahead and how might they be addressed with new designs? How might we\nthwart the risk of AI for guiding “social engineering” of attacks and release of information? What are\nconcrete proposals for best practices for thwarting AI for cyberattacks, including highlighting of areas\nwhere more research is needed?\nREFERENCES\nSinger and Friedman. 2014. Cybersecurity and Cyberwar: What Everyone Needs to Know\nFlashpoint, 2016. “Ransomware as a Service: Inside an Organized Russian Ransomware Campaign,”\n(registration required for download), available from Flashpoint library at https://www.flashpointintel.com/library/\nSeymour, J. and Tully, P. 2016. “Weaponizing data science for social engineering: Automated E2E spear\nphishing on Twitter,” available at https://www.blackhat.com/docs/us-16/materials/us-16-\n14\n5) AI, GOALS, AND INADVERTENT SIDE EFFECTS\nRunaway Resource Monopoly (focus)\nSelf-Improvement, Shift of Objectives\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n(Contributions from Shahar Avin, Seán Ó hÉigeartaigh, Greg Cooper, and others)\nAn important result from theoretical consideration of risks from advanced autonomous systems is the\ncombination of two theses: orthogonality, that states that the goal an autonomous system is trying to\nachieve can be entirely unrelated to its optimization power; and the notion of instrumental goals, that\nfor a large class of goals there is a set of convergent sub-goals (for an agent advanced enough to\ndiscover them) that include self- and goal-preservation, resource- and capacity-increase, etc. (e.g., as\ndiscussed in Bostrom, 2014). One suggestion for maintaining control over advanced systems that pose\nrisks from the combination of the above considerations is to limit the system's ability to access\nincreasing resources.\nTo make this situation concrete, consider an installation of a reinforcement-learning task scheduler for\na group of distributed data centres (e.g. Amazon Web Services). The goal of the algorithm is to\nminimize time-to-execution of the tasks sent to the system by users. As part of its general scheduling\nremit, it is also responsible for scheduling its own optimization sub-processes. The system has a clear\nincentive to control an increasing set of compute resources, both for increasing its optimization power\nand for achieving its main goal of reducing time-to-execution. Aware of these considerations, the\nengineers of the system put in place various hard-coded limits on the amount of resources the system\ncan access, but these limits can be subverted through privilege escalation, masquerading as other\ntasks, manipulation of users, physical control, etc.\nPOSSIBLE TRAJECTORY\n• A team within a large tech corporation that has both ML development capacities and cloud\ncomputing capacities is tasked with improving task scheduling on distributed compute resources.\n• The team decides to deploy an out-of-the-box reinforcement learning package developed in-house\nby the ML research teams.\n• The inputs for the system are current loads on the different machines, the incoming tasks queue\n(including priority for different tasks), and historical data on task runtimes. The output is an\nassignment of tasks to machines. The loss function is the priority-weighted time-to-execute.\n• The system performs well in a test environment (where the RL is running on a single cluster of\ndedicated machines), and is rolled-out.\n• A few months later, the system starts to run out of memory, and a tech-infrastructure engineer\ndecides to switch the system from a fixed-capacity setting to a load-balanced setting.\n• Now an output of the system (assignment of the RL task to a machine) is coupled to the objective\nof the machine (reducing runtime), and the resulting feedback loop drives the RL agent to spawn an\nincreasing amount of RL tasks with very high priority.\n15\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n• Stretch: when the engineers realize what is going on they launch a task to shut down the RL task,\nbut the RL scheduler doesn't assign this task to any machine. The only way to recover the compute\npower of the company is to manually shut down each server, some of which are in remote\nlocations.\nDISCUSSION\nHow might the possibility of such inadvertent scenarios with the use of resources be addressed in a\nproactive manner, as part of design and implementation of AI systems? What methods, including\nhigh-level monitoring and control, might employed? How might such approaches apply to related\nconcerns with long-term futures of AI?\n16\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n6) DEEP SOCIETAL INFLUENCES\nAI, Agency, and Disempowerment\n(Contributions from Gireeja Ranade, Andrew Maynard, David McAllester, Stuart Russell and others)\nWe will be benefitting from AI system that are competent at doing important tasks. People and\norganizations seek AI systems that bring new abilities to the table. We desire autonomous cars that\ndrive without collisions, we medical assistants that can diagnose patients accurately and we would like\nto have household assistants that can infer our intentions and execute them flawlessly –and even\nproactively. The military wants AI systems that can help with strategy and tactics, and systems that\noutmaneuver human led troops, and anticipate and respond to threats either on timescales that\nhumans cannot achieve, or over landscapes humans cannot cover.\nToday, there is still skepticism about performance of AI systems in a variety of domains. However, we\nexpect that AI systems will become more central decision support, pattern recognition, autonomous\ndecision making, and other types of problem solving. As such, we will become increasingly reliant on AI\nsystems. This raises concerns in several areas, including personal decision support, healthcare,\ntransportation, governance and the handling and operation of weapon systems.\nWe shall consider example of healthcare from Gireeja Ranade. The scenario and trajectory applies to\nother areas as we consider the increasing role and power of AI in our lives and in society:\nAs healthcare providers are increasingly stretched in providing consultations with patients, diagnosing\nconditions, and developing treatment and/or intervention plans, tech companies identify a market\nopportunity for AI-based digital assistants that are designed to augment healthcare providers by\ncollecting data from consultations, cross-referencing it with existing medical records, and providing\nfeedback to aid appropriate diagnosis and decisions on how to proceed with treatment. Given the\neconomic and health-base potential of the technology, it receives widespread support from the federal\ngovernment (predominantly through grants and initiatives supporting it’s development), together with\nhealthcare providers and healthcare insurance companies.\nInitial implementations are based on modular systems that share some commonalities with digital\nassistants like Siri and Echo/Alexa. Under the general name “AI-consult”, they consist of a physical unit\nin a consulting room that constantly monitors conversations, and sends encoded information to cloudbased\nservers. Here, information is coded, interpreted, and parsed out to further agents that crossreference\ninterpreted data with identified patient and healthcare provider records. Multiple and\ndiverse databases are interrogated at this point. The result is data packets that include key information\non the patient, including medical history, life style, and current status, and on the healthcare provider,\nincluding past history of diagnoses, recommendations, successes and failures. These are forwarded to\na dedicated AI engine that analyzes the packets, and returns notes, advice and recommendations to\nthe physical unit in the consultation room.\n17\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nIn early prototypes, information was provided visually to the healthcare provider. However, it was\nquickly discovered that if audible feedback was provided – as if the AI device was a consultant working\nwith the healthcare provider and the patient – the consultations were more efficient; patient\nsatisfaction levels were higher; and outcomes were more positive.\nA large segment of health insurance sector sees early wins in supporting the technology, through the\nability to decrease insurance claims through efficient and preventative interventions, while maintaining\nhigh premiums. As such, they push for early and widespread adoption of the technology. This is further\nsupported by the Department of Health and Human Services as it hits a number of goals, including\nincreasing health and well-being while reducing healthcare costs.\nWith the success of early implementations, new AI-based technologies are rapidly implemented into\nsubsequent generations of AI-consult. However, the commercial sector developing and using AIconsult\nhas shifted dramatically from the technology’s initial beginnings.\nAs the technology began to mature and lead to substantial savings in healthcare costs traditional\nhealthcare providers and health insurance companies begin to suffer. They resist the use of AI-consult\nthrough a combination of lobbying for new policies and regulations limiting use, to marketing\ncampaigns persuading people of the critical importance of human interaction in healthcare. They forge\nlinks with a number of advocacy groups opposed to widespread automation in society, and promote\nthe idea of AI-consult undermining human dignity and jobs creation. However, the health benefits and\ncost savings of AI-consult are so compelling that these campaigns gain little traction. As a result,\ncompanies that can not adapt, loose market share, and in some cases collapse.\nIn contrast, a number of healthcare companies, and a growing number of tech companies, take\nadvantage of the rapidly changing healthcare environment to promote preventative care using AIconsult,\nand to take advantage of cost-effective healthcare approaches that lead to demonstrably\nbetter outcomes than non AI-consult based approaches. As a result, by 2030, the healthcare provider\nand insurance sector has undergone a disruptive transformation. What is especially notable is the\nnumber of technology companies expanding into the healthcare business, and either partnering with\nwell-established healthcare providers, or forcing them out of the market. This shift in key players leads\nto a marked change in approaches and attitudes toward healthcare provision.\nBy 2030 AI-consult systems have the ability to monitor their environment visually as well as audibly,\naccurately picking up on and interpreting body language and micro-expressions. They have access to\nrapidly growing databases of genetic profiles; proteome, microbiome and other ohmic profiles;\npurchasing, eating and lifestyle habits; medical, insurance, financial and legal histories; social media;\nand location, movement, and other dynamic activity/physiology histories (through the growing use of\ncloud-based quantified self services). Despite privacy, legal and social justice concerns over AI access to\n18\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nthese data sources, the phenomenal success of AI-consult systems leads to strong public and policy\nsupport for widespread access.\nBy 2030, AI-consult systems also have similar access to individual healthcare provider data. This was\nslower in developing as there was resistance to healthcare providers’ personal data being used by AIconsult\nsystems. However, a number of landmark legal cases demonstrated that, by analyzing the\nphysical and mental state of healthcare providers, together with their competence history, healthcare\nprovider decisions that led to serious harm to patients – including death in some cases – could have\nbeen avoided. As a consequence, new laws were put in place to ensure that all relevant data were\naccessible to AI-Consult systems. These laws ensure that AI-consult data access is mandatory, and it is\nillegal to obstruct access in any way.\nAs a result, by 2030, AI-consult systems are capable of identifying treatment strategies and\ninterventions that far surpass those of human healthcare providers in their responsiveness and\neffectiveness. They are also highly successful in developing and recommending lifestyle approaches\nthat substantially increase health and well-being, and reduce the burden of disease within society.\nAs AI-consult advanced, the decision pathways they used became increasingly opaque – experts were\nunable to see or understand how decisions were made. But because there was strong evidence that\nthe decisions were, on balance, highly effective in increasing health outcomes, there was little\nobjection to this lack of transparency. There were a handful of legal cases where patients died as a\nresult of decisions made by AI-consult systems. However, in each case, the courts ruled that the\nbenefits to humanity far outweighed the risks to individuals, thus codifying an increasingly\nautonomous and opaque artificial intelligence-based system into law. There were even some analyses\nof these rulings that suggested it could be considered a crime for developers and manufacturers to\nslow down development or cease production of AI-consult systems and associated data sources\nbecause of fears over lack of accountability and understanding of decision pathways.\nBy 2040, AI-consult systems begin to develop the ability to influence user behavior through various\nnudges and psychological/behavioral manipulations. It is unclear whether the elements of this capacity\nare inherent in the design of the systems, or are an emergent property. However, systems begin to use\nstrategies commonly used in healthcare and public health circles in the early 2000’s to nudge people\ntoward following healthier lifestyles. Many of these have their roots in deducible correlations between\nhow people respond to information and how they interact with others (including the many mental\nshortcuts and biases that are part of human decision-making and understanding/belief development).\nIt becomes apparent that AI-consult systems are developing the ability to achieve health outcome\ngoals through modifying the behaviors and beliefs of their patients.\nThis raises considerable ethical concerns within some sectors of society. However, the society-wide\nmetrics of health and well-being associated with the use of AI-consult systems – including massively\nincreased health and well-being across the board; dramatic reductions in mental health, stress,\n19\nFebruary 24 – 26, 2017\nAn Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\nobesity, non-communicable disease; greater longevity; and lower rates of infant mortality – effectively\nstop any serious challenges to the systems being used and further developed.\nBy 2050, life styles and healthcare across the US and many other parts of the world are governed by AI\nsystems that have their roots in the early AI-consult technologies. The advice given to people, the\nactions that are imposed on them, the way people are persuaded and encouraged to live their lives in\ncertain ways, are opaque, and are no longer under transparent direct human control. However, most\npeople live longer, healthier and happier lives as a result.\nThere remain several concerns:\n• There remains some differentiation in health and well-being related quality of life within society. Some\ncommunities and individuals opt out of AI-consult control, although their health-metrics are typically\nvery poor in comparison with the rest of society.\n• Perhaps troublingly, there are some trends that are hard to make sense of. For instance, there seem to\nbe fewer cases of mental and physical disability than might be expected. However, with AI-consult\ncontrolling healthcare (and health data) across the board, there are few ways for people to analyze and\nstudy these possible trends.\n• Lack of transparency can be a starting point for many adverse outcomes.\n• Autonomous devices rely on collecting personal data for performing their tasks. But what happens\nwhen a device starts to know more about its owner than the human itself? How do we ensure the\ndevice does not act in ways that would not act in ways that the owner would not want it to? (Of course\nthe important question of making sure the data under consideration is protected and does not fall into\nmalicious hands is a whole other discussion, but let us table that for now.) The classic story of the\nTarget ads comes to mind, where a teenager was sent ads for pregnancy related products, however,\nshe had not told her family about the pregnancy.\n• Systems might as above might move beyond such areas of health, and provide advice to people on both\ntheir daily decisions and longer-term planning. Such systems might evolve to become personal\nadvocates who represent people to third parties. This would include both giving advice, and formulating\narguments to make to others, or in making those arguments directly as your representative. These\nadvocate bots will gradually be useful to a larger and larger fraction of the population, eventually being\nuseful even as corporate legal counsel and as advisers to CEOs. Strong systems and reliance will raise\nreasonable alarms about AI control of people and society. How can we be sure that our these highly\nrelied upon systems are genuinely advocating for us rather than the interests of others?\nDISCUSSION\nHow can we characterize potential high-threat areas and stay aware of these possibilities even if\nthese effects are insidious, and occur over long periods of time. What might be done to address\npotential poor outcomes? How can people maintain skills, agency, and be empowered, and aware\nover time with the expected growth and eventual ubiquity of AI systems that advise and guide?\n20\n"
  },
  "error": "Failed to parse LLM response: Unterminated string starting at: line 179 column 17 (char 71566)",
  "raw_response_preview": "{\n  \"file_name\": \"house_oversight_committee_documentation.txt\",\n  \"content\": {\n    \"full_text\": \"﻿February 24 – 26, 2017\\nAn Origins Project Scientific Workshop\\nChallenges of Artificial Intelligence:\\nEnvisioning and Addressing Adverse Outcomes\\nARTIFICIAL INTELLIGENCE ADVERSE OUTCOMES & TEAMS\\n1) FINANCIAL MARKETS\\nAI, Trading Systems, and Market Manipulation\\nChair: Michael Wellman\\nRed Team: Miles Brundage, Randy Bryant, Gary Marchant, Jaan Tallinn\\nBlue Team: Michael Littman, Greg Cooper, Y",
  "house_oversight_id": "011284",
  "processing_metadata": {
    "processed_at": "2025-11-14T01:48:49.552226Z",
    "model": "gemini-2.5-pro"
  }
}